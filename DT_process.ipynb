{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e305e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9df95c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import numpy\n",
    "\n",
    "from statsmodels import robust\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.optimize import leastsq\n",
    "from scipy import stats\n",
    "from scipy.interpolate import splev, splrep, interp1d, LSQUnivariateSpline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96d99537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oneflux import ONEFluxError\n",
    "from oneflux.partition.daytime import partitioning_dt, PARAM_DTYPE\n",
    "from oneflux.partition.auxiliary import FLOAT_PREC, NAN, NAN_TEST, nan, not_nan\n",
    "from oneflux.partition.auxiliary import compare_col_to_pvwave, FLOAT_PREC, DOUBLE_PREC, NAN, NAN_TEST, nan, not_nan\n",
    "from oneflux.partition.library import STRING_HEADERS, DT_OUTPUT_DIR, EXTRA_FILENAME\n",
    "from oneflux.graph.compare import plot_comparison, compute_plot_param_diffs\n",
    "from oneflux.utils.files import file_exists_not_empty, check_create_directory\n",
    "from oneflux.utils.files import check_create_directory\n",
    "from oneflux.utils.helper_fns import islessthan\n",
    "from oneflux.partition.library import QC_AUTO_DIR, METEO_PROC_DIR, NEE_PROC_DIR, DT_OUTPUT_DIR, HEADER_SEPARATOR, EXTRA_FILENAME, DT_STR\n",
    "from oneflux.partition.library import load_output, get_latitude, add_empty_vars, create_data_structures, nomi, newselif, nlinlts2, check_parameters, remove_errored_entries, jacobian, ONEFluxPartitionError\n",
    "\n",
    "from oneflux.graph.compare import plot_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52cd8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_parasets(data, winsize, fguess, trimperc, name_out, dt_output_dir, site_id, ustar_type, percentile_num, year):\n",
    "    \"\"\"\n",
    "    :Task:  This function is responsible to find the best parameters to \n",
    "            represent the model that will fit the data the most.\n",
    "\n",
    "    :Explanation:   To understand this function we have to know the terms ok and nok\n",
    "                    for okay and not-okay. Basically the algorithm is about iterating\n",
    "                    through a range of days (The days are calculated and being saved \n",
    "                    to day_begin and day_end); each range represents a window.\n",
    "\n",
    "                    We go through a process of parameter evaluation and based\n",
    "                    on the retrieved parameters, we decide if they're okay or not okay,\n",
    "                    and save them to the proper arrays, whether it's params_ok or params_nok.\n",
    "\n",
    "                    To effectively come up with the closely best parameters, we try \n",
    "                    3 different guesses in the \"for j\" loop. After trying the 3 different\n",
    "                    guesses and find the most suiltable parameters of the 3, we validate\n",
    "                    them by calling the function \"check_parameters\".\n",
    "\n",
    "                    Based on specified conditions, we decide which model function to \n",
    "                    use to come up with the proper parameters (e.g lloyd_taylor, hlrc_lloydvpd, etc).\n",
    "\n",
    "\n",
    "    :param data: data structure for partitioning\n",
    "    :type data: numpy.ndarray\n",
    "    :param winsize: window size to get best parameters within each window\n",
    "    :type winsize: int\n",
    "    :param fguess: the initial guesses for the optimization function to start with\n",
    "    :type fguess: array of floats\n",
    "    :param trimperc: percentage to trim\n",
    "    :type trimperc: float\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting estimate_parasets of daytime for nee_{u}_{p}_{s}_{y}\".format(u=ustar_type, p=percentile_num, s=site_id, y=year))\n",
    "\n",
    "\n",
    "    ###############################################\n",
    "    #### self note: pvwave dimensions are reversed,\n",
    "    #### row and col and flipped in translation\n",
    "    ###############################################\n",
    "\n",
    "    #### Creating the arrays we're going to use\n",
    "    n_parasets = int(365 / winsize) * 2\n",
    "    params = numpy.zeros((3, 2 * len(fguess), n_parasets), dtype=FLOAT_PREC)\n",
    "    params_ok = numpy.zeros((2 * len(fguess), n_parasets), dtype=FLOAT_PREC)\n",
    "    params_nok = numpy.zeros((2 * len(fguess), n_parasets), dtype=FLOAT_PREC)\n",
    "    rmse = numpy.zeros(3, dtype=FLOAT_PREC)\n",
    "    #ind = fltarr(n_parasets, 3, 3)\n",
    "    ind = numpy.zeros((3, 3, n_parasets), dtype=FLOAT_PREC)\n",
    "    ind_ok = numpy.zeros((3, n_parasets), dtype=FLOAT_PREC)\n",
    "    p_cor = numpy.zeros((3, 6, n_parasets), dtype=FLOAT_PREC)\n",
    "    p_cor_ok = numpy.zeros((6, n_parasets), dtype=FLOAT_PREC)\n",
    "\n",
    "    JTJ_inv_ok = numpy.zeros((n_parasets, len(fguess) - 1, len(fguess) - 1), dtype=DOUBLE_PREC)\n",
    "    whichmodel = numpy.zeros(3, dtype=int)\n",
    "    whichmodel_ok = numpy.zeros(n_parasets, dtype=int)\n",
    "    res_cor = numpy.zeros(3, dtype=DOUBLE_PREC)\n",
    "    res_cor_ok = numpy.zeros(n_parasets, dtype=DOUBLE_PREC)\n",
    "\n",
    "    params_all = numpy.zeros((2 * len(fguess), n_parasets), dtype=FLOAT_PREC)\n",
    "    params_all_timestamp = numpy.zeros((n_parasets, 2 * len(fguess) + 2), dtype=FLOAT_PREC)\n",
    "\n",
    "    # my code\n",
    "    new_dtype = PARAM_DTYPE\n",
    "    ### intitalize extra diagnostics output\n",
    "    params_all_for_ranges = numpy.zeros(n_parasets, dtype=new_dtype)\n",
    "    params_all_for_ranges['year'] = int(year)\n",
    "    params_all_for_ranges['nee_avg'] = NAN\n",
    "    params_all_for_ranges['ta_avg'] = NAN\n",
    "    params_all_for_ranges['rg_avg'] = NAN\n",
    "    params_all_for_ranges['nee_std'] = NAN\n",
    "    params_all_for_ranges['ta_std'] = NAN\n",
    "    params_all_for_ranges['rg_std'] = NAN\n",
    "    #end of my code\n",
    "\n",
    "    '''\n",
    "    n_parasets = long(365 / winsize) * 2\n",
    "    params = fltarr(n_parasets, 2 * n_elements(fguess), 3)\n",
    "    params_ok = fltarr(n_parasets, 2 * n_elements(fguess))\n",
    "    params_nok = fltarr(n_parasets, 2 * n_elements(fguess))\n",
    "    rmse = fltarr(3)\n",
    "    ind = fltarr(n_parasets, 3, 3)\n",
    "    ind_ok = fltarr(n_parasets, 3)\n",
    "    p_cor = fltarr(n_parasets, 6, 3)\n",
    "    p_cor_ok = fltarr(n_parasets, 6)\n",
    "\n",
    "    JTJ_inv_ok = dblarr(n_elements(fguess) - 1, n_elements(fguess) - 1, n_parasets)\n",
    "    whichmodel = intarr(3)\n",
    "    whichmodel_ok = intarr(n_parasets)\n",
    "    res_cor = dblarr(3)\n",
    "    res_cor_ok = dblarr(n_parasets)\n",
    "\n",
    "\n",
    "        JTJ_inv = dblarr(n_elements(fguess) - 1, n_elements(fguess) - 1, 3)\n",
    "    '''\n",
    "\n",
    "\n",
    "    #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "    #;;; whichmodel: choice of model  ;;;\n",
    "    #;;; 0: HLRC_LloydVPD             ;;;\n",
    "    #;;; 1: HLRC_Lloyd                ;;;\n",
    "    #;;; 2: HLRC_Lloyd_afix           ;;;\n",
    "    #;;; 3: HLRC_LloydVPD_afix        ;;;\n",
    "    #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "    i_ok = 0\n",
    "    i_nok = 0\n",
    "    betafac = [0.5, 1, 2]\n",
    "\n",
    "    lloydtemp_e0 = None\n",
    "    lloydtemp_e0_se = None\n",
    "\n",
    "    #numpy.savetxt(fname='../dt_set_before_es_2013_y_python.csv', X=data, delimiter=',', fmt='%s', header=','.join(data.dtype.names), comments='')\n",
    "    #exit()\n",
    "\n",
    "    #### Iterate through each parameter set to create this set\n",
    "    for i in range(n_parasets):\n",
    "        JTJ_inv = numpy.zeros((3, len(fguess) - 1, len(fguess) - 1), dtype=DOUBLE_PREC)\n",
    "        #JTJ_inv = numpy.zeros((len(fguess) - 1, 3, len(fguess) - 1), dtype=DOUBLE_PREC)\n",
    "\n",
    "        #### Defining the range of window of data we're going\n",
    "        #### to use for optimization\n",
    "        day_begin = i * winsize / 2.0\n",
    "        day_end = day_begin + winsize\n",
    "\n",
    "        day_begin2 = 0\n",
    "        day_end2 = numpy.amax(data['julday'])\n",
    "\n",
    "        if i > 1:\n",
    "            day_begin2 = (i - 2) * winsize / 2.0\n",
    "        if i < n_parasets - 2:\n",
    "            day_end2 = (i + 2) * winsize / 2.0 + winsize\n",
    "\n",
    "#        print(\"#######################################################################\")\n",
    "#        print(\"#######################################################################\")\n",
    "#        print(\"#######################################################################\")\n",
    "#\n",
    "#        print(\"i\")\n",
    "#        print(i)\n",
    "#        print(\"i_ok\")\n",
    "#        print(i_ok)\n",
    "#        print(\"day_begin\")\n",
    "#        print(day_begin)\n",
    "#        print(\"day_end\")\n",
    "#        print(day_end)\n",
    "#        print(\"day_begin2\")\n",
    "#        print(day_begin2)\n",
    "#        print(\"day_end2\")\n",
    "#        print(day_end2)\n",
    "\n",
    "\n",
    "        #### Creating the masks of the data. We'll be using\n",
    "        #### these masks to select the data that fits certain\n",
    "        #### conditions for processing\n",
    "        sub_mask = ((data['julday'] > day_begin) & (data['julday'] <= day_end) & (data['nee_fqc'] == 0))\n",
    "        subn_mask = ((data['julday'] > day_begin2) & (data['julday'] <= day_end2) & (data['nee_fqc'] == 0) & (data['rg'] <= 4))\n",
    "        subd_mask = ((data['julday'] > day_begin) & (data['julday'] <= day_end) & (data['nee_fqc'] == 0) & (data['rg'] > 4))\n",
    "\n",
    "        #### Get the data that correspond to the masks in the previous step\n",
    "        sub, _, _ = newselif(data=data, condition=sub_mask, drop=True)\n",
    "        subn, _, _ = newselif(data=data, condition=subn_mask, drop=True)\n",
    "        subd, _, _ = newselif(data=data, condition=subd_mask, drop=True)\n",
    "\n",
    "\n",
    "        '''\n",
    "        print(\"name_out, day_begin2, day_end2\")\n",
    "        print(\"name_out, \" + str(day_begin2) + \", \" + str(day_end2))\n",
    "        print(\"int((day_begin + winsize / 2.0) * 48.0)\")\n",
    "        print(int((day_begin + winsize / 2.0) * 48.0))\n",
    "        '''\n",
    "\n",
    "        #ind[i][:][:] = int((day_begin + winsize / 2.0) * 48.0)\n",
    "        #ind[i, :, :] = int((day_begin + winsize / 2.0) * 48.0)\n",
    "\n",
    "        #### Calculate the first index of the window we're using now\n",
    "        ind[:, :, i] = int((day_begin + winsize / 2.0) * 48.0)\n",
    "\n",
    "        '''\n",
    "        #print(\"ind[:, :, i]\")\n",
    "        #print(ind[:, :, i])\n",
    "        #exit()\n",
    "\n",
    "        print(\"numpy.any(subn_mask)\")\n",
    "        print(numpy.any(subn_mask))\n",
    "        print(\"numpy.any(subd_mask)\")\n",
    "        print(numpy.any(subd_mask))\n",
    "        #print(\"subn['nee_fs_unc']\")\n",
    "        #print(subn['nee_fs_unc'])\n",
    "        print(\"subd['nee_fs_unc']\")\n",
    "        print(subd['nee_fs_unc'])\n",
    "\n",
    "        #if i == 45:\n",
    "        #    exit()\n",
    "        '''\n",
    "\n",
    "        if numpy.amin(subn['nee_fs_unc']) < 0:\n",
    "            subn['nee_fs_unc'][:] = 1\n",
    "\n",
    "        if numpy.amin(subd['nee_fs_unc']) < 0:\n",
    "            subd['nee_fs_unc'][:] = 1\n",
    "\n",
    "        '''\n",
    "        if i == 173:\n",
    "            print(\"subn['nee_f'].shape\")\n",
    "            print(subn['nee_f'].shape)\n",
    "        '''\n",
    "\n",
    "        E0set = 0\n",
    "        #### If the data in subn within the window is <= 10, then use\n",
    "        #### the lloydtemp_e0 from the previous window\n",
    "        if subn['nee_f'].shape[0] <= 10 and i_ok > 0 and lloydtemp_e0 != None:\n",
    "            lloydtemp_e0 = params_ok[4, i_ok - 1]\n",
    "            lloydtemp_e0_se = params_ok[9, i_ok - 1]\n",
    "            #ind[i][0][:] = ind_ok[i_ok - 1][0]\n",
    "            ind[:, 0, i] = ind_ok[0, i_ok - 1]\n",
    "            E0set = 1\n",
    "\n",
    "            '''\n",
    "            print(\"lloydtemp_e0\")\n",
    "            print(lloydtemp_e0)\n",
    "            print(\"lloydtemp_e0_se\")\n",
    "            print(lloydtemp_e0_se)\n",
    "            '''\n",
    "\n",
    "        '''\n",
    "        print(\"i\")\n",
    "        print(i)\n",
    "        print(\"i_ok\")\n",
    "        print(i_ok)\n",
    "        print(\"subn['nee_f'].shape[0]\")\n",
    "        print(subn['nee_f'].shape[0])\n",
    "        print(\"E0set\")\n",
    "        print(E0set)\n",
    "        '''\n",
    "\n",
    "        #### Chech if the data is suitable for optimization (to find the model)\n",
    "        if (subn['nee_f'].shape[0] > 10 or E0set == 1) and subd['nee_f'].shape[0] > 10:\n",
    "            #### Calling percentiles_fn to get the values of the chosen\n",
    "            #### percentiles from the \"nee_f\" data array after sorting it\n",
    "            percs = percentiles_fn(data=sub, columns=['nee_f'], values=[0.03, 0.97])\n",
    "            #### Setting initial value for beta amplitude of NEE\n",
    "            beta = abs(percs[0] - percs[1])\n",
    "\n",
    "            #### Setting initial value for rb to be the average\n",
    "            #### of the \"nee_f\" data\n",
    "            rb = numpy.average(subn['nee_f'])\n",
    "            fguess[3] = rb\n",
    "\n",
    "            if E0set == 0:\n",
    "                # estimate temperature sensitivity from data\n",
    "                '''\n",
    "                print(\"****************************\")\n",
    "                print(\"Starting LloydTemp\")\n",
    "                print(\"****************************\")\n",
    "                '''\n",
    "                #status, rref, e0, rref_se, e0_se, residuals, covariance_matrix, cor_matrix, lt_rmse, ls_status = nlinlts2(data=subn, lts_func=\"LloydTemp\", depvar='nee_f', indepvar_arr=['tair_f'], npara=2, xguess=fguess[3:4+1], mprior=numpy.array(fguess[3:4+1], dtype=FLOAT_PREC), sigm=numpy.array([800, 1000]), sigd=subn['nee_fs_unc'])\n",
    "\n",
    "                #### Starting the optimization using the \"LloyedTemp\" function\n",
    "                lloyedTemp_result = nlinlts2(data=subn, lts_func=\"LloydTemp\", depvar='nee_f', indepvar_arr=['tair_f'], npara=2, xguess=fguess[3:4 + 1], mprior=numpy.array(fguess[3:4 + 1], dtype=FLOAT_PREC), sigm=numpy.array([800, 1000]), sigd=subn['nee_fs_unc'])\n",
    "\n",
    "                #### Setting the returned model parameters\n",
    "                status = lloyedTemp_result['status']\n",
    "                rref = lloyedTemp_result['rref']\n",
    "                e0 = lloyedTemp_result['e0']\n",
    "                rref_se = lloyedTemp_result['rref_std_error']\n",
    "                e0_se = lloyedTemp_result['e0_std_error']\n",
    "                residuals = lloyedTemp_result['residuals']\n",
    "                covariance_matrix = lloyedTemp_result['cov_matrix']\n",
    "                cor_matrix = lloyedTemp_result['cor_matrix']\n",
    "                lt_rmse = lloyedTemp_result['rmse']\n",
    "                ls_status = lloyedTemp_result['ls_status']\n",
    "\n",
    "                lloydtemp_e0 = e0\n",
    "                lloydtemp_e0_se = e0_se\n",
    "\n",
    "                if covariance_matrix is None or cor_matrix is None:\n",
    "                    raise ONEFluxPartitionBrokenOptError('LloydTemp', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "                '''\n",
    "                print(\"fguess\")\n",
    "                print(fguess)\n",
    "                print(\"fguess[3:4+1]\")\n",
    "                print(fguess[3:4+1])\n",
    "                print(\"status\")\n",
    "                print(status)\n",
    "                print(\"rref\")\n",
    "                print(rref)\n",
    "                print(\"e0\")\n",
    "                print(e0)\n",
    "                print(\"rref_se\")\n",
    "                print(rref_se)\n",
    "                print(\"e0_se\")\n",
    "                print(e0_se)\n",
    "                print(\"residuals\")\n",
    "                print(residuals)\n",
    "                print(\"covariance_matrix\")\n",
    "                print(covariance_matrix)\n",
    "                print(\"cor_matrix\")\n",
    "                print(cor_matrix)\n",
    "                print(\"ls_status\")\n",
    "                print(ls_status)\n",
    "                '''\n",
    "\n",
    "                #### Check that the returned e0 is within range\n",
    "                #### if not, then get the e0 set from the previous\n",
    "                #### parameter set; if this doesn't work, set it to the limits.\n",
    "                if e0 < 50 or e0 > 400:\n",
    "                    if i_ok > 0:\n",
    "                        e0 = params_ok[4, i_ok - 1]\n",
    "                        e0_se = params_ok[9, i_ok - 1]\n",
    "                        #ind[i][0][:] = ind_ok[i_ok - 1][0]\n",
    "                        ind[:, 0, i] = ind_ok[0, i_ok - 1]\n",
    "                    elif e0 < 50:\n",
    "                        e0 = 50\n",
    "                        e0_se = NAN\n",
    "                    elif e0 > 400:\n",
    "                        e0 = 400\n",
    "                        e0_se = NAN\n",
    "                #end if\n",
    "            #end if\n",
    "\n",
    "            subd['e0_1_from_tair'][:] = e0\n",
    "\n",
    "            #### Finding slope of three different initial guess values\n",
    "            #### and choose the best of three\n",
    "            for j in range(2 + 1):\n",
    "                '''\n",
    "                print(\"===========\")\n",
    "                print(\"j\")\n",
    "                print(j)\n",
    "                print(\"===========\")\n",
    "                '''\n",
    "\n",
    "                #### Change second value of fguess to\n",
    "                #### beta * (half initial guess, initial guess and double initial guess)\n",
    "                fguess[1] = beta * betafac[j]\n",
    "\n",
    "                # estimate parameters of the HLRC with fixed E0\n",
    "                '''\n",
    "                print(\"****************************\")\n",
    "                print(\"Starting HLRC_LloydVPD\")\n",
    "                print(\"****************************\")\n",
    "                '''\n",
    "                #numpy.savetxt(fname=\"dt_subd_2005_y_i_91_python.csv\", X=subd, delimiter=',', fmt='%s', header=','.join(subd.dtype.names), comments='')\n",
    "\n",
    "                #### Starting the optimization using the \"HLRC_LloydVPD\" function\n",
    "                hlrclvpd_results = nlinlts2(data=subd, lts_func=\"HLRC_LloydVPD\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'vpd_f'], npara=4, xguess=fguess[0:3 + 1], mprior=numpy.array(fguess[0:3 + 1], dtype=FLOAT_PREC), sigm=numpy.array([10, 600, 50, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                #print(nlinlts2(data=subd, lts_func=\"HLRC_LloydVPD\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'vpd_f'], npara=4, xguess=fguess[0:3+1], mprior=numpy.array(fguess[0:3+1], dtype=FLOAT_PREC), sigm=numpy.array([10, 600, 50, 80]), sigd=subd['nee_fs_unc']))\n",
    "                #hlrclvpd_status, hlrclvpd_alpha, hlrclvpd_beta, hlrclvpd_k, hlrclvpd_rref, hlrclvpd_alpha_se, hlrclvpd_beta_se, hlrclvpd_k_se, hlrclvpd_rref_se, hlrclvpd_residuals, hlrclvpd_cov_matrix, hlrclvpd_cor_matrix, hlrclvpd_rmse, hlrclvpd_ls_status = nlinlts2(data=subd, lts_func=\"HLRC_LloydVPD\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'vpd_f'], npara=4, xguess=fguess[0:3+1], mprior=numpy.array(fguess[0:3+1], dtype=FLOAT_PREC), sigm=numpy.array([10, 600, 50, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                #### Setting the returned model parameters\n",
    "                hlrclvpd_status = hlrclvpd_results['status']\n",
    "                hlrclvpd_alpha = hlrclvpd_results['alpha']\n",
    "                hlrclvpd_beta = hlrclvpd_results['beta']\n",
    "                hlrclvpd_k = hlrclvpd_results['k']\n",
    "                hlrclvpd_rref = hlrclvpd_results['rref']\n",
    "                hlrclvpd_alpha_se = hlrclvpd_results['alpha_std_error']\n",
    "                hlrclvpd_beta_se = hlrclvpd_results['beta_std_error']\n",
    "                hlrclvpd_k_se = hlrclvpd_results['k_std_error']\n",
    "                hlrclvpd_rref_se = hlrclvpd_results['rref_std_error']\n",
    "                hlrclvpd_residuals = hlrclvpd_results['residuals']\n",
    "                hlrclvpd_cov_matrix = hlrclvpd_results['cov_matrix']\n",
    "                hlrclvpd_cor_matrix = hlrclvpd_results['cor_matrix']\n",
    "                hlrclvpd_rmse = hlrclvpd_results['rmse']\n",
    "                hlrclvpd_ls_status = hlrclvpd_results['ls_status']\n",
    "\n",
    "                if hlrclvpd_cov_matrix is None or hlrclvpd_cor_matrix is None:\n",
    "                    raise ONEFluxPartitionBrokenOptError('HLRC_LloydVPD', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "                '''\n",
    "                if hlrclvpd_residuals == None:\n",
    "                    # Handle error\n",
    "                '''\n",
    "\n",
    "                '''\n",
    "                print(\"fguess[0:3+1]\")\n",
    "                print(fguess[0:3+1])\n",
    "                print(\"hlrclvpd_alpha\")\n",
    "                print(hlrclvpd_alpha)\n",
    "                print(\"hlrclvpd_beta\")\n",
    "                print(hlrclvpd_beta)\n",
    "                print(\"hlrclvpd_k\")\n",
    "                print(hlrclvpd_k)\n",
    "                print(\"hlrclvpd_rref\")\n",
    "                print(hlrclvpd_rref)\n",
    "\n",
    "                print(\"hlrclvpd_alpha_se\")\n",
    "                print(hlrclvpd_alpha_se)\n",
    "                print(\"hlrclvpd_beta_se\")\n",
    "                print(hlrclvpd_beta_se)\n",
    "                print(\"hlrclvpd_k_se\")\n",
    "                print(hlrclvpd_k_se)\n",
    "                print(\"hlrclvpd_rref_se\")\n",
    "                print(hlrclvpd_rref_se)\n",
    "\n",
    "                print(\"====================\")\n",
    "                print(\"hlrclvpd_residuals\")\n",
    "                print(hlrclvpd_residuals)\n",
    "                print(\"hlrclvpd_cov_matrix\")\n",
    "                print(hlrclvpd_cov_matrix)\n",
    "                print(\"hlrclvpd_cor_matrix\")\n",
    "                print(hlrclvpd_cor_matrix)\n",
    "\n",
    "                print(\"len(hlrclvpd_residuals)\")\n",
    "                print(len(hlrclvpd_residuals))\n",
    "                '''\n",
    "\n",
    "                #### Specifying which model we chose for this iteration (j -> modified fguess)\n",
    "                whichmodel[j] = 0\n",
    "\n",
    "                res_cor[j] = (hlrclvpd_residuals ** 2).sum() / (len(hlrclvpd_residuals) * (1.0 - trimperc / 100.0) - 4)\n",
    "\n",
    "                #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "                params[j, :, i] = numpy.array([hlrclvpd_alpha, hlrclvpd_beta, hlrclvpd_k, hlrclvpd_rref, e0, hlrclvpd_alpha_se, hlrclvpd_beta_se, hlrclvpd_k_se, hlrclvpd_rref_se, e0_se])\n",
    "\n",
    "                if params[j, 2, i] == 0:\n",
    "                    whichmodel[j] = 1\n",
    "\n",
    "                p_cor[j, :, i] = numpy.array([hlrclvpd_cor_matrix[0][1], hlrclvpd_cor_matrix[0][2], hlrclvpd_cor_matrix[0][3], hlrclvpd_cor_matrix[1][2], hlrclvpd_cor_matrix[1][3], hlrclvpd_cor_matrix[2][3]])\n",
    "\n",
    "                rmse[j] = hlrclvpd_rmse\n",
    "\n",
    "                JTJ_inv[j, :, :] = numpy.copy(hlrclvpd_cov_matrix)\n",
    "\n",
    "                #### Check if parameter \"k\" is zero\n",
    "                if params[j, 2, i] == 0:\n",
    "                    JTJ_inv_temp = numpy.zeros((len(fguess) - 1, len(fguess) - 1), dtype=DOUBLE_PREC)\n",
    "                    whichmodel[j] = 1\n",
    "\n",
    "                    JTJ_inv_temp[0][0] = hlrclvpd_cov_matrix[0][0]\n",
    "                    JTJ_inv_temp[0][1] = hlrclvpd_cov_matrix[0][1]\n",
    "                    JTJ_inv_temp[1][0] = hlrclvpd_cov_matrix[1][0]\n",
    "                    JTJ_inv_temp[1][1] = hlrclvpd_cov_matrix[1][1]\n",
    "\n",
    "                    JTJ_inv_temp[0][2] = hlrclvpd_cov_matrix[0][3]\n",
    "                    JTJ_inv_temp[1][2] = hlrclvpd_cov_matrix[1][3]\n",
    "                    JTJ_inv_temp[2][2] = hlrclvpd_cov_matrix[3][3]\n",
    "                    JTJ_inv_temp[2][0] = hlrclvpd_cov_matrix[3][0]\n",
    "                    JTJ_inv_temp[2][1] = hlrclvpd_cov_matrix[3][1]\n",
    "\n",
    "                    JTJ_inv[j, :, :] = numpy.copy(JTJ_inv_temp)\n",
    "\n",
    "\n",
    "                #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                #;; check k, if less than zero estimate parameters without VPD effect      ;;\n",
    "                #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                if params[j, 2, i] < 0:\n",
    "                    '''\n",
    "                    print(\"****************************\")\n",
    "                    print(\"Starting HLRC_Lloyd\")\n",
    "                    print(\"****************************\")\n",
    "                    '''\n",
    "\n",
    "                    #hlrcl_status, hlrcl_alpha, hlrcl_beta, hlrcl_rref, hlrcl_alpha_se, hlrcl_beta_se, hlrcl_rref_se, hlrcl_residuals, hlrcl_cov_matrix, hlrcl_cor_matrix, hlrcl_rmse, hlrcl_ls_status = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair'], npara=3, xguess=numpy.array([fguess[0], fguess[1], fguess[3]]), mprior=numpy.array([fguess[0], fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([10, 600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                    #### Starting the optimization using the \"HLRC_Lloyd\" function\n",
    "                    hlrcl_results = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair'], npara=3, xguess=numpy.array([fguess[0], fguess[1], fguess[3]]), mprior=numpy.array([fguess[0], fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([10, 600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                    #### Setting the returned model parameters\n",
    "                    hlrcl_status = hlrcl_results['status']\n",
    "                    hlrcl_alpha = hlrcl_results['alpha']\n",
    "                    hlrcl_beta = hlrcl_results['beta']\n",
    "                    hlrcl_rref = hlrcl_results['rref']\n",
    "                    hlrcl_alpha_se = hlrcl_results['alpha_std_error']\n",
    "                    hlrcl_beta_se = hlrcl_results['beta_std_error']\n",
    "                    hlrcl_rref_se = hlrcl_results['rref_std_error']\n",
    "                    hlrcl_residuals = hlrcl_results['residuals']\n",
    "                    hlrcl_cov_matrix = hlrcl_results['cov_matrix']\n",
    "                    hlrcl_cor_matrix = hlrcl_results['cor_matrix']\n",
    "                    hlrcl_rmse = hlrcl_results['rmse']\n",
    "                    hlrcl_ls_status = hlrcl_results['ls_status']\n",
    "\n",
    "                    if hlrcl_cov_matrix is None or hlrcl_cor_matrix is None:\n",
    "                        raise ONEFluxPartitionBrokenOptError('HLRC_Lloyd', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "                    '''\n",
    "                    print(\"numpy.array([fguess[0], fguess[1], fguess[3]])\")\n",
    "                    print(numpy.array([fguess[0], fguess[1], fguess[3]]))\n",
    "                    print(\"hlrcl_alpha\")\n",
    "                    print(hlrcl_alpha)\n",
    "                    print(\"hlrcl_beta\")\n",
    "                    print(hlrcl_beta)\n",
    "                    print(\"hlrcl_rref\")\n",
    "                    print(hlrcl_rref)\n",
    "\n",
    "                    print(\"hlrcl_alpha_se\")\n",
    "                    print(hlrcl_alpha_se)\n",
    "                    print(\"hlrcl_beta_se\")\n",
    "                    print(hlrcl_beta_se)\n",
    "                    print(\"hlrcl_rref_se\")\n",
    "                    print(hlrcl_rref_se)\n",
    "\n",
    "                    print(\"====================\")\n",
    "                    print(\"hlrcl_residuals\")\n",
    "                    print(hlrcl_residuals)\n",
    "                    print(\"hlrcl_cov_matrix\")\n",
    "                    print(hlrcl_cov_matrix)\n",
    "                    print(\"hlrcl_cor_matrix\")\n",
    "                    print(hlrcl_cor_matrix)\n",
    "\n",
    "                    print(\"len(hlrcl_residuals)\")\n",
    "                    print(len(hlrcl_residuals))\n",
    "                    '''\n",
    "\n",
    "                    #### Specifying which model we chose for this iteration (j -> modified fguess)\n",
    "                    whichmodel[j] = 1\n",
    "\n",
    "                    res_cor[j] = (hlrcl_residuals ** 2).sum() / (len(hlrcl_residuals) * (1.0 - trimperc / 100.0) - 3)\n",
    "\n",
    "                    #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "                    params[j, :, i] = numpy.array([hlrcl_alpha, hlrcl_beta, 0, hlrcl_rref, e0, hlrcl_alpha_se, hlrcl_beta_se, 0, hlrcl_rref_se, e0_se])\n",
    "\n",
    "                    p_cor[j, :, i] = numpy.array([hlrcl_cor_matrix[0][1], NAN, hlrcl_cor_matrix[0][2], NAN, hlrcl_cor_matrix[1][2], NAN])\n",
    "\n",
    "                    rmse[j] = hlrcl_rmse\n",
    "\n",
    "                    JTJ_inv[j, 0:3, 0:3] = numpy.copy(hlrcl_cov_matrix)\n",
    "\n",
    "\n",
    "                    #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                    #;; check alpha, if less than zero estimate parameters with fixed alpha of last window and without VPD effect ;;\n",
    "                    #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                    if (params[j, 0, i] > 0.22) and i_ok > 0:\n",
    "                        if params_ok[0, i_ok - 1] > 0:\n",
    "                            alpha = params_ok[0, i_ok - 1]\n",
    "                            subd['alpha_1_from_tair'][:] = alpha\n",
    "                            ind[j, 1, i] = ind_ok[1, i_ok - 1]\n",
    "\n",
    "                            #hlrcl_status_afix, hlrcl_beta_afix, hlrcl_rref_afix, hlrcl_beta_se_afix, hlrcl_rref_se_afix, hlrcl_residuals_afix, hlrcl_cov_matrix_afix, hlrcl_cor_matrix_afix, hlrcl_rmse_afix, hlrcl_ls_status_afix = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'alpha_1_from_tair'], npara=2, xguess=numpy.array([fguess[1], fguess[3]]), mprior=numpy.array([fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                            #### Starting the optimization using the \"HLRC_Lloyd_afix\" function\n",
    "                            hlrcl_results_afix = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'alpha_1_from_tair'], npara=2, xguess=numpy.array([fguess[1], fguess[3]]), mprior=numpy.array([fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                            #### Setting the returned model parameters\n",
    "                            hlrcl_status_afix = hlrcl_results_afix['status']\n",
    "                            hlrcl_beta_afix = hlrcl_results_afix['beta']\n",
    "                            hlrcl_rref_afix = hlrcl_results_afix['rref']\n",
    "                            hlrcl_beta_se_afix = hlrcl_results_afix['beta_std_error']\n",
    "                            hlrcl_rref_se_afix = hlrcl_results_afix['rref_std_error']\n",
    "                            hlrcl_residuals_afix = hlrcl_results_afix['residuals']\n",
    "                            hlrcl_cov_matrix_afix = hlrcl_results_afix['cov_matrix']\n",
    "                            hlrcl_cor_matrix_afix = hlrcl_results_afix['cor_matrix']\n",
    "                            hlrcl_rmse_afix = hlrcl_results_afix['rmse']\n",
    "                            hlrcl_ls_status_afix = hlrcl_results_afix['ls_status']\n",
    "\n",
    "                            if hlrcl_cov_matrix_afix is None or hlrcl_cor_matrix_afix is None:\n",
    "                                raise ONEFluxPartitionBrokenOptError('HLRC_Lloyd_afix', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "                            '''\n",
    "                            print(\"numpy.array([fguess[1], fguess[3]])\")\n",
    "                            print(numpy.array([fguess[1], fguess[3]]))\n",
    "                            print(\"hlrcl_beta_afix\")\n",
    "                            print(hlrcl_beta_afix)\n",
    "                            print(\"hlrcl_rref_afix\")\n",
    "                            print(hlrcl_rref_afix)\n",
    "\n",
    "                            print(\"hlrcl_beta_se_afix\")\n",
    "                            print(hlrcl_beta_se_afix)\n",
    "                            print(\"hlrcl_rref_se_afix\")\n",
    "                            print(hlrcl_rref_se_afix)\n",
    "\n",
    "                            print(\"====================\")\n",
    "                            print(\"hlrcl_residuals_afix\")\n",
    "                            print(hlrcl_residuals_afix)\n",
    "                            print(\"hlrcl_cov_matrix_afix\")\n",
    "                            print(hlrcl_cov_matrix_afix)\n",
    "                            print(\"hlrcl_cor_matrix_afix\")\n",
    "                            print(hlrcl_cor_matrix_afix)\n",
    "\n",
    "                            print(\"len(hlrcl_residuals_afix)\")\n",
    "                            print(len(hlrcl_residuals_afix))\n",
    "                            '''\n",
    "\n",
    "                            #### Specifying which model we chose for this iteration (j -> modified fguess)\n",
    "                            whichmodel[j] = 2\n",
    "\n",
    "                            res_cor[j] = (hlrcl_residuals_afix ** 2).sum() / (len(hlrcl_residuals_afix) * (1.0 - trimperc / 100.0) - 2)\n",
    "\n",
    "                            #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "                            params[j, :, i] = numpy.array([alpha, hlrcl_beta_afix, 0, hlrcl_rref_afix, e0, NAN, hlrcl_beta_se_afix, 0, hlrcl_rref_se_afix, e0_se])\n",
    "\n",
    "                            p_cor[j, :, i] = numpy.array([NAN, NAN, NAN, NAN, hlrcl_cor_matrix_afix[0][1], NAN])\n",
    "\n",
    "                            rmse[j] = hlrcl_rmse_afix\n",
    "\n",
    "                            JTJ_inv[j, 0:2, 0:2] = numpy.copy(hlrcl_cov_matrix_afix)\n",
    "\n",
    "\n",
    "                #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                #;; check alpha, if gt 0.22 estimate parameters with fixed alpha of last window                   ;;\n",
    "                #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                elif (params[j, 0, i] > 0.22) and (i_ok > 0):\n",
    "                    if params_ok[0, i_ok - 1] > 0:\n",
    "                        alpha = params_ok[0, i_ok - 1]\n",
    "                        subd['alpha_1_from_tair'][:] = alpha\n",
    "                        ind[j, 1, i] = ind_ok[1, i_ok - 1]\n",
    "\n",
    "                        '''\n",
    "                        print(\"****************************\")\n",
    "                        print(\"Starting HLRC_LloydVPD_afix\")\n",
    "                        print(\"****************************\")\n",
    "                        '''\n",
    "                        #hlrclvpd_status_afix, hlrclvpd_beta_afix, hlrclvpd_k_afix, hlrclvpd_rref_afix, hlrclvpd_beta_se_afix, hlrclvpd_k_se_afix, hlrclvpd_rref_se_afix, hlrclvpd_residuals_afix, hlrclvpd_cov_matrix_afix, hlrclvpd_cor_matrix_afix, hlrclvpd_rmse_afix, hlrclvpd_ls_status_afix = nlinlts2(data=subd, lts_func=\"HLRC_LloydVPD_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'vpd_f', 'alpha_1_from_tair'], npara=3, xguess=numpy.array([fguess[1], fguess[2], fguess[3]]), mprior=numpy.array([fguess[1], fguess[2], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 50, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                        #### Starting the optimization using the \"HLRC_LloydVPD_afix\" function\n",
    "                        hlrclvpd_results = nlinlts2(data=subd, lts_func=\"HLRC_LloydVPD_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'vpd_f', 'alpha_1_from_tair'], npara=3, xguess=numpy.array([fguess[1], fguess[2], fguess[3]]), mprior=numpy.array([fguess[1], fguess[2], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 50, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                        #### Setting the returned model parameters\n",
    "                        hlrclvpd_status_afix = hlrclvpd_results['status']\n",
    "                        hlrclvpd_beta_afix = hlrclvpd_results['beta']\n",
    "                        hlrclvpd_k_afix = hlrclvpd_results['k']\n",
    "                        hlrclvpd_rref_afix = hlrclvpd_results['rref']\n",
    "                        hlrclvpd_beta_se_afix = hlrclvpd_results['beta_std_error']\n",
    "                        hlrclvpd_k_se_afix = hlrclvpd_results['k_std_error']\n",
    "                        hlrclvpd_rref_se_afix = hlrclvpd_results['rref_std_error']\n",
    "                        hlrclvpd_residuals_afix = hlrclvpd_results['residuals']\n",
    "                        hlrclvpd_cov_matrix_afix = hlrclvpd_results['cov_matrix']\n",
    "                        hlrclvpd_cor_matrix_afix = hlrclvpd_results['cor_matrix']\n",
    "                        hlrclvpd_rmse_afix = hlrclvpd_results['rmse']\n",
    "                        hlrclvpd_ls_status_afix = hlrclvpd_results['ls_status']\n",
    "\n",
    "                        if hlrclvpd_cov_matrix_afix is None or hlrclvpd_cor_matrix_afix is None:\n",
    "                            raise ONEFluxPartitionBrokenOptError('HLRC_LloydVPD_afix', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "                        '''\n",
    "                        print(\"numpy.array([fguess[1], fguess[2], fguess[3]])\")\n",
    "                        print(numpy.array([fguess[1], fguess[2], fguess[3]]))\n",
    "                        print(\"hlrclvpd_beta_afix\")\n",
    "                        print(hlrclvpd_beta_afix)\n",
    "                        print(\"hlrclvpd_k_afix\")\n",
    "                        print(hlrclvpd_k_afix)\n",
    "                        print(\"hlrclvpd_rref_afix\")\n",
    "                        print(hlrclvpd_rref_afix)\n",
    "\n",
    "                        print(\"hlrclvpd_beta_se_afix\")\n",
    "                        print(hlrclvpd_beta_se_afix)\n",
    "                        print(\"hlrclvpd_k_se_afix\")\n",
    "                        print(hlrclvpd_k_se_afix)\n",
    "                        print(\"hlrclvpd_rref_se_afix\")\n",
    "                        print(hlrclvpd_rref_se_afix)\n",
    "\n",
    "                        print(\"====================\")\n",
    "                        print(\"hlrclvpd_residuals_afix\")\n",
    "                        print(hlrclvpd_residuals_afix)\n",
    "                        print(\"hlrclvpd_cov_matrix_afix\")\n",
    "                        print(hlrclvpd_cov_matrix_afix)\n",
    "                        print(\"hlrclvpd_cor_matrix_afix\")\n",
    "                        print(hlrclvpd_cor_matrix_afix)\n",
    "\n",
    "                        print(\"len(hlrclvpd_residuals_afix)\")\n",
    "                        print(len(hlrclvpd_residuals_afix))\n",
    "                        '''\n",
    "\n",
    "                        #### Specifying which model we chose for this iteration (j -> modified fguess)\n",
    "                        whichmodel[j] = 3\n",
    "\n",
    "                        res_cor[j] = (hlrclvpd_residuals_afix ** 2).sum() / (len(hlrclvpd_residuals_afix) * (1.0 - trimperc / 100.0) - 3)\n",
    "\n",
    "                        #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "                        params[j, :, i] = numpy.array([alpha, hlrclvpd_beta_afix, hlrclvpd_k_afix, hlrclvpd_rref_afix, e0, 0, hlrclvpd_beta_se_afix, hlrclvpd_k_se_afix, hlrclvpd_rref_se_afix, e0_se])\n",
    "\n",
    "                        p_cor[j, :, i] = numpy.array([NAN, NAN, NAN, hlrclvpd_cor_matrix_afix[0][1], hlrclvpd_cor_matrix_afix[0][2], hlrclvpd_cor_matrix_afix[1][2]])\n",
    "\n",
    "                        rmse[j] = hlrclvpd_rmse_afix\n",
    "\n",
    "                        JTJ_inv[j, 0:3, 0:3] = numpy.copy(hlrclvpd_cov_matrix_afix)\n",
    "\n",
    "                        #### Check if parameter \"k\" is 0\n",
    "                        if params[j, 2, i] == 0:\n",
    "                            JTJ_inv_temp = numpy.zeros((len(fguess) - 1, len(fguess) - 1), dtype=DOUBLE_PREC)\n",
    "                            whichmodel[j] = 2\n",
    "\n",
    "                            JTJ_inv_temp[0][0] = hlrclvpd_cov_matrix_afix[0][0]\n",
    "                            JTJ_inv_temp[0][1] = hlrclvpd_cov_matrix_afix[2][0]\n",
    "                            JTJ_inv_temp[1][0] = hlrclvpd_cov_matrix_afix[0][2]\n",
    "                            JTJ_inv_temp[1][1] = hlrclvpd_cov_matrix_afix[2][2]\n",
    "\n",
    "                            JTJ_inv[j, :, :] = numpy.copy(JTJ_inv_temp)\n",
    "\n",
    "\n",
    "                        #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                        #;; check k, if less than zero estimate parameters without VPD effect and with fixed alpha of last window ;;\n",
    "                        #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                        if params[j, 2, i] < 0:\n",
    "                            '''\n",
    "                            print(\"****************************\")\n",
    "                            print(\"Starting HLRC_Lloyd_afix\")\n",
    "                            print(\"****************************\")\n",
    "                            '''\n",
    "                            #hlrcl_status_afix, hlrcl_beta_afix, hlrcl_rref_afix, hlrcl_beta_se_afix, hlrcl_rref_se_afix, hlrcl_residuals_afix, hlrcl_cov_matrix_afix, hlrcl_cor_matrix_afix, hlrcl_rmse_afix, hlrcl_ls_status_afix = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'alpha_1_from_tair'], npara=2, xguess=numpy.array([fguess[1], fguess[3]]), mprior=numpy.array([fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                            #### Starting the optimization using the \"HLRC_Lloyd_afix\" function\n",
    "                            hlrcl_results_afix = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'alpha_1_from_tair'], npara=2, xguess=numpy.array([fguess[1], fguess[3]]), mprior=numpy.array([fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                            #### Setting the returned model parameters\n",
    "                            hlrcl_status_afix = hlrcl_results_afix['status']\n",
    "                            hlrcl_beta_afix = hlrcl_results_afix['beta']\n",
    "                            hlrcl_rref_afix = hlrcl_results_afix['rref']\n",
    "                            hlrcl_beta_se_afix = hlrcl_results_afix['beta_std_error']\n",
    "                            hlrcl_rref_se_afix = hlrcl_results_afix['rref_std_error']\n",
    "                            hlrcl_residuals_afix = hlrcl_results_afix['residuals']\n",
    "                            hlrcl_cov_matrix_afix = hlrcl_results_afix['cov_matrix']\n",
    "                            hlrcl_cor_matrix_afix = hlrcl_results_afix['cor_matrix']\n",
    "                            hlrcl_rmse_afix = hlrcl_results_afix['rmse']\n",
    "                            hlrcl_ls_status_afix = hlrcl_results_afix['ls_status']\n",
    "\n",
    "                            if hlrcl_cov_matrix_afix is None or hlrcl_cor_matrix_afix is None:\n",
    "                                raise ONEFluxPartitionBrokenOptError('HLRC_Lloyd_afix', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "                            '''\n",
    "                            print(\"numpy.array([fguess[1], fguess[3]])\")\n",
    "                            print(numpy.array([fguess[1], fguess[3]]))\n",
    "                            print(\"hlrcl_beta_afix\")\n",
    "                            print(hlrcl_beta_afix)\n",
    "                            print(\"hlrcl_rref_afix\")\n",
    "                            print(hlrcl_rref_afix)\n",
    "\n",
    "                            print(\"hlrcl_beta_se_afix\")\n",
    "                            print(hlrcl_beta_se_afix)\n",
    "                            print(\"hlrcl_rref_se_afix\")\n",
    "                            print(hlrcl_rref_se_afix)\n",
    "\n",
    "                            print(\"====================\")\n",
    "                            print(\"hlrcl_residuals_afix\")\n",
    "                            print(hlrcl_residuals_afix)\n",
    "                            print(\"hlrcl_cov_matrix_afix\")\n",
    "                            print(hlrcl_cov_matrix_afix)\n",
    "                            print(\"hlrcl_cor_matrix_afix\")\n",
    "                            print(hlrcl_cor_matrix_afix)\n",
    "\n",
    "                            print(\"len(hlrcl_residuals_afix)\")\n",
    "                            print(len(hlrcl_residuals_afix))\n",
    "                            '''\n",
    "\n",
    "                            #### Specifying which model we chose for this iteration (modified fguess)\n",
    "                            whichmodel[j] = 2\n",
    "\n",
    "                            res_cor[j] = (hlrcl_residuals_afix ** 2).sum() / (len(hlrcl_residuals_afix) * (1.0 - trimperc / 100.0) - 2)\n",
    "\n",
    "                            #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "                            params[j, :, i] = numpy.array([alpha, hlrcl_beta_afix, 0, hlrcl_rref_afix, e0, 0, hlrcl_beta_se_afix, 0, hlrcl_rref_se_afix, e0_se])\n",
    "\n",
    "                            p_cor[j, :, i] = numpy.array([NAN, NAN, NAN, NAN, hlrcl_cor_matrix_afix[0][1], NAN])\n",
    "\n",
    "                            rmse[j] = hlrcl_rmse_afix\n",
    "\n",
    "                            JTJ_inv[j, 0:2, 0:2] = numpy.copy(hlrcl_cov_matrix_afix)\n",
    "\n",
    "\n",
    "                #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                #;; check if alpha or beta less than 0, if yes set to 0                                                 ;;\n",
    "                #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                if params[j, 0, i] < 0 or params[j, 1, i] < 0:\n",
    "                    '''\n",
    "                    print(\"****************************\")\n",
    "                    print(\"Starting LloydT_E0fix\")\n",
    "                    print(\"****************************\")\n",
    "                    '''\n",
    "                    #lt_status_e0fix, lt_rref_e0fix, lt_rref_se_e0fix, lt_residuals_e0fix, lt_cov_matrix_e0fix, lt_cor_matrix_e0fix, lt_rmse_e0fix, lt_ls_status_e0fix = nlinlts2(data=subd, lts_func=\"LloydT_E0fix\", depvar='nee_f', indepvar_arr=['tair_f', 'e0_1_from_tair'], npara=1, xguess=numpy.array([fguess[3]]), mprior=numpy.array([fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                    #### Starting the optimization using the \"LloydT_E0fix\" function\n",
    "                    lt_results_e0fix = nlinlts2(data=subd, lts_func=\"LloydT_E0fix\", depvar='nee_f', indepvar_arr=['tair_f', 'e0_1_from_tair'], npara=1, xguess=numpy.array([fguess[3]]), mprior=numpy.array([fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                    #### Setting the returned model parameters\n",
    "                    lt_status_e0fix = lt_results_e0fix['status']\n",
    "                    lt_rref_e0fix = lt_results_e0fix['rref']\n",
    "                    lt_rref_se_e0fix = lt_results_e0fix['rref_std_error']\n",
    "                    lt_residuals_e0fix = lt_results_e0fix['residuals']\n",
    "                    lt_cov_matrix_e0fix = lt_results_e0fix['cov_matrix']\n",
    "                    lt_cor_matrix_e0fix = lt_results_e0fix['cor_matrix']\n",
    "                    lt_rmse_e0fix = lt_results_e0fix['rmse']\n",
    "                    lt_ls_status_e0fix = lt_results_e0fix['ls_status']\n",
    "\n",
    "                    if lt_cov_matrix_e0fix is None or lt_cor_matrix_e0fix is None:\n",
    "                        raise ONEFluxPartitionBrokenOptError('LloydT_E0fix', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "                    '''\n",
    "                    print(\"numpy.array([fguess[3]])\")\n",
    "                    print(numpy.array([fguess[3]]))\n",
    "                    print(\"lt_rref_e0fix\")\n",
    "                    print(lt_rref_e0fix)\n",
    "\n",
    "                    print(\"lt_rref_se_e0fix\")\n",
    "                    print(lt_rref_se_e0fix)\n",
    "\n",
    "                    print(\"====================\")\n",
    "                    print(\"lt_residuals_e0fix\")\n",
    "                    print(lt_residuals_e0fix)\n",
    "                    print(\"lt_cov_matrix_e0fix\")\n",
    "                    print(lt_cov_matrix_e0fix)\n",
    "                    print(\"lt_cor_matrix_e0fix\")\n",
    "                    print(lt_cor_matrix_e0fix)\n",
    "\n",
    "                    print(\"len(lt_residuals_e0fix)\")\n",
    "                    print(len(lt_residuals_e0fix))\n",
    "                    '''\n",
    "\n",
    "                    #### Specifying which model we chose for this iteration (j -> modified fguess)\n",
    "                    whichmodel[j] = 4\n",
    "\n",
    "                    res_cor[j] = (lt_residuals_e0fix ** 2).sum() / (len(lt_residuals_e0fix) * (1.0 - trimperc / 100.0) - 1)\n",
    "\n",
    "                    #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "                    params[j, :, i] = numpy.array([0, 0, 0, lt_rref_e0fix, e0, 0, 0, 0, lt_rref_se_e0fix, e0_se])\n",
    "\n",
    "                    p_cor[j, :, i] = numpy.array([NAN, NAN, NAN, NAN, NAN, NAN])\n",
    "\n",
    "                    rmse[j] = lt_rmse_e0fix\n",
    "\n",
    "                    JTJ_inv[j, 0, 0] = numpy.copy(lt_cov_matrix_e0fix)\n",
    "\n",
    "\n",
    "                is_pars_ok = check_parameters(params=params[j, :, i], fguess=fguess)\n",
    "                if is_pars_ok == 0:\n",
    "                    rmse[j] = 9999.0\n",
    "\n",
    "            # end of \"for j\"\n",
    "\n",
    "        #### Find which iteration \"j\" that resulted in the most minimum rmse\n",
    "        jmin = numpy.where(rmse == numpy.min(numpy.abs(rmse)))\n",
    "        jmin = jmin[0]\n",
    "\n",
    "        '''\n",
    "        print(\"rmse\")\n",
    "        print(rmse)\n",
    "        print(\"numpy.abs(rmse)\")\n",
    "        print(numpy.abs(rmse))\n",
    "        print(\"numpy.min(numpy.abs(rmse))\")\n",
    "        print(numpy.min(numpy.abs(rmse)))\n",
    "        print(\"jmin\")\n",
    "        print(jmin)\n",
    "        print(\"jmin[0]\")\n",
    "        print(jmin[0])\n",
    "        '''\n",
    "\n",
    "        #### Check if the paramters chosen of the current set are valid\n",
    "        is_pars_ok = check_parameters(params=params[jmin[0], :, i], fguess=fguess)\n",
    "\n",
    "        '''\n",
    "        print(\"i\")\n",
    "        print(i)\n",
    "        print(\"is_pars_ok\")\n",
    "        print(is_pars_ok)\n",
    "\n",
    "        #if i == 12:\n",
    "        #    exit()\n",
    "        '''\n",
    "\n",
    "        #### This if statement is weird but it's in the pvwave code\n",
    "        if ind[jmin[0], 1, i] == 6616: # TODO: investigate and replace statement\n",
    "            msg = \"DT EXIT EXCEPTION: exact number of indices\"\n",
    "            print(msg)\n",
    "            raise ONEFluxPartitionError(msg)\n",
    "\n",
    "        #### If the current set of parameters is valid\n",
    "        #### then we choose it for the current window\n",
    "        if is_pars_ok == 1:\n",
    "            # my code\n",
    "            '''\n",
    "            if i >= 16 and i <= 28:\n",
    "                print(\"###***###***###***#\")\n",
    "                print(\"i\")\n",
    "                print(i)\n",
    "                print(\"i_ok\")\n",
    "                print(i_ok)\n",
    "                print(\"whichmodel[jmin[0]]\")\n",
    "                print(whichmodel[jmin[0]])\n",
    "                print(\"jmin\")\n",
    "                print(jmin)\n",
    "                print(\"ind[jmin[0], :, i]\")\n",
    "                print(ind[jmin[0], :, i])\n",
    "                print(\"###***###***###***#\")\n",
    "                #if i == 28:\n",
    "                #    exit()\n",
    "            '''\n",
    "            params_all_for_ranges['i'][i] = i\n",
    "            params_all_for_ranges['day'][i] = i * 4 + 1 - i * 2\n",
    "            params_all_for_ranges['i_ok'][i] = i_ok\n",
    "            params_all_for_ranges['alpha'][i] = params[jmin[0], 0, i]\n",
    "            params_all_for_ranges['beta'][i] = params[jmin[0], 1, i]\n",
    "            params_all_for_ranges['k'][i] = params[jmin[0], 2, i]\n",
    "            params_all_for_ranges['rref'][i] = params[jmin[0], 3, i]\n",
    "            params_all_for_ranges['e0'][i] = params[jmin[0], 4, i]\n",
    "            #end of code\n",
    "\n",
    "            params_ok[:, i_ok] = params[jmin[0], :, i]\n",
    "            ind_ok[:, i_ok] = ind[jmin[0], :, i]\n",
    "            p_cor_ok[:, i_ok] = p_cor[jmin[0], :, i]\n",
    "            whichmodel_ok[i_ok] = whichmodel[jmin[0]]\n",
    "            JTJ_inv_ok[i_ok, :, :] = JTJ_inv[jmin[0], :, :]\n",
    "            res_cor_ok[i_ok] = res_cor[jmin[0]]\n",
    "            i_ok = i_ok + 1\n",
    "\n",
    "        #### else this window won't work and we will use the\n",
    "        #### previous window\n",
    "        else:\n",
    "            # my code\n",
    "            '''\n",
    "            if i == 21:\n",
    "                print(\"###***###***###***\")\n",
    "                print(\"i\")\n",
    "                print(i)\n",
    "                print(\"i_nok\")\n",
    "                print(i_nok)\n",
    "                print(\"###***###***###***\")\n",
    "                #exit()\n",
    "            '''\n",
    "\n",
    "            params_all_for_ranges['i'][i] = i\n",
    "            params_all_for_ranges['day'][i] = i * 4 + 1 - i * 2\n",
    "            params_all_for_ranges['i_ok'][i] = -9999\n",
    "            params_all_for_ranges['alpha'][i] = -9999.0\n",
    "            params_all_for_ranges['beta'][i] = -9999.0\n",
    "            params_all_for_ranges['k'][i] = -9999.0\n",
    "            params_all_for_ranges['rref'][i] = -9999.0\n",
    "            params_all_for_ranges['e0'][i] = -9999.0\n",
    "            #end of code\n",
    "\n",
    "            params_nok[:, i_nok] = params[jmin[0], :, i]\n",
    "            i_nok = i_nok + 1\n",
    "\n",
    "        params_all[:, i] = params[jmin[0], :, i]\n",
    "        params_all_timestamp[i, :] = numpy.append([is_pars_ok, day_begin], numpy.transpose(params[jmin[0], :, i]))\n",
    "    # end of \"for i\"\n",
    "\n",
    "    #### Setting the final valid parameters to be returned\n",
    "    if i_ok > 0:\n",
    "        params_return = params_ok[:, 0:i_ok]\n",
    "        ind_return = ind_ok[:, 0:i_ok]\n",
    "        p_correl_return = p_cor_ok[:, 0:i_ok]\n",
    "        whichmodel_return = whichmodel_ok[0:i_ok]\n",
    "        JTJ_inv_return = JTJ_inv_ok[0:i_ok, :, :]\n",
    "        res_cor_return = res_cor_ok[0:i_ok]\n",
    "    else:\n",
    "        nan_arr = numpy.empty(len(fguess) + 3)\n",
    "        nan_arr.fill(NAN)\n",
    "        return nan_arr, None, None, None, None\n",
    "\n",
    "    # My code (not in pvwave)\n",
    "    i_ok_temp = 0\n",
    "    for i in range(n_parasets):\n",
    "        if params_all_for_ranges['i_ok'][i] >= 0:\n",
    "            if i_ok_temp == 0:\n",
    "                index_begin, index_end = 0, int(ind_return[2, i_ok_temp + 1])\n",
    "#                print(\"index [ 0]: \", index_begin, index_end)\n",
    "                params_all_for_ranges['ind_begin'][i] = index_begin\n",
    "                params_all_for_ranges['ind_end'][i] = index_end\n",
    "                params_all_for_ranges['subset_size'][i] = params_all_for_ranges['ind_end'][i] - params_all_for_ranges['ind_begin'][i]\n",
    "                ### populate variability (STD) for input data\n",
    "#                print(\"****STD [ 0]: \", index_begin, index_end, numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end]), numpy.nanstd(data['tair'][index_begin:index_end]), numpy.nanstd(data['rg'][index_begin:index_end]))\n",
    "                params_all_for_ranges['nee_avg'][i] = numpy.nanmean(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "                params_all_for_ranges['ta_avg'][i] = numpy.nanmean(data['tair'][index_begin:index_end])\n",
    "                params_all_for_ranges['rg_avg'][i] = numpy.nanmean(data['rg'][index_begin:index_end])\n",
    "                params_all_for_ranges['nee_std'][i] = numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "                params_all_for_ranges['ta_std'][i] = numpy.nanstd(data['tair'][index_begin:index_end])\n",
    "                params_all_for_ranges['rg_std'][i] = numpy.nanstd(data['rg'][index_begin:index_end])\n",
    "            elif i_ok_temp == (i_ok - 1):\n",
    "                index_begin, index_end = int(ind_return[2, i_ok_temp - 1]), int(numpy.max(data['ind']))\n",
    "#                print(\"index [-1]: \", index_begin, index_end)\n",
    "                params_all_for_ranges['ind_begin'][i] = index_begin\n",
    "                params_all_for_ranges['ind_end'][i] = index_end\n",
    "                params_all_for_ranges['subset_size'][i] = params_all_for_ranges['ind_end'][i] - params_all_for_ranges['ind_begin'][i]\n",
    "                ### populate variability (STD) for input data\n",
    "#                print(\"****STD [-1]: \", index_begin, index_end, numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end]), numpy.nanstd(data['tair'][index_begin:index_end]), numpy.nanstd(data['rg'][index_begin:index_end]))\n",
    "                params_all_for_ranges['nee_avg'][i] = numpy.nanmean(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "                params_all_for_ranges['ta_avg'][i] = numpy.nanmean(data['tair'][index_begin:index_end])\n",
    "                params_all_for_ranges['rg_avg'][i] = numpy.nanmean(data['rg'][index_begin:index_end])\n",
    "                params_all_for_ranges['nee_std'][i] = numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "                params_all_for_ranges['ta_std'][i] = numpy.nanstd(data['tair'][index_begin:index_end])\n",
    "                params_all_for_ranges['rg_std'][i] = numpy.nanstd(data['rg'][index_begin:index_end])\n",
    "\n",
    "            elif i_ok_temp >= i_ok:\n",
    "                index_begin, index_end = -9999, -9999\n",
    "#                print(\"index [>=]: \", index_begin, index_end)\n",
    "                params_all_for_ranges['ind_begin'][i] = index_begin\n",
    "                params_all_for_ranges['ind_end'][i] = index_end\n",
    "                params_all_for_ranges['subset_size'][i] = -9999\n",
    "                ### populate variability (STD) for input data\n",
    "#                print(\"****STD [>=]: \", index_begin, index_end, numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end]), numpy.nanstd(data['tair'][index_begin:index_end]), numpy.nanstd(data['rg'][index_begin:index_end]))\n",
    "                params_all_for_ranges['nee_avg'][i] = NAN\n",
    "                params_all_for_ranges['ta_avg'][i] = NAN\n",
    "                params_all_for_ranges['rg_avg'][i] = NAN\n",
    "                params_all_for_ranges['nee_std'][i] = NAN\n",
    "                params_all_for_ranges['ta_std'][i] = NAN\n",
    "                params_all_for_ranges['rg_std'][i] = NAN\n",
    "            else:\n",
    "                index_begin, index_end = int(ind_return[2, i_ok_temp - 1]), int(ind_return[2, i_ok_temp + 1])\n",
    "#                print(\"index [el]: \", index_begin, index_end)\n",
    "                params_all_for_ranges['ind_begin'][i] = index_begin\n",
    "                params_all_for_ranges['ind_end'][i] = index_end\n",
    "                params_all_for_ranges['subset_size'][i] = params_all_for_ranges['ind_end'][i] - params_all_for_ranges['ind_begin'][i]\n",
    "                ### populate variability (STD) for input data\n",
    "#                print(\"****STD [el]: \", index_begin, index_end, numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end]), numpy.nanstd(data['tair'][index_begin:index_end]), numpy.nanstd(data['rg'][index_begin:index_end]))\n",
    "                params_all_for_ranges['nee_avg'][i] = numpy.nanmean(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "                params_all_for_ranges['ta_avg'][i] = numpy.nanmean(data['tair'][index_begin:index_end])\n",
    "                params_all_for_ranges['rg_avg'][i] = numpy.nanmean(data['rg'][index_begin:index_end])\n",
    "                params_all_for_ranges['nee_std'][i] = numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "                params_all_for_ranges['ta_std'][i] = numpy.nanstd(data['tair'][index_begin:index_end])\n",
    "                params_all_for_ranges['rg_std'][i] = numpy.nanstd(data['rg'][index_begin:index_end])\n",
    "            i_ok_temp = i_ok_temp + 1\n",
    "        else:\n",
    "            params_all_for_ranges['ind_begin'][i] = -9999\n",
    "            params_all_for_ranges['ind_end'][i] = -9999\n",
    "            params_all_for_ranges['subset_size'][i] = -9999\n",
    "\n",
    "    var_names = 'alpha,beta,k,rref,e0,alpha_se,beta_se,k_se,rref_se,e0_se'\n",
    "    var_names_timestamp = 'ok,day_begin,alpha,beta,k,rref,e0,alpha_se,beta_se,k_se,rref_se,e0_se'\n",
    "\n",
    "    var_names_index = 'alpha,beta,k,rref,e0,alpha_se,beta_se,k_se,rref_se,e0_se,index1,index2,index3'\n",
    "\n",
    "    #numpy.savetxt('test_es_python_before.csv', numpy.transpose(params_return), delimiter=',', fmt='%s')\n",
    "    #numpy.savetxt('test_es_params_all_python.csv', numpy.transpose(params_all), delimiter=',', fmt='%s')\n",
    "    #numpy.savetxt('test_es_params_all_python.csv', numpy.transpose(params_all), delimiter=',', header=var_names, fmt='%s')\n",
    "    #numpy.savetxt('test_es_params_all_timestamp_python.csv', params_all_timestamp, delimiter=',', fmt='%s')\n",
    "    #numpy.savetxt('test_es_params_all_timestamp_python.csv', params_all_timestamp, delimiter=',', header=var_names_timestamp, fmt='%s')\n",
    "\n",
    "    #numpy.savetxt('test_es_params_index_all_timestamp_python.csv', numpy.transpose(numpy.concatenate((params_all, ind_ok), axis=0)), delimiter=',', header=var_names_index, fmt='%s')\n",
    "\n",
    "    filename_range = 'nee_' + ustar_type + '_' + str(percentile_num) + '_' + site_id + '_' + str(year) + '_params_after_es_python.csv'\n",
    "    numpy.savetxt(os.path.join(dt_output_dir, filename_range), params_all_for_ranges, delimiter=',', header=','.join(params_all_for_ranges.dtype.names), fmt='%s')\n",
    "    #exit()\n",
    "    # end of code\n",
    "\n",
    "    print(\"Finished estimate_parasets of daytime for nee_{u}_{p}_{s}_{y}\".format(u=ustar_type, p=percentile_num, s=site_id, y=year))\n",
    "\n",
    "    return numpy.concatenate((params_return, ind_return), axis=0), whichmodel_return, JTJ_inv_return, res_cor_return, p_correl_return\n",
    "    #end of estimate_parasets\n",
    "\n",
    "def percentiles_fn(data, columns, values=[0.0, 0.25, 0.5, 0.75, 1.0], remove_missing=False):\n",
    "    \"\"\"\n",
    "    Task:   Get the data values corresponding to the percentile chosen at\n",
    "            the \"values\" (array of percentiles) after sorting the data.\n",
    "\n",
    "            return -1 if no data was found\n",
    "\n",
    "    :param data: data structure for partitioning\n",
    "    :type data: numpy.ndarray\n",
    "    :param columns: columns or variable names of the data to be used\n",
    "    :type columns: str array\n",
    "    :param values: percentile values to be processed\n",
    "    :type values: float array\n",
    "    :param remove_missing: flag to remove missing values\n",
    "    :type remove_missing: boolean\n",
    "    \"\"\"\n",
    "    result = -1\n",
    "    n_elements = data[columns[0]].shape[0]\n",
    "\n",
    "    if n_elements <= 0:\n",
    "        return result\n",
    "\n",
    "    if remove_missing:\n",
    "        data = nomi(data, columns)\n",
    "\n",
    "    n_elements = data[columns[0]].shape[0]\n",
    "\n",
    "    values = numpy.array(values)\n",
    "    if max(values) > 1.0:\n",
    "        values = values * 0.01\n",
    "\n",
    "    #### Get an array of indices of the sorted data\n",
    "    sorted_index_arr = numpy.argsort(data[columns[0]])\n",
    "\n",
    "    ind = None\n",
    "    #### Iterate through each percentile and get the corresponding\n",
    "    #### value at that percentile of the sorted data\n",
    "    for i in range(len(values)):\n",
    "        if (values[i] < 0.0) or (values[i] > 1.0):\n",
    "            return -1\n",
    "\n",
    "        #### Setting ind to the percentile wanted\n",
    "        if values[i] <= 0.5:\n",
    "            ind = int(values[i] * n_elements)\n",
    "        else:\n",
    "            ind = int(values[i] * (n_elements + 1))\n",
    "\n",
    "        if ind >= n_elements:\n",
    "            ind = n_elements - int(1)\n",
    "\n",
    "        if i == 0:\n",
    "            result = data[columns[0]][sorted_index_arr[ind]]\n",
    "        else:\n",
    "            result = numpy.append(result, data[columns[0]][sorted_index_arr[ind]])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fa5f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncert_via_gapFill(data, var, del_flag=False , nomsg=False, maxMissFrac=1.0, longestMarginalgap=60):\n",
    "    \"\"\"\n",
    "    :Task: fill gaps of the chosen varname or column (for day time)\n",
    "\n",
    "    :Explanation:   We fill the gaps by picking all the nulls in a certain window and\n",
    "                    average the non gapped values in that window to fill the gaps in \n",
    "                    that window with the calculated average value.\n",
    "                    We do this 6 times. Each time we cover certain conditions to fill\n",
    "                    the gaps with (e.g TA_TOLERANCE, etc).\n",
    "    \n",
    "    :param data: data structure for partitioning\n",
    "    :type data: numpy.ndarray\n",
    "    \"\"\"\n",
    "    print(\"Starting uncert_gap_fill of daytime\")\n",
    "\n",
    "    # Setting defaults for Rg, Ta and VPD\n",
    "    RG_TOLERANCE = 50.0\n",
    "    TA_TOLERANCE = 2.5\n",
    "    VPD_TOLERANCE = 5.0\n",
    "    # window size\n",
    "    t_window_orig = 14\n",
    "    t_window = t_window_orig\n",
    "\n",
    "    var = var.lower()\n",
    "\n",
    "    # checking if data is hourly or half hourly data\n",
    "    nperday = 24\n",
    "    if data['hr'][1] - data['hr'][0] < 0.9:\n",
    "        nperday = 48\n",
    "\n",
    "    # initiating arrays to fill the gaps\n",
    "    rg = numpy.copy(data['rg'])\n",
    "    ta = numpy.copy(data['tair'])\n",
    "    vpd = numpy.copy(data['vpd'])\n",
    "    hr = numpy.copy(data['hr'].astype(float))\n",
    "    tofill = numpy.copy(data[var])\n",
    "\n",
    "    n = tofill.size\n",
    "    filled_val = numpy.empty(n)\n",
    "    filled_val.fill(NAN)\n",
    "\n",
    "    filled_n = numpy.empty(n)\n",
    "    filled_n.fill(NAN)\n",
    "\n",
    "    filled_s = numpy.empty(n)\n",
    "    filled_s.fill(NAN)\n",
    "\n",
    "    filled_srob = numpy.empty(n)\n",
    "    filled_srob.fill(NAN)\n",
    "\n",
    "    filled_med = numpy.empty(n)\n",
    "    filled_med.fill(NAN)\n",
    "\n",
    "    fillMethod = numpy.zeros(n)\n",
    "    fillWindow = numpy.zeros(n)\n",
    "    tofill_orig = numpy.copy(tofill)\n",
    "    tofill[:] = NAN\n",
    "\n",
    "    largemarginGap = numpy.zeros(n)\n",
    "    nnn = tofill.size\n",
    "    oookkk = tofill_orig > NAN_TEST\n",
    "    count = oookkk.sum()\n",
    "\n",
    "    # \"where\" return 2 arrays, I am getting the 1st one with the indices\n",
    "    # of the non null values\n",
    "    oookkk = numpy.where(tofill_orig > NAN_TEST)[0]\n",
    "    if oookkk.size == 0:\n",
    "        firstvalid = -1\n",
    "        lastvalid = -1\n",
    "    else:\n",
    "        firstvalid = numpy.amin(oookkk)\n",
    "        lastvalid = numpy.amax(oookkk)\n",
    "\n",
    "    # Checking if the consecutive gaps size in the beginning is too large\n",
    "    if firstvalid > (48 * longestMarginalgap):\n",
    "        largemarginGap[:(firstvalid + 1 - (48 * longestMarginalgap))] = 1\n",
    "    # Checking if the consecutive gaps size in the end is too large\n",
    "    if lastvalid < (nnn - (48 * longestMarginalgap)):\n",
    "        largemarginGap[(lastvalid + (48 * longestMarginalgap)):] = 1\n",
    "\n",
    "    def finalize_results():\n",
    "        \"\"\"\n",
    "        internal function to append the gap filled variables to the main data array\n",
    "        \"\"\"\n",
    "        print(\"uncert_gap_fill: Finalize Results\")\n",
    "        if del_flag:\n",
    "            data[var][:] = filled_val\n",
    "        else:\n",
    "            add_empty_vars(data=data, records=filled_val, column=str(var + \"_f_unc\"))\n",
    "\n",
    "        add_empty_vars(data=data, records=fillMethod, column=str(var + \"_fmet_unc\"))\n",
    "        add_empty_vars(data=data, records=fillWindow, column=str(var + \"_fwin_unc\"))\n",
    "        add_empty_vars(data=data, records=filled_n, column=str(var + \"_fn_unc\"))\n",
    "        add_empty_vars(data=data, records=filled_s, column=str(var + \"_fs_unc\"))\n",
    "        add_empty_vars(data=data, records=filled_srob, column=str(var + \"_fsrob_unc\"))\n",
    "        add_empty_vars(data=data, records=filled_med, column=str(var + \"_fmed_unc\"))\n",
    "\n",
    "        #toogapy = ((fillMethod == 1) & (fillWindow > 28)) | ((fillMethod == 2) & (fillWindow > 14)) | (fillMethod == 3)\n",
    "\n",
    "        fillQC = (fillMethod > 0).astype(int) + \\\n",
    "                (((fillMethod == 1) & (fillWindow > 14)) | ((fillMethod == 2) & (fillWindow > 14)) | ((fillMethod == 3) & (fillWindow > 1))).astype(int) + \\\n",
    "                (((fillMethod == 1) & (fillWindow > 56)) | ((fillMethod == 2) & (fillWindow > 28)) | ((fillMethod == 3) & (fillWindow > 5))).astype(int)\n",
    "\n",
    "        add_empty_vars(data=data, records=fillQC, column=str(var + \"_fqc_unc\"))\n",
    "        add_empty_vars(data=data, records=(fillQC <= 1).astype(int), column=str(var + \"_fqcok_unc\"))\n",
    "\n",
    "\n",
    "    # Check if gap percentage is too big\n",
    "    if (float(count) / n) < (1.0 - maxMissFrac):\n",
    "        filled_val = tofill_orig\n",
    "        fillMethod[:] = 4\n",
    "        fillWindow[:] = 9999.0\n",
    "        filled_n[:] = NAN\n",
    "        filled_s[:] = NAN\n",
    "        finalize_results()\n",
    "        return\n",
    "    else:\n",
    "        it_num = 0\n",
    "\n",
    "        # Filling using meteorological look-up (Rg, Tair, VPD), window <=28 days (Cat. A)\n",
    "        print(\"uncert_gap_fill: Starting loop #1\")\n",
    "        while True:\n",
    "            #### Getting the indices of the nulls that are between\n",
    "            #### the firstvalid and lastvalid (if the consecutive gaps are too big at both ends)\n",
    "            ko = numpy.where((tofill < NAN_TEST) & (largemarginGap == 0))[0]\n",
    "            count = len(ko)\n",
    "\n",
    "            t_window = (it_num + 1) * t_window_orig\n",
    "\n",
    "            #### Check if there are no gaps\n",
    "            if count == 0:\n",
    "                finalize_results()\n",
    "                return\n",
    "\n",
    "            #### Iterate through each index that needs to be filled\n",
    "            for index in ko:\n",
    "                #### w: Window of gaps to be covered\n",
    "                w = numpy.append(index - numpy.arange(t_window / 2.0 * nperday), index + numpy.arange(t_window / 2.0 * nperday - 1) + 1)\n",
    "                #### Clip all the indices in the window to be confined to the limits\n",
    "                numpy.clip(w, 0, n - 1, out=w)\n",
    "                w = w.astype(int)\n",
    "\n",
    "                #### Get all the indices of the non gapped values for averaging\n",
    "                ok4avg = numpy.where(tofill_orig[w] > NAN_TEST)[0]\n",
    "                count = len(ok4avg)\n",
    "\n",
    "                #### We need more than 9 non gapped values to be able to continue\n",
    "                #### this process of averaging\n",
    "                if count > 9:\n",
    "                    #### Get all the non gapped values only\n",
    "                    w = w[ok4avg]\n",
    "                    #### Get all the indices of the non gapped values that\n",
    "                    #### fit a certain condition or limits (e.g TA_TOLERANCE, etc)\n",
    "                    ok4avg = numpy.where((abs(ta[w] - ta[index]) < TA_TOLERANCE) &\n",
    "                                        (abs(rg[w] - rg[index]) < max(min(RG_TOLERANCE, rg[index]), 20)) &\n",
    "                                        (abs(vpd[w] - vpd[index]) < VPD_TOLERANCE) &\n",
    "                                        (rg[w] > NAN_TEST) &\n",
    "                                        (vpd[w] > NAN_TEST) &\n",
    "                                        (ta[w] > NAN_TEST))[0]\n",
    "                    count2 = len(ok4avg)\n",
    "\n",
    "\n",
    "                    # ok4avg = numpy.where((abs(ta[w] - ta[index]) < TA_TOLERANCE) &\n",
    "                    #                     (~numpy.isclose(abs(ta[w] - ta[index]), TA_TOLERANCE, rtol=1e-07, atol=0.0)) &\n",
    "                    #                     (abs(rg[w] - rg[index]) < max(min(RG_TOLERANCE, rg[index]), 20)) &\n",
    "                    #                     (~numpy.isclose(abs(rg[w] - rg[index]), max(min(RG_TOLERANCE, rg[index]), 20), rtol=1e-07, atol=0.0)) &\n",
    "                    #                     (abs(vpd[w] - vpd[index]) < VPD_TOLERANCE) &\n",
    "                    #                     (~numpy.isclose(abs(vpd[w] - vpd[index]), VPD_TOLERANCE, rtol=1e-07, atol=0.0)) &\n",
    "                    #                     (rg[w] > NAN_TEST) &\n",
    "                    #                     (vpd[w] > NAN_TEST) &\n",
    "                    #                     (ta[w] > NAN_TEST))[0]\n",
    "\n",
    "                    #### Still checking if we have more than 9 non gapped values\n",
    "                    if count2 > 9:\n",
    "                        #### Get all the stats related to those non gapped values\n",
    "                        mean_value = stats.tmean(tofill_orig[w[ok4avg]])\n",
    "                        counts_value = tofill_orig[w[ok4avg]].size\n",
    "                        std_value = stats.tstd(tofill_orig[w[ok4avg]])\n",
    "                        median_value = numpy.median(tofill_orig[w[ok4avg]])\n",
    "                        srob_value = robust.scale.mad(tofill_orig[w[ok4avg]])\n",
    "\n",
    "                        #### Fill the gaps with the mean of the non gapped values\n",
    "                        #### and save the other stats in new columns\n",
    "                        filled_val[index] = mean_value\n",
    "                        filled_n[index] = counts_value\n",
    "                        filled_s[index] = std_value\n",
    "                        filled_med[index] = median_value\n",
    "                        filled_srob[index] = srob_value\n",
    "                        fillMethod[index] = 1\n",
    "                        fillWindow[index] = (it_num + 1) * t_window_orig\n",
    "\n",
    "            #### Update tofill with all the newly filled indices\n",
    "            tofill[:] = filled_val\n",
    "            it_num = it_num + 1\n",
    "            if it_num > 1: break\n",
    "        # End of while loop\n",
    "\n",
    "        t_window = t_window_orig\n",
    "\n",
    "        #### Filling with meteorological drivers LUT (only Rg)\n",
    "        #### Filling using meteoroligical look-up (Rg only), window <=14 days (Cat. A)\n",
    "        tofill[:] = filled_val\n",
    "\n",
    "        #pvwave_file_path = '../pvwave_NEE_f_1.csv'\n",
    "        #file_basename = 'after_loop_1_1999_y'\n",
    "        #var_name = 'NEE_fqc_unc'\n",
    "        #compare_results_pv_py(py_data=data, pvwave_file_path=pvwave_file_path, var=var_name, file_basename=file_basename, single_array=fillMethod, save_csv=True, show_diff_index=True, show_diff_thresh=0.5)\n",
    "        #exit()\n",
    "\n",
    "        it_num = 0\n",
    "\n",
    "        print(\"uncert_gap_fill: Starting loop #2\")\n",
    "        while True:\n",
    "            t_window = (it_num + 1) * t_window_orig\n",
    "            #### Getting the indices of the nulls that are between\n",
    "            #### the firstvalid and lastvalid (if the consecutive gaps are too big at both ends)\n",
    "            ko = numpy.where((tofill < NAN_TEST) & (largemarginGap == 0))[0]\n",
    "            count = len(ko)\n",
    "\n",
    "            #### Check if there are no gaps\n",
    "            if count == 0:\n",
    "                finalize_results()\n",
    "                return\n",
    "\n",
    "            #### Iterate through each index that needs to be filled\n",
    "            for index in ko:\n",
    "                #### w: Window of gaps to be covered\n",
    "                w = numpy.append(index - numpy.arange(t_window / 2.0 * nperday), index + numpy.arange(t_window / 2.0 * nperday - 1) + 1)\n",
    "                #### Clip all the indices in the window to be confined to the limits\n",
    "                numpy.clip(w, 0, n - 1, out=w)\n",
    "                w = w.astype(int)\n",
    "\n",
    "                #### Get all the indices of the non gapped values for averaging.\n",
    "                #### Also get all the indices of the non gapped values that\n",
    "                #### fit a certain condition or limits (e.g TA_TOLERANCE, etc).\n",
    "                ok4avg = numpy.where((abs(rg[w] - rg[index]) < max(min(RG_TOLERANCE, rg[index]), 20)) &\n",
    "                                    (tofill_orig[w] > NAN_TEST) &\n",
    "                                    (rg[w] > NAN_TEST))[0]\n",
    "\n",
    "                '''ok4avg = numpy.where((abs(rg[w] - rg[index]) < max(min(RG_TOLERANCE, rg[index]), 20)) &\n",
    "                                    (~numpy.isclose(abs(rg[w] - rg[index]), max(min(RG_TOLERANCE, rg[index]), 20), rtol=1e-07, atol=0.0)) &\n",
    "                                    (tofill_orig[w] > NAN_TEST) &\n",
    "                                    (rg[w] > NAN_TEST))[0]\n",
    "                                    '''\n",
    "                count = len(ok4avg)\n",
    "\n",
    "                #### We need more than 9 non gapped values to be able to continue\n",
    "                #### this process of averaging\n",
    "                if count > 9:\n",
    "                    #### Get all the stats related to those non gapped values\n",
    "                    mean_value = stats.tmean(tofill_orig[w[ok4avg]])\n",
    "                    counts_value = tofill_orig[w[ok4avg]].size\n",
    "                    std_value = stats.tstd(tofill_orig[w[ok4avg]])\n",
    "                    median_value = numpy.median(tofill_orig[w[ok4avg]])\n",
    "                    srob_value = robust.scale.mad(tofill_orig[w[ok4avg]])\n",
    "\n",
    "                    #### Fill the gaps with the mean of the non gapped values\n",
    "                    #### and save the other stats in new columns\n",
    "                    filled_val[index] = mean_value\n",
    "                    filled_n[index] = counts_value\n",
    "                    filled_s[index] = std_value\n",
    "                    filled_med[index] = median_value\n",
    "                    filled_srob[index] = srob_value\n",
    "                    fillMethod[index] = 2\n",
    "                    fillWindow[index] = (it_num + 1) * t_window_orig\n",
    "\n",
    "            #### Update tofill with all the newly filled indices\n",
    "            tofill[:] = filled_val\n",
    "            it_num = it_num + 1\n",
    "            if it_num > 0: break\n",
    "        # End of while loop\n",
    "\n",
    "        t_window_orig_half = 1\n",
    "\n",
    "        #### still missing values then fill with average diurnal values, and increase time_window until all is filled\n",
    "        #### Still missing values filled with average diurnal values +-1 hour, window 1 day (Cat. A)\n",
    "        tofill[:] = filled_val\n",
    "        it_num = 0\n",
    "\n",
    "        #pvwave_file_path = '../pvwave_NEE_f_2.csv'\n",
    "        #file_basename = 'after_loop_2_1999_y'\n",
    "        #var_name = 'NEE_fqc_unc'\n",
    "        #compare_results_pv_py(py_data=data, pvwave_file_path=pvwave_file_path, var=var_name, file_basename=file_basename, single_array=fillMethod, save_csv=True, show_diff_index=True, show_diff_thresh=0.5)\n",
    "        #exit()\n",
    "\n",
    "        print(\"uncert_gap_fill: Starting loop #3\")\n",
    "        while True:\n",
    "            #### Getting the indices of the nulls that are between\n",
    "            #### the firstvalid and lastvalid (if the consecutive gaps are too big at both ends)\n",
    "            ko = numpy.where((tofill < NAN_TEST) & (largemarginGap == 0))[0]\n",
    "            count = len(ko)\n",
    "\n",
    "            t_window = (2 * it_num + 1) * t_window_orig_half\n",
    "\n",
    "            #### Check if there are no gaps\n",
    "            if count == 0:\n",
    "                finalize_results()\n",
    "                return\n",
    "\n",
    "            #### Iterate through each index that needs to be filled\n",
    "            for index in ko:\n",
    "                #### w: Window of gaps to be covered\n",
    "                w = numpy.append(index - numpy.arange(t_window / 2.0 * nperday), index + numpy.arange(t_window / 2.0 * nperday - 1) + 1)\n",
    "                #### Clip all the indices in the window to be confined to the limits\n",
    "                numpy.clip(w, 0, n - 1, out=w)\n",
    "                w = w.astype(int)\n",
    "\n",
    "                #### Get all the indices of the non gapped values for averaging.\n",
    "                #### Also get all the indices of the non gapped values that\n",
    "                #### fit a certain condition or limits (e.g TA_TOLERANCE, etc).\n",
    "                ok4avg = numpy.where((abs(hr[w] - hr[index]) < 1.1) &\n",
    "                                    (tofill_orig[w] > NAN_TEST))[0]\n",
    "                count = len(ok4avg)\n",
    "\n",
    "                #### We need more than 9 non gapped values to be able to continue\n",
    "                #### this process of averaging\n",
    "                if count > 9:\n",
    "                    #### Get all the stats related to those non gapped values\n",
    "                    mean_value = stats.tmean(tofill_orig[w[ok4avg]])\n",
    "                    counts_value = tofill_orig[w[ok4avg]].size\n",
    "                    std_value = stats.tstd(tofill_orig[w[ok4avg]])\n",
    "                    median_value = numpy.median(tofill_orig[w[ok4avg]])\n",
    "                    srob_value = robust.scale.mad(tofill_orig[w[ok4avg]])\n",
    "\n",
    "                    #### Fill the gaps with the mean of the non gapped values\n",
    "                    #### and save the other stats in new columns\n",
    "                    filled_val[index] = mean_value\n",
    "                    filled_n[index] = counts_value\n",
    "                    filled_s[index] = std_value\n",
    "                    filled_med[index] = median_value\n",
    "                    filled_srob[index] = srob_value\n",
    "                    fillMethod[index] = 3\n",
    "                    fillWindow[index] = t_window\n",
    "\n",
    "            #### Update tofill with all the newly filled indices\n",
    "            tofill[:] = filled_val\n",
    "            it_num = it_num + 1\n",
    "            if it_num > 2: break\n",
    "        # End of while loop\n",
    "\n",
    "        #*********** Values that are filled until here are best filled, since itnum was always small\n",
    "        #*********** Except for method 3 where the qc=2 would already be reached\n",
    "        #**** now next iteration\n",
    "        #### Filling with meteorological drivers LUT (all met)\n",
    "        #### Filling using meteorological look-up (Rg, Tair, VPD), window >=28 days (Cat. B)\n",
    "        it_num = 2\n",
    "\n",
    "        #pvwave_file_path = '../pvwave_NEE_f_3.csv'\n",
    "        #file_basename = 'after_loop_3_1999_y'\n",
    "        #var_name = 'NEE_fqc_unc'\n",
    "        #compare_results_pv_py(py_data=data, pvwave_file_path=pvwave_file_path, var=var_name, file_basename=file_basename, single_array=fillMethod, save_csv=True, show_diff_index=True, show_diff_thresh=0.5)\n",
    "        #exit()\n",
    "\n",
    "        print(\"uncert_gap_fill: Starting loop #4\")\n",
    "        while True:\n",
    "            #### Getting the indices of the nulls that are between\n",
    "            #### the firstvalid and lastvalid (if the consecutive gaps are too big at both ends)\n",
    "            ko = numpy.where((tofill < NAN_TEST) & (largemarginGap == 0))[0]\n",
    "            count = len(ko)\n",
    "\n",
    "            t_window = (it_num + 1) * t_window_orig\n",
    "\n",
    "            #### Check if there are no gaps\n",
    "            if count == 0:\n",
    "                finalize_results()\n",
    "                return\n",
    "\n",
    "            #### Iterate through each index that needs to be filled\n",
    "            for index in ko:\n",
    "                #### w: Window of gaps to be covered\n",
    "                w = numpy.append(index - numpy.arange(t_window / 2.0 * nperday), index + numpy.arange(t_window / 2.0 * nperday - 1) + 1)\n",
    "                #### Clip all the indices in the window to be confined to the limits\n",
    "                numpy.clip(w, 0, n - 1, out=w)\n",
    "                w = w.astype(int)\n",
    "\n",
    "                #### Get all the indices of the non gapped values for averaging\n",
    "                ok4avg = numpy.where(tofill_orig[w] > NAN_TEST)[0]\n",
    "                count = len(ok4avg)\n",
    "\n",
    "                #### We need more than 9 non gapped values to be able to continue\n",
    "                #### this process of averaging\n",
    "                if count > 9:\n",
    "                    #### Get all the non gapped values only\n",
    "                    w = w[ok4avg]\n",
    "                    #### Get all the indices of the non gapped values that\n",
    "                    #### fit a certain condition or limits (e.g TA_TOLERANCE, etc)\n",
    "                    ok4avg = numpy.where((abs(ta[w] - ta[index]) < TA_TOLERANCE) &\n",
    "                                        (abs(rg[w] - rg[index]) < max(min(RG_TOLERANCE, rg[index]), 20)) &\n",
    "                                        (abs(vpd[w] - vpd[index]) < VPD_TOLERANCE) &\n",
    "                                        (rg[w] > NAN_TEST) &\n",
    "                                        (vpd[w] > NAN_TEST) &\n",
    "                                        (ta[w] > NAN_TEST))[0]\n",
    "\n",
    "                    '''ok4avg = numpy.where((abs(ta[w] - ta[index]) < TA_TOLERANCE) &\n",
    "                                        (~numpy.isclose(abs(ta[w] - ta[index]), TA_TOLERANCE, rtol=1e-07, atol=0.0)) &\n",
    "                                        (abs(rg[w] - rg[index]) < max(min(RG_TOLERANCE, rg[index]), 20)) &\n",
    "                                        (~numpy.isclose(abs(rg[w] - rg[index]), max(min(RG_TOLERANCE, rg[index]), 20), rtol=1e-07, atol=0.0)) &\n",
    "                                        (abs(vpd[w] - vpd[index]) < VPD_TOLERANCE) &\n",
    "                                        (~numpy.isclose(abs(vpd[w] - vpd[index]), VPD_TOLERANCE, rtol=1e-07, atol=0.0)) &\n",
    "                                        (rg[w] > NAN_TEST) &\n",
    "                                        (vpd[w] > NAN_TEST) &\n",
    "                                        (ta[w] > NAN_TEST))[0]\n",
    "                                        '''\n",
    "\n",
    "                    count2 = len(ok4avg)\n",
    "\n",
    "                    #### Still checking if we have more than 9 non gapped values\n",
    "                    if count2 > 9:\n",
    "                        #### Get all the stats related to those non gapped values\n",
    "                        mean_value = stats.tmean(tofill_orig[w[ok4avg]])\n",
    "                        counts_value = tofill_orig[w[ok4avg]].size\n",
    "                        std_value = stats.tstd(tofill_orig[w[ok4avg]])\n",
    "                        median_value = numpy.median(tofill_orig[w[ok4avg]])\n",
    "                        srob_value = robust.scale.mad(tofill_orig[w[ok4avg]])\n",
    "\n",
    "                        #### Fill the gaps with the mean of the non gapped values\n",
    "                        #### and save the other stats in new columns\n",
    "                        filled_val[index] = mean_value\n",
    "                        filled_n[index] = counts_value\n",
    "                        filled_s[index] = std_value\n",
    "                        filled_med[index] = median_value\n",
    "                        filled_srob[index] = srob_value\n",
    "                        fillMethod[index] = 1\n",
    "                        fillWindow[index] = (it_num + 1) * t_window_orig\n",
    "\n",
    "            #### Update tofill with all the newly filled indices\n",
    "            tofill[:] = filled_val\n",
    "            it_num = it_num + 1\n",
    "            if it_num > 10: break\n",
    "        # End of while loop\n",
    "\n",
    "        t_window = t_window_orig\n",
    "\n",
    "        #### Filling with meteorological drivers LUT (only Rg)\n",
    "        #### Filling using meteorological look-up (Rg only), window >=14 days (Cat. B)\n",
    "        tofill[:] = filled_val\n",
    "        it_num = 1\n",
    "\n",
    "        #pvwave_file_path = '../pvwave_NEE_f_4.csv'\n",
    "        #file_basename = 'after_loop_4_1999_y'\n",
    "        #var_name = 'NEE_fqc_unc'\n",
    "        #compare_results_pv_py(py_data=data, pvwave_file_path=pvwave_file_path, var=var_name, file_basename=file_basename, single_array=fillMethod, save_csv=True, show_diff_index=True, show_diff_thresh=0.5)\n",
    "        #exit()\n",
    "\n",
    "        print(\"uncert_gap_fill: Starting loop #5\")\n",
    "        while True:\n",
    "            t_window = (it_num + 1) * t_window_orig\n",
    "            #### Getting the indices of the nulls that are between\n",
    "            #### the firstvalid and lastvalid (if the consecutive gaps are too big at both ends)\n",
    "            ko = numpy.where((tofill < NAN_TEST) & (largemarginGap == 0))[0]\n",
    "            count = len(ko)\n",
    "\n",
    "            #### Check if there are no gaps\n",
    "            if count == 0:\n",
    "                finalize_results()\n",
    "                return\n",
    "\n",
    "            #### Iterate through each index that needs to be filled\n",
    "            for index in ko:\n",
    "                #### w: Window of gaps to be covered\n",
    "                w = numpy.append(index - numpy.arange(t_window / 2.0 * nperday), index + numpy.arange(t_window / 2.0 * nperday - 1) + 1)\n",
    "                #### Clip all the indices in the window to be confined to the limits\n",
    "                numpy.clip(w, 0, n - 1, out=w)\n",
    "                w = w.astype(int)\n",
    "\n",
    "                #### Get all the indices of the non gapped values for averaging.\n",
    "                #### Also get all the indices of the non gapped values that\n",
    "                #### fit a certain condition or limits (e.g TA_TOLERANCE, etc).\n",
    "                ok4avg = numpy.where((abs(rg[w] - rg[index]) < max(min(RG_TOLERANCE, rg[index]), 20)) &\n",
    "                                    (tofill_orig[w] > NAN_TEST) &\n",
    "                                    (rg[w] > NAN_TEST))[0]\n",
    "\n",
    "                '''ok4avg = numpy.where((abs(rg[w] - rg[index]) < max(min(RG_TOLERANCE, rg[index]), 20)) &\n",
    "                                    (~numpy.isclose(abs(rg[w] - rg[index]), max(min(RG_TOLERANCE, rg[index]), 20), rtol=1e-07, atol=0.0)) &\n",
    "                                    (tofill_orig[w] > NAN_TEST) &\n",
    "                                    (rg[w] > NAN_TEST))[0]'''\n",
    "                count = len(ok4avg)\n",
    "\n",
    "                #### We need more than 9 non gapped values to be able to continue\n",
    "                #### this process of averaging\n",
    "                if count > 9:\n",
    "                    #### Get all the stats related to those non gapped values\n",
    "                    mean_value = stats.tmean(tofill_orig[w[ok4avg]])\n",
    "                    counts_value = tofill_orig[w[ok4avg]].size\n",
    "                    std_value = stats.tstd(tofill_orig[w[ok4avg]])\n",
    "                    median_value = numpy.median(tofill_orig[w[ok4avg]])\n",
    "                    srob_value = robust.scale.mad(tofill_orig[w[ok4avg]])\n",
    "\n",
    "                    #### Fill the gaps with the mean of the non gapped values\n",
    "                    #### and save the other stats in new columns\n",
    "                    filled_val[index] = mean_value\n",
    "                    filled_n[index] = counts_value\n",
    "                    filled_s[index] = std_value\n",
    "                    filled_med[index] = median_value\n",
    "                    filled_srob[index] = srob_value\n",
    "                    fillMethod[index] = 2\n",
    "                    fillWindow[index] = (it_num + 1) * t_window_orig\n",
    "\n",
    "            #### Update tofill with all the newly filled indices\n",
    "            tofill[:] = filled_val\n",
    "            it_num = it_num + 1\n",
    "            if it_num > 10: break\n",
    "        # End of while loop\n",
    "\n",
    "        #**** still missing values then fill with average diurnal values, and increase time_window until all is filled\n",
    "        #if not KEYWORD_SET(nomsg) THEN msg, /inf, 'Still missing values filled with average diurnal values +-1 hour, window 7-210 days (Cat. C)'\n",
    "        t_window_orig_half = t_window_orig * 0.5\n",
    "        t_window = t_window_orig_half\n",
    "\n",
    "        tofill[:] = filled_val\n",
    "        it_num = 0\n",
    "\n",
    "        #pvwave_file_path = '../pvwave_NEE_f_5.csv'\n",
    "        #file_basename = 'after_loop_5_1999_y'\n",
    "        #var_name = 'NEE_fqc_unc'\n",
    "        #compare_results_pv_py(py_data=data, pvwave_file_path=pvwave_file_path, var=var_name, file_basename=file_basename, single_array=fillMethod, save_csv=True, show_diff_index=True, show_diff_thresh=0.5)\n",
    "        #exit()\n",
    "\n",
    "        print(\"uncert_gap_fill: Starting loop #6\")\n",
    "        while True:\n",
    "            #### Getting the indices of the nulls that are between\n",
    "            #### the firstvalid and lastvalid (if the consecutive gaps are too big at both ends)\n",
    "            ko = numpy.where((tofill < NAN_TEST) & (largemarginGap == 0))[0]\n",
    "            count = len(ko)\n",
    "\n",
    "            t_window = (it_num + 1) * t_window_orig_half\n",
    "\n",
    "            #### Check if there are no gaps\n",
    "            if count == 0:\n",
    "                finalize_results()\n",
    "                return\n",
    "\n",
    "            #### Iterate through each index that needs to be filled\n",
    "            for index in ko:\n",
    "                #### w: Window of gaps to be covered\n",
    "                w = numpy.append(index - numpy.arange(t_window / 2.0 * nperday), index + numpy.arange(t_window / 2.0 * nperday - 1) + 1)\n",
    "                #### Clip all the indices in the window to be confined to the limits\n",
    "                numpy.clip(w, 0, n - 1, out=w)\n",
    "                w = w.astype(int)\n",
    "\n",
    "                #### Get all the indices of the non gapped values for averaging.\n",
    "                #### Also get all the indices of the non gapped values that\n",
    "                #### fit a certain condition or limits (e.g TA_TOLERANCE, etc).\n",
    "                ok4avg = numpy.where((abs(hr[w] - hr[index]) < 1.1) &\n",
    "                                    (tofill_orig[w] > NAN_TEST))[0]\n",
    "                count = len(ok4avg)\n",
    "\n",
    "                #### We need more than 9 non gapped values to be able to continue\n",
    "                #### this process of averaging\n",
    "                if count > 9:\n",
    "                    #### Get all the stats related to those non gapped values\n",
    "                    mean_value = stats.tmean(tofill_orig[w[ok4avg]])\n",
    "                    counts_value = tofill_orig[w[ok4avg]].size\n",
    "                    std_value = stats.tstd(tofill_orig[w[ok4avg]])\n",
    "                    median_value = numpy.median(tofill_orig[w[ok4avg]])\n",
    "                    srob_value = robust.scale.mad(tofill_orig[w[ok4avg]])\n",
    "\n",
    "                    #### Fill the gaps with the mean of the non gapped values\n",
    "                    #### and save the other stats in new columns\n",
    "                    filled_val[index] = mean_value\n",
    "                    filled_n[index] = counts_value\n",
    "                    filled_s[index] = std_value\n",
    "                    filled_med[index] = median_value\n",
    "                    filled_srob[index] = srob_value\n",
    "                    fillMethod[index] = 3\n",
    "                    fillWindow[index] = t_window\n",
    "\n",
    "            #### Update tofill with all the newly filled indices\n",
    "            tofill[:] = filled_val\n",
    "            it_num = it_num + 1\n",
    "            if it_num > 60: break\n",
    "\n",
    "        # End of while loop\n",
    "\n",
    "        #pvwave_file_path = '../pvwave_NEE_f_6.csv'\n",
    "        #file_basename = 'after_loop_6_1999_y'\n",
    "        #var_name = 'NEE_fqc_unc'\n",
    "        #compare_results_pv_py(py_data=data, pvwave_file_path=pvwave_file_path, var=var_name, file_basename=file_basename, single_array=fillMethod, save_csv=True, show_diff_index=True, show_diff_thresh=0.5)\n",
    "\n",
    "        #print(\"After the 6 loops\")\n",
    "        finalize_results()\n",
    "\n",
    "        print(\"Finished uncert_gap_fill of daytime\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "981a5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flux(data, params, dt_output_dir, site_id, ustar_type, percentile_num, year):\n",
    "\n",
    "    print(\"Starting compute_flux of daytime for nee_{u}_{p}_{s}_{y}\".format(u=ustar_type, p=percentile_num, s=site_id, y=year))\n",
    "    filename_range = 'nee_' + ustar_type + '_' + str(percentile_num) + '_' + site_id + '_' + str(year) + '_params_after_es_python.csv'\n",
    "\n",
    "    n_params = len(params[:, 0])\n",
    "    n_parasets = len(params[0, :])\n",
    "    n_set = len(data['nee_f'])\n",
    "    Reco_mat = np.empty((n_parasets, n_set))\n",
    "    Reco_mat.fill(NAN)\n",
    "    Reco = np.zeros(n_set, dtype=FLOAT_PREC)\n",
    "    GPP_mat = np.empty((n_parasets, n_set))\n",
    "    GPP_mat.fill(NAN)\n",
    "    GPP = np.zeros(n_set, dtype=FLOAT_PREC)\n",
    "    partition_flag1 = np.zeros(n_set, dtype=FLOAT_PREC)\n",
    "    partition_flag2 = np.zeros(n_set, dtype=FLOAT_PREC)\n",
    "\n",
    "    reco_gpp_orig = np.zeros((10, n_set), dtype=FLOAT_PREC)\n",
    "\n",
    "    '''\n",
    "    print(\"n_params\")\n",
    "    print(n_params)\n",
    "    print(\"n_parasets\")\n",
    "    print(n_parasets)\n",
    "    print(\"n_set\")\n",
    "    print(n_set)\n",
    "    '''\n",
    "\n",
    "    #### We iterate over the \"okay\" parameters we got from estimate_params\n",
    "    for i in range(n_parasets):\n",
    "        sub = None\n",
    "\n",
    "        #### Getting the data in each window while keeping in mind\n",
    "        #### the position of each window. i.e we handle the data of\n",
    "        #### the first window and the last window.\n",
    "        if i == 0:\n",
    "            ind_begin = 0\n",
    "            ind_end = params[n_params - 1, i + 1]\n",
    "            sub_mask = ((data['ind'] >= ind_begin) & (data['ind'] < ind_end))\n",
    "            sub, _, _ = newselif(data=data, condition=sub_mask, drop=True)\n",
    "        elif i == (n_parasets - 1):\n",
    "            ind_begin = params[n_params - 1, i - 1]\n",
    "            ind_end = np.max(data['ind'])\n",
    "            sub_mask = ((data['ind'] >= ind_begin) & (data['ind'] <= ind_end))\n",
    "            sub, _, _ = newselif(data=data, condition=sub_mask, drop=True)\n",
    "        else:\n",
    "            ind_begin = params[n_params - 1, i - 1]\n",
    "            ind_end = params[n_params - 1, i + 1]\n",
    "            sub_mask = ((data['ind'] >= ind_begin) & (data['ind'] < ind_end))\n",
    "            sub, _, _ = newselif(data=data, condition=sub_mask, drop=True)\n",
    "\n",
    "            '''\n",
    "            #if i == 15:\n",
    "            if i >= 13 and i <= 24:\n",
    "                print(\"i\")\n",
    "                print(i)\n",
    "                print(\"ind_begin\")\n",
    "                print(ind_begin)\n",
    "                print(\"ind_end\")\n",
    "                print(ind_end)\n",
    "                print(\"rref\")\n",
    "                print(params[3, i])\n",
    "                print(\"e0\")\n",
    "                print(params[4, i])\n",
    "                print(\"sub['tair_f'].size\")\n",
    "                print(sub['tair_f'].size)\n",
    "                #if i == 24:\n",
    "                #    exit()\n",
    "            '''\n",
    "\n",
    "        #### We apply the best parameters we got on the right model\n",
    "        #### functions to estimate the Reco and GPP. We do this by fitting\n",
    "        #### the right paramters, applied at the data in each window.\n",
    "        Reco_mat[i, sub['ind'].astype(int)] = lloyd_taylor_dt(ta_f=sub['tair_f'], parameter=params[3:4 + 1, i])\n",
    "        GPP_mat[i, sub['ind'].astype(int)] = gpp_vpd(rg_f=sub['rg_f'], vpd_f=sub['vpd_f'], parameter=np.transpose(params[0:2 + 1, i]))\n",
    "\n",
    "        '''\n",
    "        if i == 15:\n",
    "            #print(\"sub['ind'][392]\")\n",
    "            #print(sub['ind'][392])\n",
    "            #print(\"sub['tair_f'][392]\")\n",
    "            #print(sub['tair_f'][392])\n",
    "            print(\"params[3:4+1, i]\")\n",
    "            print(params[3:4+1, i])\n",
    "            #print(\"Reco_mat[i, sub['ind'][392].astype(int)]\")\n",
    "            #print(Reco_mat[i, sub['ind'][392].astype(int)])\n",
    "        '''\n",
    "\n",
    "    #end for i\n",
    "\n",
    "\n",
    "    #### We iterate over each data point in the whole dataset.\n",
    "    #### i.e j could represent around 17,000 data points.\n",
    "    for j in range(n_set):\n",
    "\n",
    "        #### For each data-point \"j\", there could be up to two windows\n",
    "        #### that have covered this data-point \"j\". So we check how many\n",
    "        #### windows have covered it and save the number in count.\n",
    "        Reco_ind = np.where(Reco_mat[:, j] > NAN)[0]\n",
    "        count = len(Reco_ind)\n",
    "\n",
    "        #### If there are two windows covering this data-point \"j\", then we are going\n",
    "        #### to assign weights to each window and multiply the Reco and GPP\n",
    "        #### values we got previously with it's assigned weight. We get the\n",
    "        #### final Reco and GPP values by adding the multiplied values together.\n",
    "        if count > 1:\n",
    "            weight1 = (params[n_params - 1, Reco_ind[1]] - j) / (params[n_params - 1, Reco_ind[1]] - params[n_params - 1, Reco_ind[0]])\n",
    "            weight2 = (j - params[n_params - 1, Reco_ind[0]]) / (params[n_params - 1, Reco_ind[1]] - params[n_params - 1, Reco_ind[0]])\n",
    "            Reco[j] = Reco_mat[Reco_ind[0], j] * weight1 + Reco_mat[Reco_ind[1], j] * weight2\n",
    "            GPP[j] = GPP_mat[Reco_ind[0], j] * weight1 + GPP_mat[Reco_ind[1], j] * weight2\n",
    "            partition_flag1[j] = np.abs(params[n_params - 1, Reco_ind[1]] - j)\n",
    "            partition_flag2[j] = np.abs(j - params[n_params - 1, Reco_ind[0]])\n",
    "\n",
    "            # my code\n",
    "            reco_gpp_orig[0, j] = j\n",
    "            reco_gpp_orig[1, j] = data['year'][j]\n",
    "            reco_gpp_orig[2, j] = data['month'][j]\n",
    "            reco_gpp_orig[3, j] = data['day'][j]\n",
    "            reco_gpp_orig[4, j] = data['hr'][j]\n",
    "            reco_gpp_orig[5, j] = data['julday'][j]\n",
    "            reco_gpp_orig[6, j] = Reco_mat[Reco_ind[0], j]\n",
    "            reco_gpp_orig[7, j] = Reco_mat[Reco_ind[1], j]\n",
    "            reco_gpp_orig[8, j] = GPP_mat[Reco_ind[0], j]\n",
    "            reco_gpp_orig[9, j] = GPP_mat[Reco_ind[1], j]\n",
    "\n",
    "            '''\n",
    "            if j == 1952:\n",
    "                print(\"j\")\n",
    "                print(j)\n",
    "                print(\"count\")\n",
    "                print(count)\n",
    "                print(\"Reco_ind[0]\")\n",
    "                print(Reco_ind[0])\n",
    "                print(\"Reco_ind[1]\")\n",
    "                print(Reco_ind[1])\n",
    "                print(\"weight1\")\n",
    "                print(weight1)\n",
    "                print(\"weight2\")\n",
    "                print(weight2)\n",
    "                print(\"params[n_params - 1, Reco_ind[0]]\")\n",
    "                print(params[n_params - 1, Reco_ind[0]])\n",
    "                print(\"params[n_params - 1, Reco_ind[1]]\")\n",
    "                print(params[n_params - 1, Reco_ind[1]])\n",
    "                print(\"Reco_mat[Reco_ind[0], j]\")\n",
    "                print(Reco_mat[Reco_ind[0], j])\n",
    "                print(\"Reco_mat[Reco_ind[1], j]\")\n",
    "                print(Reco_mat[Reco_ind[1], j])\n",
    "                print(\"GPP_mat[Reco_ind[0], j]\")\n",
    "                print(GPP_mat[Reco_ind[0], j])\n",
    "                print(\"GPP_mat[Reco_ind[1], j]\")\n",
    "                print(GPP_mat[Reco_ind[1], j])\n",
    "                print(\"Reco[j]\")\n",
    "                print(Reco[j])\n",
    "                print(\"GPP[j]\")\n",
    "                print(GPP[j])\n",
    "\n",
    "                #if j == 1952:\n",
    "                #    exit()\n",
    "\n",
    "                #if j == 8840:\n",
    "                 #   exit()\n",
    "            '''\n",
    "\n",
    "                #end code\n",
    "\n",
    "        #### If there is only one window covering this data-point \"j\", then\n",
    "        #### we are going to assign the final Reco and GPP values\n",
    "        #### to the previously calculated Reco and GPP.\n",
    "        elif count == 1:\n",
    "            Reco[j] = Reco_mat[Reco_ind[0], j]\n",
    "            GPP[j] = GPP_mat[Reco_ind[0], j]\n",
    "            partition_flag1[j] = np.abs(params[n_params - 1, Reco_ind[0]] - j)\n",
    "\n",
    "            if Reco_ind[0] == 0:\n",
    "                partition_flag2[j] = j\n",
    "            else:\n",
    "                partition_flag2[j] = n_set - 1 - j\n",
    "\n",
    "            '''\n",
    "            if j == 5605:\n",
    "                print(\"j\")\n",
    "                print(j)\n",
    "                print(\"count\")\n",
    "                print(count)\n",
    "                print(\"Reco_ind[0]\")\n",
    "                print(Reco_ind[0])\n",
    "                print(\"Reco_mat[Reco_ind[0], j]\")\n",
    "                print(Reco_mat[Reco_ind[0], j])\n",
    "                print(\"GPP_mat[Reco_ind[0], j]\")\n",
    "                print(GPP_mat[Reco_ind[0], j])\n",
    "                print(\"Reco[j]\")\n",
    "                print(Reco[j])\n",
    "                print(\"GPP[j]\")\n",
    "                print(GPP[j])\n",
    "\n",
    "                #if j == 8840:\n",
    "                #    exit()\n",
    "            '''\n",
    "\n",
    "        #### If there is no window covering this data-point \"j\", then exit.\n",
    "        else: # TODO: investigate and replace behavior (same as broken opt error?)\n",
    "            msg = \"DT EXIT EXCEPTION: no window covering data point j\"\n",
    "            print(msg)\n",
    "            raise ONEFluxPartitionError(msg)\n",
    "    #### end \"for j\"\n",
    "\n",
    "    #print(\"Reco\")\n",
    "    #print(Reco)\n",
    "    #print(\"GPP\")\n",
    "    #print(GPP)\n",
    "    #print(\"partition_flag1\")\n",
    "    #print(partition_flag1)\n",
    "    #print(\"partition_flag2\")\n",
    "    #print(partition_flag2)\n",
    "\n",
    "    #exit()\n",
    "\n",
    "    #return np.concatenate((Reco, GPP, partition_flag1, partition_flag2), axis=0)\n",
    "\n",
    "    #### Saving the previously calculated Reco and GPP values of the overlapped windows.\n",
    "    var_names_reco_gpp = \"j,year,month,day,hr,julday,reco_first,reco_second,gpp_first,gpp_second\"\n",
    "\n",
    "    filename_reco = 'nee_' + ustar_type + '_' + str(percentile_num) + '_' + site_id + '_' + str(year) + '_reco_before_weights_python.csv'\n",
    "    np.savetxt(os.path.join(dt_output_dir, filename_reco), np.transpose(reco_gpp_orig), delimiter=',', fmt='%s', header=var_names_reco_gpp, comments='')\n",
    "    #exit()\n",
    "\n",
    "    print(\"Finished compute_flux of daytime for nee_{u}_{p}_{s}_{y}\".format(u=ustar_type, p=percentile_num, s=site_id, y=year))\n",
    "\n",
    "    return Reco, GPP, partition_flag1, partition_flag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43099d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_var(data, params, whichmodel, JTJ_inv, res_cor):\n",
    "\n",
    "    n_params = len(params[:, 0])\n",
    "    n_parasets = len(params[0, :])\n",
    "    n_set = len(data['nee_f'])\n",
    "    var_GPP_mat = np.empty((n_parasets, n_set))\n",
    "    var_GPP_mat.fill(NAN)\n",
    "    var_GPP = np.zeros(n_set, dtype=DOUBLE_PREC)\n",
    "\n",
    "    '''\n",
    "    print(\"n_params\")\n",
    "    print(n_params)\n",
    "    print(\"n_parasets\")\n",
    "    print(n_parasets)\n",
    "    print(\"n_set\")\n",
    "    print(n_set)\n",
    "    '''\n",
    "\n",
    "    #### Iterate over each parameter\n",
    "    for i in range(n_parasets):\n",
    "        sub = None\n",
    "        if i == 0:\n",
    "            ind_begin = 0\n",
    "            ind_end = params[n_params - 1, i + 1]\n",
    "            sub_mask = ((data['ind'] >= ind_begin) & (data['ind'] < ind_end))\n",
    "            sub, _, _ = newselif(data=data, condition=sub_mask, drop=True)\n",
    "        elif i == (n_parasets - 1):\n",
    "            ind_begin = params[n_params - 1, i - 1]\n",
    "            ind_end = np.max(data['ind'])\n",
    "            sub_mask = ((data['ind'] >= ind_begin) & (data['ind'] <= ind_end))\n",
    "            sub, _, _ = newselif(data=data, condition=sub_mask, drop=True)\n",
    "        else:\n",
    "            ind_begin = params[n_params - 1, i - 1]\n",
    "            ind_end = params[n_params - 1, i + 1]\n",
    "            sub_mask = ((data['ind'] >= ind_begin) & (data['ind'] < ind_end))\n",
    "            sub, _, _ = newselif(data=data, condition=sub_mask, drop=True)\n",
    "\n",
    "        #print(\"sub['ind'].size\")\n",
    "        #print(sub['ind'].size)\n",
    "\n",
    "        #print(\"i\")\n",
    "        #print(i)\n",
    "\n",
    "        #### params_filled_arr is just a replicated array filled with\n",
    "        #### the value of E0 of the current window\n",
    "        params_filled_arr = np.empty(sub['ind'].size)\n",
    "        params_filled_arr.fill(params[4, i])\n",
    "\n",
    "        #### params_filled_arr2 is just a replicated array filled with\n",
    "        #### the value of alpha of the current window\n",
    "        params_filled_arr2 = np.empty(sub['ind'].size)\n",
    "        params_filled_arr2.fill(params[0, i])\n",
    "\n",
    "        #### Based on the model we picked, we use it to get the predicted values.\n",
    "        if whichmodel[i] == 0:\n",
    "            #print(\"params[0:3+1, i]\")\n",
    "            #print(params[0:3+1, i])\n",
    "            var_GPP_mat[i, sub['ind'].astype(int)] = varpred(func=\"HLRC_LloydVPD\", data=sub, params_filled_arr=params_filled_arr, JTJ_inv=JTJ_inv[i, :, :],\n",
    "                                                optpara=params[0:3 + 1, i], res=res_cor[i])\n",
    "        elif whichmodel[i] == 1:\n",
    "            var_GPP_mat[i, sub['ind'].astype(int)] = varpred(func=\"HLRC_Lloyd\", data=sub, params_filled_arr=params_filled_arr, JTJ_inv=JTJ_inv[i, 0:2 + 1, 0:2 + 1],\n",
    "                                                optpara=[params[0, i], params[1, i], params[3, i]], res=res_cor[i])\n",
    "        elif whichmodel[i] == 2:\n",
    "            var_GPP_mat[i, sub['ind'].astype(int)] = varpred(func=\"HLRC_Lloyd_afix\", data=sub, params_filled_arr=params_filled_arr, params_filled_arr2=params_filled_arr2, JTJ_inv=JTJ_inv[i, 0:1 + 1, 0:1 + 1],\n",
    "                                                optpara=[params[1, i], params[3, i]], res=res_cor[i])\n",
    "        elif whichmodel[i] == 3:\n",
    "            var_GPP_mat[i, sub['ind'].astype(int)] = varpred(func=\"HLRC_LloydVPD_afix\", data=sub, params_filled_arr=params_filled_arr, params_filled_arr2=params_filled_arr2, JTJ_inv=JTJ_inv[i, 0:2 + 1, 0:2 + 1],\n",
    "                                                optpara=params[1:3 + 1, i], res=res_cor[i])\n",
    "        elif whichmodel[i] == 4:\n",
    "            #print(\"var_GPP_mat.shape\")\n",
    "            #print(var_GPP_mat.shape)\n",
    "            #print(\"sub['ind'].shape\")\n",
    "            #print(sub['ind'].shape)\n",
    "            #print(\"var_GPP_mat[i, sub['ind'].astype(int)].shape\")\n",
    "            #print(var_GPP_mat[i, sub['ind'].astype(int)].shape)\n",
    "            var_GPP_mat[i, sub['ind'].astype(int)] = varpred(func=\"LloydT_E0fix\", data=sub, params_filled_arr=params_filled_arr, JTJ_inv=JTJ_inv[i, 0, 0],\n",
    "                                                optpara=params[3, i], res=res_cor[i])\n",
    "            #if i == 2:\n",
    "            #    exit()\n",
    "\n",
    "    #end for i\n",
    "\n",
    "    #### Weight the predicted values and sum the values\n",
    "    for j in range(n_set):\n",
    "        GPP_ind = np.where(var_GPP_mat[:, j] > NAN)[0]\n",
    "        count = len(GPP_ind)\n",
    "        if count > 1:\n",
    "            weight1 = (params[n_params - 1, GPP_ind[1]] - j) / (params[n_params - 1, GPP_ind[1]] - params[n_params - 1, GPP_ind[0]])\n",
    "            weight2 = (j - params[n_params - 1, GPP_ind[0]]) / (params[n_params - 1, GPP_ind[1]] - params[n_params - 1, GPP_ind[0]])\n",
    "            var_GPP[j] = var_GPP_mat[GPP_ind[0], j] * (weight1 * weight1) + var_GPP_mat[GPP_ind[1], j] * (weight2 * weight2)\n",
    "        elif count == 1:\n",
    "            var_GPP[j] = var_GPP_mat[GPP_ind[0], j]\n",
    "        else:\n",
    "            var_GPP[j] = NAN\n",
    "    #end for j\n",
    "\n",
    "    #print(\"var_GPP\")\n",
    "    #print(var_GPP)\n",
    "    #exit()\n",
    "\n",
    "    print(\"Finished compute_var of daytime\")\n",
    "\n",
    "    return var_GPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "768098b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def varpred(func, data, JTJ_inv, optpara, res, params_filled_arr, params_filled_arr2=None):\n",
    "\n",
    "    #print(\"Starting varpred of daytime\")\n",
    "\n",
    "    #### Calculate the jacobian matrix\n",
    "    jac = jacobian(func=func, data=data, params_filled_arr=params_filled_arr, params_filled_arr2=params_filled_arr2, params=optpara)\n",
    "\n",
    "    '''\n",
    "    print(\"jac.shape\")\n",
    "    print(jac.shape)\n",
    "    print(\"jac.size\")\n",
    "    print(jac.size)\n",
    "\n",
    "    #print(\"JTJ_inv.shape\")\n",
    "    #print(JTJ_inv.shape)\n",
    "    #print(\"JTJ_inv.size\")\n",
    "    #print(JTJ_inv.size)\n",
    "    #print(\"JTJ_inv\")\n",
    "    #print(JTJ_inv)\n",
    "    '''\n",
    "\n",
    "    varY = None\n",
    "    if JTJ_inv.size == 1:\n",
    "        #print(\"JTJ_inv.size == 1\")\n",
    "        # in pvwave multiplying an array with it's transpose is like\n",
    "        # multiplying it without the transpose\n",
    "        #varY = (jac) * JTJ_inv * np.transpose(jac) * res\n",
    "        varY = (jac) * JTJ_inv * (jac) * res\n",
    "    else:\n",
    "        #print(\"JTJ_inv.size != 1\")\n",
    "        x = np.dot(np.transpose(jac), JTJ_inv)\n",
    "        y = np.dot(x, jac)\n",
    "        #print(\"y\")\n",
    "        #print(y * res)\n",
    "        varY = np.diagonal(y * res)\n",
    "\n",
    "    #print(\"varY\")\n",
    "    #print(varY)\n",
    "\n",
    "    #print(\"varY.shape\")\n",
    "    #print(varY.shape)\n",
    "\n",
    "    #print(\"varY.size\")\n",
    "    #print(varY.size)\n",
    "\n",
    "    #exit()\n",
    "\n",
    "    #print(\"Finished varpred of daytime\")\n",
    "\n",
    "    return varY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "734bd27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_output(filename, delimiter=',', skip_header=1):\n",
    "    \"\"\"\n",
    "    Loads 'output' formatted file (e.g., from output of nee_proc or meteo_proc)\n",
    "    \n",
    "    :param filename: Name of file to be loaded\n",
    "    :type filename: str\n",
    "    \"\"\"\n",
    "    print(\"Started loading '{f}'\".format(f=filename))\n",
    "\n",
    "    print(\"Started loading headers\")\n",
    "    with open(filename, 'r') as f:\n",
    "        header_line = f.readline()\n",
    "    headers = [i.strip().replace('.', HEADER_SEPARATOR).lower() for i in header_line.strip().split(delimiter)]\n",
    "    print(\"Finished loading headers: {h}\".format(h=headers))\n",
    "\n",
    "    print(\"Started loading data\")\n",
    "    dtype = [(i, ('a25' if i.lower() in STRING_HEADERS else FLOAT_PREC)) for i in headers]\n",
    "    vfill = [('' if i.lower() in STRING_HEADERS else np.NaN) for i in headers]\n",
    "    data = np.genfromtxt(fname=filename, dtype=dtype, names=headers, delimiter=delimiter, skip_header=skip_header, missing_values='-9999,-9999.0,-6999,-6999.0, ', usemask=True)\n",
    "    data = np.ma.filled(data, vfill)\n",
    "\n",
    "    new_dtype = dtype + [('year', FLOAT_PREC), ('month', FLOAT_PREC), ('day', FLOAT_PREC), ('hour', FLOAT_PREC), ('minute', FLOAT_PREC)]\n",
    "    new_data = np.zeros(len(data), dtype=new_dtype)\n",
    "    for h in headers:\n",
    "        new_data[h] = data[h]\n",
    "    print(\"Finished loading data\")\n",
    "\n",
    "    print(\"Started loading timestamps\")\n",
    "    timestamp_list = []\n",
    "\n",
    "    # TODO: the arrays below were added as a workaround for slower performance for structured arrays in np 1.10.1;\n",
    "    #       once fixed (1.10.2?), array_* should be removed\n",
    "    #       see bug: https://github.com/np/np/issues/6467\n",
    "    array_year = np.empty(len(data), dtype='i4')\n",
    "    array_month = np.empty(len(data), dtype='i4')\n",
    "    array_day = np.empty(len(data), dtype='i4')\n",
    "    array_hour = np.empty(len(data), dtype='i4')\n",
    "    array_minute = np.empty(len(data), dtype='i4')\n",
    "    it = np.nditer(new_data['timestamp_end'], flags=['f_index'])\n",
    "    while not it.finished:\n",
    "        timestamp = datetime.strptime(str(it.value), \"b'%Y%m%d%H%M'\")\n",
    "        array_year[it.index] = timestamp.year\n",
    "        array_month[it.index] = timestamp.month\n",
    "        array_day[it.index] = timestamp.day\n",
    "        array_hour[it.index] = timestamp.hour\n",
    "        array_minute[it.index] = timestamp.minute\n",
    "        timestamp_list.append(timestamp)\n",
    "        it.iternext()\n",
    "    new_data['year'][:] = array_year\n",
    "    new_data['month'][:] = array_month\n",
    "    new_data['day'][:] = array_day\n",
    "    new_data['hour'][:] = array_hour\n",
    "    new_data['minute'][:] = array_minute\n",
    "    year_array = np.unique(ar=new_data['year'])\n",
    "\n",
    "    print(\"Finished loading timestamps: first(END)={f}, last(END)={l}, years={y}\".format(f=new_data['timestamp_end'][0], l=new_data['timestamp_end'][-1], y=list(year_array)))\n",
    "\n",
    "    # need to remove last entry when using end-of-averaging period convention\n",
    "    year_list = sorted([int(i) for i in year_array])[:-1]\n",
    "\n",
    "    print(\"Finished loading '{f}'\".format(f=filename))\n",
    "    return new_data, headers, timestamp_list, year_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b06609af",
   "metadata": {},
   "outputs": [],
   "source": [
    "NT_STR = 'NT'\n",
    "DT_STR = 'DT'\n",
    "\n",
    "QC_AUTO_DIR = \"02_qc_auto\"\n",
    "METEO_PROC_DIR = \"07_meteo_proc\"\n",
    "NEE_PROC_DIR = \"08_nee_proc\"\n",
    "NT_OUTPUT_DIR = \"10_nee_partition_nt\"\n",
    "DT_OUTPUT_DIR = \"11_nee_partition_dt\"\n",
    "\n",
    "STRING_HEADERS = ['isodate', 'timestamp', 'timestamp_start', 'timestamp_end', 'dtime', 'date', 'time']\n",
    "\n",
    "HEADER_SEPARATOR = '__'\n",
    "\n",
    "PERCENTILES_DATA_COLUMNS = ['1.25', '3.75', '6.25', '8.75', '11.25', '13.75', '16.25', '18.75',\n",
    "                            '21.25', '23.75', '26.25', '28.75', '31.25', '33.75', '36.25', '38.75',\n",
    "                            '41.25', '43.75', '46.25', '48.75', '50', '51.25', '53.75', '56.25', '58.75',\n",
    "                            '61.25', '63.75', '66.25', '68.75', '71.25', '73.75', '76.25', '78.75',\n",
    "                            '81.25', '83.75', '86.25', '88.75', '91.25', '93.75', '96.25', '98.75', ]\n",
    "PERCENTILES_DATA_COLUMNS = [i.replace('.', HEADER_SEPARATOR) for i in PERCENTILES_DATA_COLUMNS]\n",
    "\n",
    "EXTRA_FILENAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80c73c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME_TEMPLATE = \"nee_{prod}_{perc}_{s}_{y}{add}.{e}\"\n",
    "PROD_TO_COMPARE = ['c', 'y']\n",
    "PERC_TO_COMPARE = ['1.25', '3.75', '6.25', '8.75', '11.25', '13.75', '16.25', '18.75',\n",
    "                   '21.25', '23.75', '26.25', '28.75', '31.25', '33.75', '36.25', '38.75',\n",
    "                   '41.25', '43.75', '46.25', '48.75',\n",
    "                   '50',\n",
    "                   '51.25', '53.75', '56.25', '58.75',\n",
    "                   '61.25', '63.75', '66.25', '68.75', '71.25', '73.75', '76.25', '78.75',\n",
    "                   '81.25', '83.75', '86.25', '88.75', '91.25', '93.75', '96.25', '98.75']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d9347bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20260\\2034497364.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0myears_to_compare\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2005\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2006\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpartitioning_dt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msiteid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msitedir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprod_to_compare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperc_to_compare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myears_to_compare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\one_flux\\ONEFlux\\oneflux\\partition\\daytime.py\u001b[0m in \u001b[0;36mpartitioning_dt\u001b[1;34m(datadir, siteid, sitedir, prod_to_compare, perc_to_compare, years_to_compare)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[1;31m#### call flux_part_gl2010 for day time (main partitioning process)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                 \u001b[0mresult_year_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflux_part_gl2010\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworking_year_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_output_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdt_output_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msite_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msiteid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mustar_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mustar_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercentile_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mresult_year_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\one_flux\\ONEFlux\\oneflux\\partition\\daytime.py\u001b[0m in \u001b[0;36mflux_part_gl2010\u001b[1;34m(data, name_file, name_out, dt_output_dir, site_id, ustar_type, percentile_num, year)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;31m#### Calling estimate_parasets to get the best model for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;31m#### the NEE data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m     \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhichmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJTJ_inv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_cor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_correl_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_parasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mh_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwinsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwinsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfguess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfguess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrimperc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrimperc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_output_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdt_output_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msite_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msite_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mustar_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mustar_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercentile_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpercentile_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[0mparamsOK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m9999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\one_flux\\ONEFlux\\oneflux\\partition\\daytime.py\u001b[0m in \u001b[0;36mestimate_parasets\u001b[1;34m(data, winsize, fguess, trimperc, name_out, dt_output_dir, site_id, ustar_type, percentile_num, year)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 \u001b[1;31m#### Starting the optimization using the \"HLRC_LloydVPD\" function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m                 \u001b[0mhlrclvpd_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlinlts2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlts_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"HLRC_LloydVPD\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nee_f'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindepvar_arr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rg_f'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tair_f'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'e0_1_from_tair'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vpd_f'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpara\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxguess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfguess\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmprior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfguess\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_PREC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nee_fs_unc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m                 \u001b[1;31m#print(nlinlts2(data=subd, lts_func=\"HLRC_LloydVPD\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'vpd_f'], npara=4, xguess=fguess[0:3+1], mprior=numpy.array(fguess[0:3+1], dtype=FLOAT_PREC), sigm=numpy.array([10, 600, 50, 80]), sigd=subd['nee_fs_unc']))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\one_flux\\ONEFlux\\oneflux\\partition\\library.py\u001b[0m in \u001b[0;36mnlinlts2\u001b[1;34m(data, lts_func, depvar, indepvar_arr, npara, xguess, mprior, sigm, sigd, window_size, trim_perc)\u001b[0m\n\u001b[0;32m    587\u001b[0m                                                                                   \u001b[0mentries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_dep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                                                                                   \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_dep\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                                                                                   return_residuals_cov_mat=True)\n\u001b[0m\u001b[0;32m    590\u001b[0m     '''\n\u001b[0;32m    591\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ending least_squares\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\one_flux\\ONEFlux\\oneflux\\partition\\library.py\u001b[0m in \u001b[0;36mleast_squares\u001b[1;34m(func, initial_guess, entries, iterations, stop, return_residuals_cov_mat)\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m                 \u001b[0m_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No convergence (code '{p}'), retrying ({r}-fold limit increase). Least squares message: [[{m}]]\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNO_CONVERGENCE_RETRY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 906\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_guess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_guess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mentries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miterations\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mNO_CONVERGENCE_RETRY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_residuals_cov_mat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_residuals_cov_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    907\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[0m_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No convergence (code '{p}'), stopping. Least squares message: [[{m}]]\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\one_flux\\ONEFlux\\oneflux\\partition\\library.py\u001b[0m in \u001b[0;36mleast_squares\u001b[1;34m(func, initial_guess, entries, iterations, stop, return_residuals_cov_mat)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[1;31m# call to scipy.optimize.leastsq (implementation of the Levenberg-Marquardt algorithm)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 900\u001b[1;33m     \u001b[0mpars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleastsq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_guess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxfev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSTEP_BOUND_FACTOR\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#ftol=1.11e-16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m# and (info['nfev'] == iterations):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\envs\\hsi\\lib\\site-packages\\scipy\\optimize\\minpack.py\u001b[0m in \u001b[0;36mleastsq\u001b[1;34m(func, x0, args, Dfun, full_output, col_deriv, ftol, xtol, gtol, maxfev, epsfcn, factor, diag)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[0mmaxfev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         retval = _minpack._lmdif(func, x0, args, full_output, ftol, xtol,\n\u001b[1;32m--> 424\u001b[1;33m                                  gtol, maxfev, epsfcn, factor, diag)\n\u001b[0m\u001b[0;32m    425\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcol_deriv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\one_flux\\ONEFlux\\oneflux\\partition\\library.py\u001b[0m in \u001b[0;36mtrimmed_bayes_res\u001b[1;34m(par, nee, trim_perc)\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[1;31m#if leastsq_count[0] == 6:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[1;31m#    par = numpy.array([-0.020768575, 1.9603746, 0.0, 0.40222415], dtype=DOUBLE_PREC)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhlrc_lloydvpd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrg_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindepvar_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDOUBLE_PREC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mta_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindepvar_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDOUBLE_PREC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindepvar_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDOUBLE_PREC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvpd_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindepvar_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDOUBLE_PREC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;31m#print(\"par\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m             \u001b[1;31m#print(par)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datadir = '../'\n",
    "siteid = 'US-ARc'\n",
    "sitedir = 'US-ARc_sample_input'\n",
    "prod_to_compare = PROD_TO_COMPARE\n",
    "perc_to_compare = PERC_TO_COMPARE\n",
    "years_to_compare = range(2005,2006+1)\n",
    "\n",
    "#partitioning_dt(datadir, siteid, sitedir, prod_to_compare, perc_to_compare, years_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d5af40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started DT partitioning of US-ARc\n",
      "Will now load meteo file '../US-ARc_sample_input\\07_meteo_proc\\US-ARc_meteo_hh.csv'\n"
     ]
    }
   ],
   "source": [
    "print(\"Started DT partitioning of {s}\".format(s=siteid))\n",
    "\n",
    "sitedir_full = os.path.join(datadir, sitedir)\n",
    "qc_auto_dir = os.path.join(sitedir_full, QC_AUTO_DIR)\n",
    "meteo_proc_dir = os.path.join(sitedir_full, METEO_PROC_DIR)\n",
    "nee_proc_dir = os.path.join(sitedir_full, NEE_PROC_DIR)\n",
    "dt_output_dir = os.path.join(sitedir_full, DT_OUTPUT_DIR)\n",
    "\n",
    "# reformat percentiles to compare into data column labels\n",
    "percentiles_data_columns = [i.replace('.', HEADER_SEPARATOR) for i in perc_to_compare]\n",
    "\n",
    "# check and create output dir if needed\n",
    "if os.path.isdir(sitedir_full) and not os.path.isdir(dt_output_dir):\n",
    "    check_create_directory(directory=dt_output_dir)\n",
    "\n",
    "# load meteo proc results\n",
    "meteo_proc_f = os.path.join(meteo_proc_dir, '{s}_meteo_hh.csv'.format(s=siteid))\n",
    "if not os.path.isfile(meteo_proc_f):\n",
    "    msg = \"Meteo proc file not found '{f}'\".format(f=meteo_proc_f)\n",
    "    print(msg)\n",
    "    raise ONEFluxError(msg)\n",
    "print(\"Will now load meteo file '{f}'\".format(f=meteo_proc_f))\n",
    "whole_dataset_meteo, headers_meteo, timestamp_list_meteo, year_list_meteo = load_output(meteo_proc_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b7e31443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['c', 'y'], '../US-ARc_sample_input\\\\08_nee_proc')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_to_compare, nee_proc_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a18288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing UStar threshold type 'y'\n",
      "Will now load nee percentiles file '../US-ARc_sample_input\\08_nee_proc\\US-ARc_NEE_percentiles_y_hh.csv'\n"
     ]
    }
   ],
   "source": [
    "# iterate through UStar threshold types\n",
    "# for ustar_type in prod_to_compare:\n",
    "ustar_type = 'y'\n",
    "print(\"Started processing UStar threshold type '{u}'\".format(u=ustar_type))\n",
    "\n",
    "# load nee proc results (percentiles file)\n",
    "nee_proc_percentiles_f = os.path.join(nee_proc_dir, '{s}_NEE_percentiles_{u}_hh.csv'.format(s=siteid, u=ustar_type))\n",
    "if not os.path.isfile(nee_proc_percentiles_f):\n",
    "    msg = \"NEE proc file not found '{f}', trying '{n}'\".format(f=nee_proc_percentiles_f, n='{f}')\n",
    "    nee_proc_percentiles_f = os.path.join(nee_proc_dir, '{s}_NEE_percentiles_{u}.csv'.format(s=siteid, u=ustar_type))\n",
    "    msg = msg.format(f=nee_proc_percentiles_f)\n",
    "    print(msg)\n",
    "\n",
    "    if not os.path.isfile(nee_proc_percentiles_f):\n",
    "        if ustar_type == 'y':\n",
    "            msg = \"NEE proc file not found '{f}'\".format(f=nee_proc_percentiles_f)\n",
    "            print(msg)\n",
    "            raise ONEFluxError(msg)\n",
    "        elif ustar_type == 'c':\n",
    "            msg = \"NEE proc file not found '{f}', skipping (CUT not computed?)\".format(f=nee_proc_percentiles_f)\n",
    "            print(msg)\n",
    "            #continue\n",
    "        else:\n",
    "            msg = \"Invalid USTAR type '{u}'\".format(u=ustar_type)\n",
    "            raise ONEFluxError(msg)\n",
    "print(\"Will now load nee percentiles file '{f}'\".format(f=nee_proc_percentiles_f))\n",
    "whole_dataset_nee, headers_nee, timestamp_list_nee, year_list_nee = load_output(nee_proc_percentiles_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b32fed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35040,),\n",
       " [2005, 2006],\n",
       " range(2005, 2007),\n",
       " '../US-ARc_sample_input\\\\02_qc_auto',\n",
       " ['1__25',\n",
       "  '3__75',\n",
       "  '6__25',\n",
       "  '8__75',\n",
       "  '11__25',\n",
       "  '13__75',\n",
       "  '16__25',\n",
       "  '18__75',\n",
       "  '21__25',\n",
       "  '23__75',\n",
       "  '26__25',\n",
       "  '28__75',\n",
       "  '31__25',\n",
       "  '33__75',\n",
       "  '36__25',\n",
       "  '38__75',\n",
       "  '41__25',\n",
       "  '43__75',\n",
       "  '46__25',\n",
       "  '48__75',\n",
       "  '50',\n",
       "  '51__25',\n",
       "  '53__75',\n",
       "  '56__25',\n",
       "  '58__75',\n",
       "  '61__25',\n",
       "  '63__75',\n",
       "  '66__25',\n",
       "  '68__75',\n",
       "  '71__25',\n",
       "  '73__75',\n",
       "  '76__25',\n",
       "  '78__75',\n",
       "  '81__25',\n",
       "  '83__75',\n",
       "  '86__25',\n",
       "  '88__75',\n",
       "  '91__25',\n",
       "  '93__75',\n",
       "  '96__25',\n",
       "  '98__75'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_dataset_nee.shape, year_list_nee, years_to_compare, qc_auto_dir, percentiles_data_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1bf049c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started processing year '2005'\n",
      "Started processing percentile '1__25'\n",
      "Output file found, skipping: '../US-ARc_sample_input\\11_nee_partition_dt\\nee_y_1.25_US-ARc_2005.csv'\n",
      "First site-year available (2005), removing first midnight entry from meteo only\n",
      "Site-year (2005), adding first midnight entry from next year for meteo and nee\n",
      "Site-year 2005: first NEE 'b'200501010030'' and first meteo 'b'200501010030''\n",
      "Site-year 2005:  last NEE 'b'200601010000'' and  last meteo 'b'200601010000''\n"
     ]
    }
   ],
   "source": [
    "# iterate through each year\n",
    "#for iteration, year in enumerate(year_list_nee):\n",
    "iteration, year = 0, 2005\n",
    "\n",
    "print(\"Started processing year '{y}'\".format(y=year))\n",
    "qc_auto_nee_f = os.path.join(qc_auto_dir, '{s}_qca_nee_{y}.csv'.format(s=siteid, y=year)) # US-ARc_qca_nee_2005.csv\n",
    "if not os.path.isfile(qc_auto_nee_f):\n",
    "    msg = \"QC auto file not found '{f}'\".format(f=qc_auto_nee_f)\n",
    "    print(msg)\n",
    "    #continue\n",
    "latitude = get_latitude(filename=qc_auto_nee_f)\n",
    "\n",
    "# iterate through UStar threshold values\n",
    "#for percentile in percentiles_data_columns:\n",
    "percentile = '1__25'\n",
    "print(\"Started processing percentile '{p}'\".format(p=percentile))\n",
    "percentile_print = percentile.replace(HEADER_SEPARATOR, '.')\n",
    "output_filename = os.path.join(dt_output_dir, \"nee_{t}_{p}_{s}_{y}{extra}.csv\".format(t=ustar_type, p=percentile_print, s=siteid, y=year, extra=EXTRA_FILENAME))\n",
    "temp_output_filename = os.path.join(dt_output_dir, \"nee_{t}_{p}_{s}_{y}{extra}.csv\".format(t=ustar_type, p=percentile_print, s=siteid, y=year, extra='{extra}'))\n",
    "if os.path.isfile(output_filename):\n",
    "    print(\"Output file found, skipping: '{f}'\".format(f=output_filename))\n",
    "    #continue\n",
    "else:\n",
    "    print(\"Output file missing, will be processed: '{f}'\".format(f=output_filename))\n",
    "\n",
    "# create masks for current year for both nee and meteo\n",
    "year_mask_nee = (whole_dataset_nee['year'] == year)\n",
    "year_mask_meteo = (whole_dataset_meteo['year'] == year)\n",
    "\n",
    "# account for first entry being from previous year\n",
    "if iteration == 0:\n",
    "    print(\"First site-year available ({y}), removing first midnight entry from meteo only\".format(y=year))\n",
    "    first_meteo = np.where(year_mask_meteo == 1)[0][0]\n",
    "    first_nee = None\n",
    "    year_mask_meteo[first_meteo] = 0\n",
    "else:\n",
    "    print(\"Regular site-year ({y}), removing first midnight entry from meteo and nee\".format(y=year))\n",
    "    first_meteo = np.where(year_mask_meteo == 1)[0][0]\n",
    "    first_nee = np.where(year_mask_nee == 1)[0][0]\n",
    "    year_mask_meteo[first_meteo] = 0\n",
    "    year_mask_nee[first_nee] = 0\n",
    "\n",
    "# account for last entry being from next year\n",
    "print(\"Site-year ({y}), adding first midnight entry from next year for meteo and nee\".format(y=year))\n",
    "last_meteo = np.where(year_mask_meteo == 1)[0][-1] + 1\n",
    "last_nee = np.where(year_mask_nee == 1)[0][-1] + 1\n",
    "year_mask_meteo[last_meteo] = 1\n",
    "year_mask_nee[last_nee] = 1\n",
    "\n",
    "print(\"Site-year {y}: first NEE '{tn}' and first meteo '{tm}'\".format(y=year, tn=whole_dataset_nee[year_mask_nee][0]['timestamp_end'], tm=whole_dataset_meteo[year_mask_meteo][0]['timestamp_end']))\n",
    "print(\"Site-year {y}:  last NEE '{tn}' and  last meteo '{tm}'\".format(y=year, tn=whole_dataset_nee[year_mask_nee][-1]['timestamp_end'], tm=whole_dataset_meteo[year_mask_meteo][-1]['timestamp_end']))\n",
    "\n",
    "if np.sum(year_mask_nee) != np.sum(year_mask_meteo):\n",
    "    msg = \"Incompatible array sizes (nee={n}, meteo={m}) for year '{y}' while processing '{f}'\".format(y=year, f=output_filename, n=np.sum(year_mask_nee), m=np.sum(year_mask_meteo))\n",
    "    print(msg)\n",
    "    raise ONEFluxError(msg)\n",
    "\n",
    "#### Get a cleaned-up organized np version of the data\n",
    "working_year_data = create_data_structures(ustar_type=ustar_type, whole_dataset_nee=whole_dataset_nee, whole_dataset_meteo=whole_dataset_meteo,\n",
    "                                           percentile=percentile, year_mask_nee=year_mask_nee, year_mask_meteo=year_mask_meteo, latitude=latitude, part_type=DT_STR)\n",
    "\n",
    "#### Remove entries that fall into specified error-ranges\n",
    "working_year_data = remove_errored_entries(ustar_type=ustar_type, site=siteid, site_dir=sitedir_full, year=year, working_year_data=working_year_data)\n",
    "\n",
    "name_out = str(siteid) + \"_\" + str(year) + \"_\" + str(ustar_type)\n",
    "\n",
    "name_file = \"nee_\" + str(ustar_type) + \"_\" + str(percentile) + \"_\" + str(siteid) + \"_\" + str(year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ff1f783e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17520,),\n",
       " 'nee_y_1__25_US-ARc_2005',\n",
       " 'US-ARc_2005_y',\n",
       " '../US-ARc_sample_input\\\\11_nee_partition_dt',\n",
       " 'US-ARc',\n",
       " 'y',\n",
       " '1__25',\n",
       " 2005)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_year_data.shape,name_file,name_out,dt_output_dir,siteid,ustar_type,percentile,year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "48fc99f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting flux_part_gl2010 for daytime for nee_y_1__25_US-ARc_2005\n"
     ]
    }
   ],
   "source": [
    "#def flux_part_gl2010(data, name_file, name_out, dt_output_dir, site_id, ustar_type, percentile_num, year):\n",
    "data=working_year_data\n",
    "percentile_num=percentile\n",
    "site_id = siteid\n",
    "print(\"Starting flux_part_gl2010 for daytime for nee_{u}_{p}_{s}_{y}\".format(u=ustar_type, p=percentile_num, s=site_id, y=year))\n",
    "\n",
    "add_empty_vars(data=data, records=data['vpd'], column='vpd_f', unit='-')\n",
    "\n",
    "h_data = np.copy(data)\n",
    "\n",
    "# initiating defaults for both the gap filling and optimization (estimate_params) functions\n",
    "#################################################\n",
    "max_miss_frac = 0.4\n",
    "winsize = 4\n",
    "trimperc = 0.0\n",
    "fguess = [0.01, 30.0, 0.0, 5.0, 100.0]\n",
    "#################################################\n",
    "\n",
    "n_data = h_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "71ddaf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting uncert_gap_fill of daytime\n",
      "uncert_gap_fill: Starting loop #1\n",
      "uncert_gap_fill: Starting loop #2\n",
      "uncert_gap_fill: Starting loop #3\n",
      "uncert_gap_fill: Starting loop #4\n",
      "uncert_gap_fill: Starting loop #5\n",
      "uncert_gap_fill: Starting loop #6\n",
      "uncert_gap_fill: Finalize Results\n"
     ]
    }
   ],
   "source": [
    "# Imaginary step to drop specific column names. Remove anything that ends with 'unc'\n",
    "# drop(data, '##unc')\n",
    "\n",
    "#print(data.dtype.names)\n",
    "#print(\"----------\")\n",
    "#print(\"\\n\".join(s for s in data.dtype.names if \"day\".lower() in s.lower()))\n",
    "\n",
    "#compare_results_pv_py(py_data=h_data, pvwave_file_path='../test_before_uncert_gapfill.csv', var='NEE')\n",
    "\n",
    "# Compute uncertainties via gap filling\n",
    "uncert_via_gapFill(data=h_data, var='NEE'.lower(), nomsg=True, maxMissFrac=1.0)\n",
    "\n",
    "#pvwave_file_path = '../test_after_uncert_gapfill.csv'\n",
    "#file_basename = 'after_gapfill_1999_y'\n",
    "#compare_results_pv_py(py_data=h_data, pvwave_file_path=pvwave_file_path, var='NEE_f_unc', file_basename=file_basename, save_csv=True, show_diff_index=True, show_diff_thresh=0.01)\n",
    "#compare_results_pv_py(py_data=h_data, pvwave_file_path=pvwave_file_path, var='NEE_fmet_unc', file_basename=file_basename, save_csv=True, show_diff_index=True, show_diff_thresh=0.01)\n",
    "#compare_results_pv_py(py_data=h_data, pvwave_file_path=pvwave_file_path, var='NEE_fwin_unc', file_basename=file_basename, save_csv=True, show_diff_index=True, show_diff_thresh=0.01)\n",
    "#compare_results_pv_py(py_data=h_data, pvwave_file_path=pvwave_file_path, var='NEE_fn_unc', file_basename=file_basename, save_csv=True, show_diff_index=True, show_diff_thresh=0.01)\n",
    "#compare_results_pv_py(py_data=h_data, pvwave_file_path=pvwave_file_path, var='NEE_fs_unc', file_basename=file_basename, save_csv=True, show_diff_index=True, show_diff_thresh=0.01)\n",
    "#compare_results_pv_py(py_data=h_data, pvwave_file_path=pvwave_file_path, var='NEE_fsrob_unc', file_basename=file_basename, save_csv=True, show_diff_index=True, show_diff_thresh=0.01)\n",
    "#compare_results_pv_py(py_data=h_data, pvwave_file_path=pvwave_file_path, var='NEE_fmed_unc', file_basename=file_basename, save_csv=True, show_diff_index=True, show_diff_thresh=0.01)\n",
    "#compare_results_pv_py(py_data=h_data, pvwave_file_path=pvwave_file_path, var='NEE_fqc_unc', file_basename=file_basename, save_csv=True, show_diff_index=True, show_diff_thresh=0.01)\n",
    "#compare_results_pv_py(py_data=h_data, pvwave_file_path=pvwave_file_path, var='NEE_fqcOK_unc', file_basename=file_basename, save_csv=True, show_diff_index=True, show_diff_thresh=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a5c6991d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " [0.01, 30.0, 0.0, 5.0, 100.0],\n",
       " 0.0,\n",
       " '../US-ARc_sample_input\\\\11_nee_partition_dt',\n",
       " 'US-ARc',\n",
       " 'y',\n",
       " '1__25',\n",
       " 2005)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winsize,fguess,trimperc,dt_output_dir,site_id,ustar_type,percentile_num,year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d2da5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(n_data)\n",
    "add_empty_vars(data=h_data, records=ind, column=str(\"ind\"))\n",
    "\n",
    "NEE_fqcok = (h_data['nee_f'] > -999).astype(int) * h_data['nee_fqcok']\n",
    "add_empty_vars(data=h_data, records=NEE_fqcok, column=str(\"nee_fqcok\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fcc78d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def estimate_parasets(data, winsize, fguess, trimperc, name_out, dt_output_dir, site_id, ustar_type, percentile_num, year):\n",
    "data=h_data\n",
    "\n",
    "#### Creating the arrays we're going to use\n",
    "n_parasets = int(365 / winsize) * 2  # 182\n",
    "params = numpy.zeros((3, 2 * len(fguess), n_parasets), dtype=FLOAT_PREC) # shape is [3, 2*5, 182]\n",
    "params_ok = numpy.zeros((2 * len(fguess), n_parasets), dtype=FLOAT_PREC)\n",
    "params_nok = numpy.zeros((2 * len(fguess), n_parasets), dtype=FLOAT_PREC)\n",
    "rmse = numpy.zeros(3, dtype=FLOAT_PREC)\n",
    "#ind = fltarr(n_parasets, 3, 3)\n",
    "ind = numpy.zeros((3, 3, n_parasets), dtype=FLOAT_PREC)\n",
    "ind_ok = numpy.zeros((3, n_parasets), dtype=FLOAT_PREC)\n",
    "p_cor = numpy.zeros((3, 6, n_parasets), dtype=FLOAT_PREC)\n",
    "p_cor_ok = numpy.zeros((6, n_parasets), dtype=FLOAT_PREC)\n",
    "\n",
    "JTJ_inv_ok = numpy.zeros((n_parasets, len(fguess) - 1, len(fguess) - 1), dtype=DOUBLE_PREC)\n",
    "whichmodel = numpy.zeros(3, dtype=int)\n",
    "whichmodel_ok = numpy.zeros(n_parasets, dtype=int)\n",
    "res_cor = numpy.zeros(3, dtype=DOUBLE_PREC)\n",
    "res_cor_ok = numpy.zeros(n_parasets, dtype=DOUBLE_PREC)\n",
    "\n",
    "params_all = numpy.zeros((2 * len(fguess), n_parasets), dtype=FLOAT_PREC)\n",
    "params_all_timestamp = numpy.zeros((n_parasets, 2 * len(fguess) + 2), dtype=FLOAT_PREC)\n",
    "\n",
    "# my code\n",
    "new_dtype = PARAM_DTYPE\n",
    "### intitalize extra diagnostics output\n",
    "params_all_for_ranges = numpy.zeros(n_parasets, dtype=new_dtype)\n",
    "params_all_for_ranges['year'] = int(year)\n",
    "params_all_for_ranges['nee_avg'] = NAN\n",
    "params_all_for_ranges['ta_avg'] = NAN\n",
    "params_all_for_ranges['rg_avg'] = NAN\n",
    "params_all_for_ranges['nee_std'] = NAN\n",
    "params_all_for_ranges['ta_std'] = NAN\n",
    "params_all_for_ranges['rg_std'] = NAN\n",
    "#end of my code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9bc4d0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>i_ok</th>\n",
       "      <th>ind_begin</th>\n",
       "      <th>ind_end</th>\n",
       "      <th>subset_size</th>\n",
       "      <th>nee_avg</th>\n",
       "      <th>nee_std</th>\n",
       "      <th>ta_avg</th>\n",
       "      <th>ta_std</th>\n",
       "      <th>rg_avg</th>\n",
       "      <th>rg_std</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>k</th>\n",
       "      <th>rref</th>\n",
       "      <th>e0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  i  day  i_ok  ind_begin  ind_end  subset_size  nee_avg  nee_std  \\\n",
       "0    2005  0    0     0          0        0            0  -9999.0  -9999.0   \n",
       "1    2005  0    0     0          0        0            0  -9999.0  -9999.0   \n",
       "2    2005  0    0     0          0        0            0  -9999.0  -9999.0   \n",
       "3    2005  0    0     0          0        0            0  -9999.0  -9999.0   \n",
       "4    2005  0    0     0          0        0            0  -9999.0  -9999.0   \n",
       "..    ... ..  ...   ...        ...      ...          ...      ...      ...   \n",
       "177  2005  0    0     0          0        0            0  -9999.0  -9999.0   \n",
       "178  2005  0    0     0          0        0            0  -9999.0  -9999.0   \n",
       "179  2005  0    0     0          0        0            0  -9999.0  -9999.0   \n",
       "180  2005  0    0     0          0        0            0  -9999.0  -9999.0   \n",
       "181  2005  0    0     0          0        0            0  -9999.0  -9999.0   \n",
       "\n",
       "     ta_avg  ta_std  rg_avg  rg_std  alpha  beta    k  rref   e0  \n",
       "0   -9999.0 -9999.0 -9999.0 -9999.0    0.0   0.0  0.0   0.0  0.0  \n",
       "1   -9999.0 -9999.0 -9999.0 -9999.0    0.0   0.0  0.0   0.0  0.0  \n",
       "2   -9999.0 -9999.0 -9999.0 -9999.0    0.0   0.0  0.0   0.0  0.0  \n",
       "3   -9999.0 -9999.0 -9999.0 -9999.0    0.0   0.0  0.0   0.0  0.0  \n",
       "4   -9999.0 -9999.0 -9999.0 -9999.0    0.0   0.0  0.0   0.0  0.0  \n",
       "..      ...     ...     ...     ...    ...   ...  ...   ...  ...  \n",
       "177 -9999.0 -9999.0 -9999.0 -9999.0    0.0   0.0  0.0   0.0  0.0  \n",
       "178 -9999.0 -9999.0 -9999.0 -9999.0    0.0   0.0  0.0   0.0  0.0  \n",
       "179 -9999.0 -9999.0 -9999.0 -9999.0    0.0   0.0  0.0   0.0  0.0  \n",
       "180 -9999.0 -9999.0 -9999.0 -9999.0    0.0   0.0  0.0   0.0  0.0  \n",
       "181 -9999.0 -9999.0 -9999.0 -9999.0    0.0   0.0  0.0   0.0  0.0  \n",
       "\n",
       "[182 rows x 18 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_params = pd.DataFrame(params_all_for_ranges)\n",
    "pd_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "53879856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "#;;; whichmodel: choice of model  ;;;\n",
    "#;;; 0: HLRC_LloydVPD             ;;;\n",
    "#;;; 1: HLRC_Lloyd                ;;;\n",
    "#;;; 2: HLRC_Lloyd_afix           ;;;\n",
    "#;;; 3: HLRC_LloydVPD_afix        ;;;\n",
    "#;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "i_ok = 0\n",
    "i_nok = 0\n",
    "betafac = [0.5, 1, 2]\n",
    "\n",
    "lloydtemp_e0 = None\n",
    "lloydtemp_e0_se = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "90c8dddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5376    0.0\n",
       "5377    0.0\n",
       "5379    0.0\n",
       "5380    0.0\n",
       "5381    0.0\n",
       "       ... \n",
       "5553    0.0\n",
       "5554    0.0\n",
       "5555    0.0\n",
       "5556    0.0\n",
       "5558    0.0\n",
       "Name: nee_fqc, Length: 111, dtype: float32"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_h_data = pd.DataFrame(data)\n",
    "pd_h_data[((pd_h_data['julday'] > day_begin) & (pd_h_data['julday'] <= day_end) & (pd_h_data['nee_fqc'] == 0))]['nee_fqc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "fb161975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(n_parasets):\n",
    "i = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a6ede1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "JTJ_inv = numpy.zeros((3, len(fguess) - 1, len(fguess) - 1), dtype=DOUBLE_PREC)\n",
    "\n",
    "day_begin = i * winsize / 2.0\n",
    "day_end = day_begin + winsize\n",
    "\n",
    "day_begin2 = 0\n",
    "day_end2 = numpy.amax(data['julday'])\n",
    "\n",
    "if i > 1:\n",
    "    day_begin2 = (i - 2) * winsize / 2.0\n",
    "if i < n_parasets - 2:\n",
    "    day_end2 = (i + 2) * winsize / 2.0 + winsize\n",
    "\n",
    "sub_mask = ((data['julday'] > day_begin) & (data['julday'] <= day_end) & (data['nee_fqc'] == 0))\n",
    "subn_mask = ((data['julday'] > day_begin2) & (data['julday'] <= day_end2) & (data['nee_fqc'] == 0) & (data['rg'] <= 4))\n",
    "subd_mask = ((data['julday'] > day_begin) & (data['julday'] <= day_end) & (data['nee_fqc'] == 0) & (data['rg'] > 4))\n",
    "\n",
    "#### Get the data that correspond to the masks in the previous step\n",
    "sub, _, _ = newselif(data=data, condition=sub_mask, drop=True)\n",
    "subn, _, _ = newselif(data=data, condition=subn_mask, drop=True)\n",
    "subd, _, _ = newselif(data=data, condition=subd_mask, drop=True)\n",
    "\n",
    "#### Calculate the first index of the window we're using now\n",
    "ind[:, :, i] = int((day_begin + winsize / 2.0) * 48.0)\n",
    "\n",
    "if numpy.amin(subn['nee_fs_unc']) < 0:\n",
    "    subn['nee_fs_unc'][:] = 1\n",
    "\n",
    "if numpy.amin(subd['nee_fs_unc']) < 0:\n",
    "    subd['nee_fs_unc'][:] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "23abe286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112.0, 116.0, 108.0, 120.0, 0, 159.15247325506542)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_begin,day_end,day_begin2,day_end2, i_ok, lloydtemp_e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "54334ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3.42147\n",
       "1      3.03719\n",
       "2      3.50319\n",
       "3      3.47909\n",
       "4      4.21278\n",
       "        ...   \n",
       "124    1.41272\n",
       "125    2.47987\n",
       "126    1.56644\n",
       "127    1.44034\n",
       "128    1.54903\n",
       "Name: nee_f, Length: 129, dtype: float32"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_sub = pd.DataFrame(subn)\n",
    "pd_sub['nee_f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cb58b826",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20260\\4121480031.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m#### Setting the parameters of this iteration (j -> modified fguess)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhlrclvpd_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhlrclvpd_beta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhlrclvpd_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhlrclvpd_rref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhlrclvpd_alpha_se\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhlrclvpd_beta_se\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhlrclvpd_k_se\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhlrclvpd_rref_se\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me0_se\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "E0set = 0\n",
    "#### If the data in subn within the window is <= 10, then use\n",
    "#### the lloydtemp_e0 from the previous window\n",
    "if subn['nee_f'].shape[0] <= 10 and i_ok > 0 and lloydtemp_e0 != None:\n",
    "    lloydtemp_e0 = params_ok[4, i_ok - 1]\n",
    "    lloydtemp_e0_se = params_ok[9, i_ok - 1]\n",
    "    #ind[i][0][:] = ind_ok[i_ok - 1][0]\n",
    "    ind[:, 0, i] = ind_ok[0, i_ok - 1]\n",
    "    E0set = 1\n",
    "\n",
    "\n",
    "#### Chech if the data is suitable for optimization (to find the model)\n",
    "if (subn['nee_f'].shape[0] > 10 or E0set == 1) and subd['nee_f'].shape[0] > 10:\n",
    "    #### Calling percentiles_fn to get the values of the chosen\n",
    "    #### percentiles from the \"nee_f\" data array after sorting it\n",
    "    percs = percentiles_fn(data=sub, columns=['nee_f'], values=[0.03, 0.97])\n",
    "    #### Setting initial value for beta amplitude of NEE\n",
    "    beta = abs(percs[0] - percs[1])\n",
    "\n",
    "    #### Setting initial value for rb to be the average\n",
    "    #### of the \"nee_f\" data\n",
    "    rb = numpy.average(subn['nee_f'])\n",
    "    fguess[3] = rb\n",
    "\n",
    "    if E0set == 0:\n",
    "        # estimate temperature sensitivity from data\n",
    "        '''\n",
    "        print(\"****************************\")\n",
    "        print(\"Starting LloydTemp\")\n",
    "        print(\"****************************\")\n",
    "        '''\n",
    "        #status, rref, e0, rref_se, e0_se, residuals, covariance_matrix, cor_matrix, lt_rmse, ls_status = nlinlts2(data=subn, lts_func=\"LloydTemp\", depvar='nee_f', indepvar_arr=['tair_f'], npara=2, xguess=fguess[3:4+1], mprior=numpy.array(fguess[3:4+1], dtype=FLOAT_PREC), sigm=numpy.array([800, 1000]), sigd=subn['nee_fs_unc'])\n",
    "\n",
    "        #### Starting the optimization using the \"LloyedTemp\" function\n",
    "        lloyedTemp_result = nlinlts2(data=subn, lts_func=\"LloydTemp\", depvar='nee_f', indepvar_arr=['tair_f'], npara=2, xguess=fguess[3:4 + 1], mprior=numpy.array(fguess[3:4 + 1], dtype=FLOAT_PREC), sigm=numpy.array([800, 1000]), sigd=subn['nee_fs_unc'])\n",
    "\n",
    "        #### Setting the returned model parameters\n",
    "        status = lloyedTemp_result['status']\n",
    "        rref = lloyedTemp_result['rref']\n",
    "        e0 = lloyedTemp_result['e0']\n",
    "        rref_se = lloyedTemp_result['rref_std_error']\n",
    "        e0_se = lloyedTemp_result['e0_std_error']\n",
    "        residuals = lloyedTemp_result['residuals']\n",
    "        covariance_matrix = lloyedTemp_result['cov_matrix']\n",
    "        cor_matrix = lloyedTemp_result['cor_matrix']\n",
    "        lt_rmse = lloyedTemp_result['rmse']\n",
    "        ls_status = lloyedTemp_result['ls_status']\n",
    "\n",
    "        lloydtemp_e0 = e0\n",
    "        lloydtemp_e0_se = e0_se\n",
    "\n",
    "        if covariance_matrix is None or cor_matrix is None:\n",
    "            raise ONEFluxPartitionBrokenOptError('LloydTemp', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "        #### Check that the returned e0 is within range\n",
    "        #### if not, then get the e0 set from the previous\n",
    "        #### parameter set; if this doesn't work, set it to the limits.\n",
    "        if e0 < 50 or e0 > 400:\n",
    "            if i_ok > 0:\n",
    "                e0 = params_ok[4, i_ok - 1]\n",
    "                e0_se = params_ok[9, i_ok - 1]\n",
    "                #ind[i][0][:] = ind_ok[i_ok - 1][0]\n",
    "                ind[:, 0, i] = ind_ok[0, i_ok - 1]\n",
    "            elif e0 < 50:\n",
    "                e0 = 50\n",
    "                e0_se = NAN\n",
    "            elif e0 > 400:\n",
    "                e0 = 400\n",
    "                e0_se = NAN\n",
    "        #end if\n",
    "    #end if\n",
    "\n",
    "    subd['e0_1_from_tair'][:] = e0\n",
    "\n",
    "    #### Finding slope of three different initial guess values\n",
    "    #### and choose the best of three\n",
    "    for j in range(2 + 1):\n",
    "        #### Change second value of fguess to\n",
    "        #### beta * (half initial guess, initial guess and double initial guess)\n",
    "        fguess[1] = beta * betafac[j]\n",
    "\n",
    "        # estimate parameters of the HLRC with fixed E0\n",
    "        '''\n",
    "        print(\"****************************\")\n",
    "        print(\"Starting HLRC_LloydVPD\")\n",
    "        print(\"****************************\")\n",
    "        '''\n",
    "        #numpy.savetxt(fname=\"dt_subd_2005_y_i_91_python.csv\", X=subd, delimiter=',', fmt='%s', header=','.join(subd.dtype.names), comments='')\n",
    "\n",
    "        #### Starting the optimization using the \"HLRC_LloydVPD\" function\n",
    "        hlrclvpd_results = nlinlts2(data=subd, lts_func=\"HLRC_LloydVPD\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'vpd_f'], npara=4, xguess=fguess[0:3 + 1], mprior=numpy.array(fguess[0:3 + 1], dtype=FLOAT_PREC), sigm=numpy.array([10, 600, 50, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "        #print(nlinlts2(data=subd, lts_func=\"HLRC_LloydVPD\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'vpd_f'], npara=4, xguess=fguess[0:3+1], mprior=numpy.array(fguess[0:3+1], dtype=FLOAT_PREC), sigm=numpy.array([10, 600, 50, 80]), sigd=subd['nee_fs_unc']))\n",
    "        #hlrclvpd_status, hlrclvpd_alpha, hlrclvpd_beta, hlrclvpd_k, hlrclvpd_rref, hlrclvpd_alpha_se, hlrclvpd_beta_se, hlrclvpd_k_se, hlrclvpd_rref_se, hlrclvpd_residuals, hlrclvpd_cov_matrix, hlrclvpd_cor_matrix, hlrclvpd_rmse, hlrclvpd_ls_status = nlinlts2(data=subd, lts_func=\"HLRC_LloydVPD\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'vpd_f'], npara=4, xguess=fguess[0:3+1], mprior=numpy.array(fguess[0:3+1], dtype=FLOAT_PREC), sigm=numpy.array([10, 600, 50, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "        #### Setting the returned model parameters\n",
    "        hlrclvpd_status = hlrclvpd_results['status']\n",
    "        hlrclvpd_alpha = hlrclvpd_results['alpha']\n",
    "        hlrclvpd_beta = hlrclvpd_results['beta']\n",
    "        hlrclvpd_k = hlrclvpd_results['k']\n",
    "        hlrclvpd_rref = hlrclvpd_results['rref']\n",
    "        hlrclvpd_alpha_se = hlrclvpd_results['alpha_std_error']\n",
    "        hlrclvpd_beta_se = hlrclvpd_results['beta_std_error']\n",
    "        hlrclvpd_k_se = hlrclvpd_results['k_std_error']\n",
    "        hlrclvpd_rref_se = hlrclvpd_results['rref_std_error']\n",
    "        hlrclvpd_residuals = hlrclvpd_results['residuals']\n",
    "        hlrclvpd_cov_matrix = hlrclvpd_results['cov_matrix']\n",
    "        hlrclvpd_cor_matrix = hlrclvpd_results['cor_matrix']\n",
    "        hlrclvpd_rmse = hlrclvpd_results['rmse']\n",
    "        hlrclvpd_ls_status = hlrclvpd_results['ls_status']\n",
    "\n",
    "        if hlrclvpd_cov_matrix is None or hlrclvpd_cor_matrix is None:\n",
    "            raise ONEFluxPartitionBrokenOptError('HLRC_LloydVPD', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "        #### Specifying which model we chose for this iteration (j -> modified fguess)\n",
    "        whichmodel[j] = 0\n",
    "\n",
    "        res_cor[j] = (hlrclvpd_residuals ** 2).sum() / (len(hlrclvpd_residuals) * (1.0 - trimperc / 100.0) - 4)\n",
    "\n",
    "        #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "        params[j, :, i] = numpy.array([hlrclvpd_alpha, hlrclvpd_beta, hlrclvpd_k, hlrclvpd_rref, e0, hlrclvpd_alpha_se, hlrclvpd_beta_se, hlrclvpd_k_se, hlrclvpd_rref_se, e0_se])\n",
    "\n",
    "        if params[j, 2, i] == 0:\n",
    "            whichmodel[j] = 1\n",
    "\n",
    "        p_cor[j, :, i] = numpy.array([hlrclvpd_cor_matrix[0][1], hlrclvpd_cor_matrix[0][2], hlrclvpd_cor_matrix[0][3], hlrclvpd_cor_matrix[1][2], hlrclvpd_cor_matrix[1][3], hlrclvpd_cor_matrix[2][3]])\n",
    "\n",
    "        rmse[j] = hlrclvpd_rmse\n",
    "\n",
    "        JTJ_inv[j, :, :] = numpy.copy(hlrclvpd_cov_matrix)\n",
    "\n",
    "        #### Check if parameter \"k\" is zero\n",
    "        if params[j, 2, i] == 0:\n",
    "            JTJ_inv_temp = numpy.zeros((len(fguess) - 1, len(fguess) - 1), dtype=DOUBLE_PREC)\n",
    "            whichmodel[j] = 1\n",
    "\n",
    "            JTJ_inv_temp[0][0] = hlrclvpd_cov_matrix[0][0]\n",
    "            JTJ_inv_temp[0][1] = hlrclvpd_cov_matrix[0][1]\n",
    "            JTJ_inv_temp[1][0] = hlrclvpd_cov_matrix[1][0]\n",
    "            JTJ_inv_temp[1][1] = hlrclvpd_cov_matrix[1][1]\n",
    "\n",
    "            JTJ_inv_temp[0][2] = hlrclvpd_cov_matrix[0][3]\n",
    "            JTJ_inv_temp[1][2] = hlrclvpd_cov_matrix[1][3]\n",
    "            JTJ_inv_temp[2][2] = hlrclvpd_cov_matrix[3][3]\n",
    "            JTJ_inv_temp[2][0] = hlrclvpd_cov_matrix[3][0]\n",
    "            JTJ_inv_temp[2][1] = hlrclvpd_cov_matrix[3][1]\n",
    "\n",
    "            JTJ_inv[j, :, :] = numpy.copy(JTJ_inv_temp)\n",
    "\n",
    "\n",
    "        #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "        #;; check k, if less than zero estimate parameters without VPD effect      ;;\n",
    "        #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "        if params[j, 2, i] < 0:\n",
    "            '''\n",
    "            print(\"****************************\")\n",
    "            print(\"Starting HLRC_Lloyd\")\n",
    "            print(\"****************************\")\n",
    "            '''\n",
    "\n",
    "            #hlrcl_status, hlrcl_alpha, hlrcl_beta, hlrcl_rref, hlrcl_alpha_se, hlrcl_beta_se, hlrcl_rref_se, hlrcl_residuals, hlrcl_cov_matrix, hlrcl_cor_matrix, hlrcl_rmse, hlrcl_ls_status = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair'], npara=3, xguess=numpy.array([fguess[0], fguess[1], fguess[3]]), mprior=numpy.array([fguess[0], fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([10, 600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "            #### Starting the optimization using the \"HLRC_Lloyd\" function\n",
    "            hlrcl_results = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair'], npara=3, xguess=numpy.array([fguess[0], fguess[1], fguess[3]]), mprior=numpy.array([fguess[0], fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([10, 600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "            #### Setting the returned model parameters\n",
    "            hlrcl_status = hlrcl_results['status']\n",
    "            hlrcl_alpha = hlrcl_results['alpha']\n",
    "            hlrcl_beta = hlrcl_results['beta']\n",
    "            hlrcl_rref = hlrcl_results['rref']\n",
    "            hlrcl_alpha_se = hlrcl_results['alpha_std_error']\n",
    "            hlrcl_beta_se = hlrcl_results['beta_std_error']\n",
    "            hlrcl_rref_se = hlrcl_results['rref_std_error']\n",
    "            hlrcl_residuals = hlrcl_results['residuals']\n",
    "            hlrcl_cov_matrix = hlrcl_results['cov_matrix']\n",
    "            hlrcl_cor_matrix = hlrcl_results['cor_matrix']\n",
    "            hlrcl_rmse = hlrcl_results['rmse']\n",
    "            hlrcl_ls_status = hlrcl_results['ls_status']\n",
    "\n",
    "            if hlrcl_cov_matrix is None or hlrcl_cor_matrix is None:\n",
    "                raise ONEFluxPartitionBrokenOptError('HLRC_Lloyd', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "            #### Specifying which model we chose for this iteration (j -> modified fguess)\n",
    "            whichmodel[j] = 1\n",
    "\n",
    "            res_cor[j] = (hlrcl_residuals ** 2).sum() / (len(hlrcl_residuals) * (1.0 - trimperc / 100.0) - 3)\n",
    "\n",
    "            #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "            params[j, :, i] = numpy.array([hlrcl_alpha, hlrcl_beta, 0, hlrcl_rref, e0, hlrcl_alpha_se, hlrcl_beta_se, 0, hlrcl_rref_se, e0_se])\n",
    "\n",
    "            p_cor[j, :, i] = numpy.array([hlrcl_cor_matrix[0][1], NAN, hlrcl_cor_matrix[0][2], NAN, hlrcl_cor_matrix[1][2], NAN])\n",
    "\n",
    "            rmse[j] = hlrcl_rmse\n",
    "\n",
    "            JTJ_inv[j, 0:3, 0:3] = numpy.copy(hlrcl_cov_matrix)\n",
    "\n",
    "\n",
    "            #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "            #;; check alpha, if less than zero estimate parameters with fixed alpha of last window and without VPD effect ;;\n",
    "            #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "            if (params[j, 0, i] > 0.22) and i_ok > 0:\n",
    "                if params_ok[0, i_ok - 1] > 0:\n",
    "                    alpha = params_ok[0, i_ok - 1]\n",
    "                    subd['alpha_1_from_tair'][:] = alpha\n",
    "                    ind[j, 1, i] = ind_ok[1, i_ok - 1]\n",
    "\n",
    "                    #hlrcl_status_afix, hlrcl_beta_afix, hlrcl_rref_afix, hlrcl_beta_se_afix, hlrcl_rref_se_afix, hlrcl_residuals_afix, hlrcl_cov_matrix_afix, hlrcl_cor_matrix_afix, hlrcl_rmse_afix, hlrcl_ls_status_afix = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'alpha_1_from_tair'], npara=2, xguess=numpy.array([fguess[1], fguess[3]]), mprior=numpy.array([fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                    #### Starting the optimization using the \"HLRC_Lloyd_afix\" function\n",
    "                    hlrcl_results_afix = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'alpha_1_from_tair'], npara=2, xguess=numpy.array([fguess[1], fguess[3]]), mprior=numpy.array([fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                    #### Setting the returned model parameters\n",
    "                    hlrcl_status_afix = hlrcl_results_afix['status']\n",
    "                    hlrcl_beta_afix = hlrcl_results_afix['beta']\n",
    "                    hlrcl_rref_afix = hlrcl_results_afix['rref']\n",
    "                    hlrcl_beta_se_afix = hlrcl_results_afix['beta_std_error']\n",
    "                    hlrcl_rref_se_afix = hlrcl_results_afix['rref_std_error']\n",
    "                    hlrcl_residuals_afix = hlrcl_results_afix['residuals']\n",
    "                    hlrcl_cov_matrix_afix = hlrcl_results_afix['cov_matrix']\n",
    "                    hlrcl_cor_matrix_afix = hlrcl_results_afix['cor_matrix']\n",
    "                    hlrcl_rmse_afix = hlrcl_results_afix['rmse']\n",
    "                    hlrcl_ls_status_afix = hlrcl_results_afix['ls_status']\n",
    "\n",
    "                    if hlrcl_cov_matrix_afix is None or hlrcl_cor_matrix_afix is None:\n",
    "                        raise ONEFluxPartitionBrokenOptError('HLRC_Lloyd_afix', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "                    #### Specifying which model we chose for this iteration (j -> modified fguess)\n",
    "                    whichmodel[j] = 2\n",
    "\n",
    "                    res_cor[j] = (hlrcl_residuals_afix ** 2).sum() / (len(hlrcl_residuals_afix) * (1.0 - trimperc / 100.0) - 2)\n",
    "\n",
    "                    #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "                    params[j, :, i] = numpy.array([alpha, hlrcl_beta_afix, 0, hlrcl_rref_afix, e0, NAN, hlrcl_beta_se_afix, 0, hlrcl_rref_se_afix, e0_se])\n",
    "\n",
    "                    p_cor[j, :, i] = numpy.array([NAN, NAN, NAN, NAN, hlrcl_cor_matrix_afix[0][1], NAN])\n",
    "\n",
    "                    rmse[j] = hlrcl_rmse_afix\n",
    "\n",
    "                    JTJ_inv[j, 0:2, 0:2] = numpy.copy(hlrcl_cov_matrix_afix)\n",
    "\n",
    "\n",
    "        #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "        #;; check alpha, if gt 0.22 estimate parameters with fixed alpha of last window                   ;;\n",
    "        #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "        elif (params[j, 0, i] > 0.22) and (i_ok > 0):\n",
    "            if params_ok[0, i_ok - 1] > 0:\n",
    "                alpha = params_ok[0, i_ok - 1]\n",
    "                subd['alpha_1_from_tair'][:] = alpha\n",
    "                ind[j, 1, i] = ind_ok[1, i_ok - 1]\n",
    "\n",
    "                '''\n",
    "                print(\"****************************\")\n",
    "                print(\"Starting HLRC_LloydVPD_afix\")\n",
    "                print(\"****************************\")\n",
    "                '''\n",
    "                #hlrclvpd_status_afix, hlrclvpd_beta_afix, hlrclvpd_k_afix, hlrclvpd_rref_afix, hlrclvpd_beta_se_afix, hlrclvpd_k_se_afix, hlrclvpd_rref_se_afix, hlrclvpd_residuals_afix, hlrclvpd_cov_matrix_afix, hlrclvpd_cor_matrix_afix, hlrclvpd_rmse_afix, hlrclvpd_ls_status_afix = nlinlts2(data=subd, lts_func=\"HLRC_LloydVPD_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'vpd_f', 'alpha_1_from_tair'], npara=3, xguess=numpy.array([fguess[1], fguess[2], fguess[3]]), mprior=numpy.array([fguess[1], fguess[2], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 50, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                #### Starting the optimization using the \"HLRC_LloydVPD_afix\" function\n",
    "                hlrclvpd_results = nlinlts2(data=subd, lts_func=\"HLRC_LloydVPD_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'vpd_f', 'alpha_1_from_tair'], npara=3, xguess=numpy.array([fguess[1], fguess[2], fguess[3]]), mprior=numpy.array([fguess[1], fguess[2], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 50, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                #### Setting the returned model parameters\n",
    "                hlrclvpd_status_afix = hlrclvpd_results['status']\n",
    "                hlrclvpd_beta_afix = hlrclvpd_results['beta']\n",
    "                hlrclvpd_k_afix = hlrclvpd_results['k']\n",
    "                hlrclvpd_rref_afix = hlrclvpd_results['rref']\n",
    "                hlrclvpd_beta_se_afix = hlrclvpd_results['beta_std_error']\n",
    "                hlrclvpd_k_se_afix = hlrclvpd_results['k_std_error']\n",
    "                hlrclvpd_rref_se_afix = hlrclvpd_results['rref_std_error']\n",
    "                hlrclvpd_residuals_afix = hlrclvpd_results['residuals']\n",
    "                hlrclvpd_cov_matrix_afix = hlrclvpd_results['cov_matrix']\n",
    "                hlrclvpd_cor_matrix_afix = hlrclvpd_results['cor_matrix']\n",
    "                hlrclvpd_rmse_afix = hlrclvpd_results['rmse']\n",
    "                hlrclvpd_ls_status_afix = hlrclvpd_results['ls_status']\n",
    "\n",
    "                if hlrclvpd_cov_matrix_afix is None or hlrclvpd_cor_matrix_afix is None:\n",
    "                    raise ONEFluxPartitionBrokenOptError('HLRC_LloydVPD_afix', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "                #### Specifying which model we chose for this iteration (j -> modified fguess)\n",
    "                whichmodel[j] = 3\n",
    "\n",
    "                res_cor[j] = (hlrclvpd_residuals_afix ** 2).sum() / (len(hlrclvpd_residuals_afix) * (1.0 - trimperc / 100.0) - 3)\n",
    "\n",
    "                #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "                params[j, :, i] = numpy.array([alpha, hlrclvpd_beta_afix, hlrclvpd_k_afix, hlrclvpd_rref_afix, e0, 0, hlrclvpd_beta_se_afix, hlrclvpd_k_se_afix, hlrclvpd_rref_se_afix, e0_se])\n",
    "\n",
    "                p_cor[j, :, i] = numpy.array([NAN, NAN, NAN, hlrclvpd_cor_matrix_afix[0][1], hlrclvpd_cor_matrix_afix[0][2], hlrclvpd_cor_matrix_afix[1][2]])\n",
    "\n",
    "                rmse[j] = hlrclvpd_rmse_afix\n",
    "\n",
    "                JTJ_inv[j, 0:3, 0:3] = numpy.copy(hlrclvpd_cov_matrix_afix)\n",
    "\n",
    "                #### Check if parameter \"k\" is 0\n",
    "                if params[j, 2, i] == 0:\n",
    "                    JTJ_inv_temp = numpy.zeros((len(fguess) - 1, len(fguess) - 1), dtype=DOUBLE_PREC)\n",
    "                    whichmodel[j] = 2\n",
    "\n",
    "                    JTJ_inv_temp[0][0] = hlrclvpd_cov_matrix_afix[0][0]\n",
    "                    JTJ_inv_temp[0][1] = hlrclvpd_cov_matrix_afix[2][0]\n",
    "                    JTJ_inv_temp[1][0] = hlrclvpd_cov_matrix_afix[0][2]\n",
    "                    JTJ_inv_temp[1][1] = hlrclvpd_cov_matrix_afix[2][2]\n",
    "\n",
    "                    JTJ_inv[j, :, :] = numpy.copy(JTJ_inv_temp)\n",
    "\n",
    "\n",
    "                #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                #;; check k, if less than zero estimate parameters without VPD effect and with fixed alpha of last window ;;\n",
    "                #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "                if params[j, 2, i] < 0:\n",
    "                    '''\n",
    "                    print(\"****************************\")\n",
    "                    print(\"Starting HLRC_Lloyd_afix\")\n",
    "                    print(\"****************************\")\n",
    "                    '''\n",
    "                    #hlrcl_status_afix, hlrcl_beta_afix, hlrcl_rref_afix, hlrcl_beta_se_afix, hlrcl_rref_se_afix, hlrcl_residuals_afix, hlrcl_cov_matrix_afix, hlrcl_cor_matrix_afix, hlrcl_rmse_afix, hlrcl_ls_status_afix = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'alpha_1_from_tair'], npara=2, xguess=numpy.array([fguess[1], fguess[3]]), mprior=numpy.array([fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                    #### Starting the optimization using the \"HLRC_Lloyd_afix\" function\n",
    "                    hlrcl_results_afix = nlinlts2(data=subd, lts_func=\"HLRC_Lloyd_afix\", depvar='nee_f', indepvar_arr=['rg_f', 'tair_f', 'e0_1_from_tair', 'alpha_1_from_tair'], npara=2, xguess=numpy.array([fguess[1], fguess[3]]), mprior=numpy.array([fguess[1], fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([600, 80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "                    #### Setting the returned model parameters\n",
    "                    hlrcl_status_afix = hlrcl_results_afix['status']\n",
    "                    hlrcl_beta_afix = hlrcl_results_afix['beta']\n",
    "                    hlrcl_rref_afix = hlrcl_results_afix['rref']\n",
    "                    hlrcl_beta_se_afix = hlrcl_results_afix['beta_std_error']\n",
    "                    hlrcl_rref_se_afix = hlrcl_results_afix['rref_std_error']\n",
    "                    hlrcl_residuals_afix = hlrcl_results_afix['residuals']\n",
    "                    hlrcl_cov_matrix_afix = hlrcl_results_afix['cov_matrix']\n",
    "                    hlrcl_cor_matrix_afix = hlrcl_results_afix['cor_matrix']\n",
    "                    hlrcl_rmse_afix = hlrcl_results_afix['rmse']\n",
    "                    hlrcl_ls_status_afix = hlrcl_results_afix['ls_status']\n",
    "\n",
    "                    if hlrcl_cov_matrix_afix is None or hlrcl_cor_matrix_afix is None:\n",
    "                        raise ONEFluxPartitionBrokenOptError('HLRC_Lloyd_afix', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "                    #### Specifying which model we chose for this iteration (modified fguess)\n",
    "                    whichmodel[j] = 2\n",
    "\n",
    "                    res_cor[j] = (hlrcl_residuals_afix ** 2).sum() / (len(hlrcl_residuals_afix) * (1.0 - trimperc / 100.0) - 2)\n",
    "\n",
    "                    #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "                    params[j, :, i] = numpy.array([alpha, hlrcl_beta_afix, 0, hlrcl_rref_afix, e0, 0, hlrcl_beta_se_afix, 0, hlrcl_rref_se_afix, e0_se])\n",
    "\n",
    "                    p_cor[j, :, i] = numpy.array([NAN, NAN, NAN, NAN, hlrcl_cor_matrix_afix[0][1], NAN])\n",
    "\n",
    "                    rmse[j] = hlrcl_rmse_afix\n",
    "\n",
    "                    JTJ_inv[j, 0:2, 0:2] = numpy.copy(hlrcl_cov_matrix_afix)\n",
    "\n",
    "\n",
    "        #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "        #;; check if alpha or beta less than 0, if yes set to 0                                                 ;;\n",
    "        #;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "        if params[j, 0, i] < 0 or params[j, 1, i] < 0:\n",
    "            '''\n",
    "            print(\"****************************\")\n",
    "            print(\"Starting LloydT_E0fix\")\n",
    "            print(\"****************************\")\n",
    "            '''\n",
    "            #lt_status_e0fix, lt_rref_e0fix, lt_rref_se_e0fix, lt_residuals_e0fix, lt_cov_matrix_e0fix, lt_cor_matrix_e0fix, lt_rmse_e0fix, lt_ls_status_e0fix = nlinlts2(data=subd, lts_func=\"LloydT_E0fix\", depvar='nee_f', indepvar_arr=['tair_f', 'e0_1_from_tair'], npara=1, xguess=numpy.array([fguess[3]]), mprior=numpy.array([fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "            #### Starting the optimization using the \"LloydT_E0fix\" function\n",
    "            lt_results_e0fix = nlinlts2(data=subd, lts_func=\"LloydT_E0fix\", depvar='nee_f', indepvar_arr=['tair_f', 'e0_1_from_tair'], npara=1, xguess=numpy.array([fguess[3]]), mprior=numpy.array([fguess[3]], dtype=FLOAT_PREC), sigm=numpy.array([80]), sigd=subd['nee_fs_unc'])\n",
    "\n",
    "            #### Setting the returned model parameters\n",
    "            lt_status_e0fix = lt_results_e0fix['status']\n",
    "            lt_rref_e0fix = lt_results_e0fix['rref']\n",
    "            lt_rref_se_e0fix = lt_results_e0fix['rref_std_error']\n",
    "            lt_residuals_e0fix = lt_results_e0fix['residuals']\n",
    "            lt_cov_matrix_e0fix = lt_results_e0fix['cov_matrix']\n",
    "            lt_cor_matrix_e0fix = lt_results_e0fix['cor_matrix']\n",
    "            lt_rmse_e0fix = lt_results_e0fix['rmse']\n",
    "            lt_ls_status_e0fix = lt_results_e0fix['ls_status']\n",
    "\n",
    "            if lt_cov_matrix_e0fix is None or lt_cor_matrix_e0fix is None:\n",
    "                raise ONEFluxPartitionBrokenOptError('LloydT_E0fix', site_id=site_id, year=year, day_begin=day_begin2, day_end=day_end2, prod=ustar_type, perc=percentile_num)\n",
    "\n",
    "            #### Specifying which model we chose for this iteration (j -> modified fguess)\n",
    "            whichmodel[j] = 4\n",
    "\n",
    "            res_cor[j] = (lt_residuals_e0fix ** 2).sum() / (len(lt_residuals_e0fix) * (1.0 - trimperc / 100.0) - 1)\n",
    "\n",
    "            #### Setting the parameters of this iteration (j -> modified fguess)\n",
    "            params[j, :, i] = numpy.array([0, 0, 0, lt_rref_e0fix, e0, 0, 0, 0, lt_rref_se_e0fix, e0_se])\n",
    "\n",
    "            p_cor[j, :, i] = numpy.array([NAN, NAN, NAN, NAN, NAN, NAN])\n",
    "\n",
    "            rmse[j] = lt_rmse_e0fix\n",
    "\n",
    "            JTJ_inv[j, 0, 0] = numpy.copy(lt_cov_matrix_e0fix)\n",
    "\n",
    "\n",
    "        is_pars_ok = check_parameters(params=params[j, :, i], fguess=fguess)\n",
    "        if is_pars_ok == 0:\n",
    "            rmse[j] = 9999.0\n",
    "\n",
    "    # end of \"for j\"\n",
    "\n",
    "#### Find which iteration \"j\" that resulted in the most minimum rmse\n",
    "jmin = numpy.where(rmse == numpy.min(numpy.abs(rmse)))\n",
    "jmin = jmin[0]\n",
    "\n",
    "#### Check if the paramters chosen of the current set are valid\n",
    "is_pars_ok = check_parameters(params=params[jmin[0], :, i], fguess=fguess)\n",
    "\n",
    "\n",
    "#### This if statement is weird but it's in the pvwave code\n",
    "if ind[jmin[0], 1, i] == 6616: # TODO: investigate and replace statement\n",
    "    msg = \"DT EXIT EXCEPTION: exact number of indices\"\n",
    "    _log.critical(msg)\n",
    "    raise ONEFluxPartitionError(msg)\n",
    "\n",
    "#### If the current set of parameters is valid\n",
    "#### then we choose it for the current window\n",
    "if is_pars_ok == 1:\n",
    "    # my code\n",
    "\n",
    "    params_all_for_ranges['i'][i] = i\n",
    "    params_all_for_ranges['day'][i] = i * 4 + 1 - i * 2\n",
    "    params_all_for_ranges['i_ok'][i] = i_ok\n",
    "    params_all_for_ranges['alpha'][i] = params[jmin[0], 0, i]\n",
    "    params_all_for_ranges['beta'][i] = params[jmin[0], 1, i]\n",
    "    params_all_for_ranges['k'][i] = params[jmin[0], 2, i]\n",
    "    params_all_for_ranges['rref'][i] = params[jmin[0], 3, i]\n",
    "    params_all_for_ranges['e0'][i] = params[jmin[0], 4, i]\n",
    "    #end of code\n",
    "\n",
    "    params_ok[:, i_ok] = params[jmin[0], :, i]\n",
    "    ind_ok[:, i_ok] = ind[jmin[0], :, i]\n",
    "    p_cor_ok[:, i_ok] = p_cor[jmin[0], :, i]\n",
    "    whichmodel_ok[i_ok] = whichmodel[jmin[0]]\n",
    "    JTJ_inv_ok[i_ok, :, :] = JTJ_inv[jmin[0], :, :]\n",
    "    res_cor_ok[i_ok] = res_cor[jmin[0]]\n",
    "    i_ok = i_ok + 1\n",
    "\n",
    "#### else this window won't work and we will use the\n",
    "#### previous window\n",
    "else:\n",
    "    # my code\n",
    "\n",
    "    params_all_for_ranges['i'][i] = i\n",
    "    params_all_for_ranges['day'][i] = i * 4 + 1 - i * 2\n",
    "    params_all_for_ranges['i_ok'][i] = -9999\n",
    "    params_all_for_ranges['alpha'][i] = -9999.0\n",
    "    params_all_for_ranges['beta'][i] = -9999.0\n",
    "    params_all_for_ranges['k'][i] = -9999.0\n",
    "    params_all_for_ranges['rref'][i] = -9999.0\n",
    "    params_all_for_ranges['e0'][i] = -9999.0\n",
    "    #end of code\n",
    "\n",
    "    params_nok[:, i_nok] = params[jmin[0], :, i]\n",
    "    i_nok = i_nok + 1\n",
    "\n",
    "params_all[:, i] = params[jmin[0], :, i]\n",
    "params_all_timestamp[i, :] = numpy.append([is_pars_ok, day_begin], numpy.transpose(params[jmin[0], :, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "bbf2181e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0], 0, 56)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params, j,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Setting the final valid parameters to be returned\n",
    "if i_ok > 0:\n",
    "    params_return = params_ok[:, 0:i_ok]\n",
    "    ind_return = ind_ok[:, 0:i_ok]\n",
    "    p_correl_return = p_cor_ok[:, 0:i_ok]\n",
    "    whichmodel_return = whichmodel_ok[0:i_ok]\n",
    "    JTJ_inv_return = JTJ_inv_ok[0:i_ok, :, :]\n",
    "    res_cor_return = res_cor_ok[0:i_ok]\n",
    "else:\n",
    "    nan_arr = numpy.empty(len(fguess) + 3)\n",
    "    nan_arr.fill(NAN)\n",
    "    return nan_arr, None, None, None, None\n",
    "\n",
    "# My code (not in pvwave)\n",
    "i_ok_temp = 0\n",
    "for i in range(n_parasets):\n",
    "    if params_all_for_ranges['i_ok'][i] >= 0:\n",
    "        if i_ok_temp == 0:\n",
    "            index_begin, index_end = 0, int(ind_return[2, i_ok_temp + 1])\n",
    "#                print(\"index [ 0]: \", index_begin, index_end)\n",
    "            params_all_for_ranges['ind_begin'][i] = index_begin\n",
    "            params_all_for_ranges['ind_end'][i] = index_end\n",
    "            params_all_for_ranges['subset_size'][i] = params_all_for_ranges['ind_end'][i] - params_all_for_ranges['ind_begin'][i]\n",
    "            ### populate variability (STD) for input data\n",
    "#                print(\"****STD [ 0]: \", index_begin, index_end, numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end]), numpy.nanstd(data['tair'][index_begin:index_end]), numpy.nanstd(data['rg'][index_begin:index_end]))\n",
    "            params_all_for_ranges['nee_avg'][i] = numpy.nanmean(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "            params_all_for_ranges['ta_avg'][i] = numpy.nanmean(data['tair'][index_begin:index_end])\n",
    "            params_all_for_ranges['rg_avg'][i] = numpy.nanmean(data['rg'][index_begin:index_end])\n",
    "            params_all_for_ranges['nee_std'][i] = numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "            params_all_for_ranges['ta_std'][i] = numpy.nanstd(data['tair'][index_begin:index_end])\n",
    "            params_all_for_ranges['rg_std'][i] = numpy.nanstd(data['rg'][index_begin:index_end])\n",
    "        elif i_ok_temp == (i_ok - 1):\n",
    "            index_begin, index_end = int(ind_return[2, i_ok_temp - 1]), int(numpy.max(data['ind']))\n",
    "#                print(\"index [-1]: \", index_begin, index_end)\n",
    "            params_all_for_ranges['ind_begin'][i] = index_begin\n",
    "            params_all_for_ranges['ind_end'][i] = index_end\n",
    "            params_all_for_ranges['subset_size'][i] = params_all_for_ranges['ind_end'][i] - params_all_for_ranges['ind_begin'][i]\n",
    "            ### populate variability (STD) for input data\n",
    "#                print(\"****STD [-1]: \", index_begin, index_end, numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end]), numpy.nanstd(data['tair'][index_begin:index_end]), numpy.nanstd(data['rg'][index_begin:index_end]))\n",
    "            params_all_for_ranges['nee_avg'][i] = numpy.nanmean(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "            params_all_for_ranges['ta_avg'][i] = numpy.nanmean(data['tair'][index_begin:index_end])\n",
    "            params_all_for_ranges['rg_avg'][i] = numpy.nanmean(data['rg'][index_begin:index_end])\n",
    "            params_all_for_ranges['nee_std'][i] = numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "            params_all_for_ranges['ta_std'][i] = numpy.nanstd(data['tair'][index_begin:index_end])\n",
    "            params_all_for_ranges['rg_std'][i] = numpy.nanstd(data['rg'][index_begin:index_end])\n",
    "\n",
    "        elif i_ok_temp >= i_ok:\n",
    "            index_begin, index_end = -9999, -9999\n",
    "#                print(\"index [>=]: \", index_begin, index_end)\n",
    "            params_all_for_ranges['ind_begin'][i] = index_begin\n",
    "            params_all_for_ranges['ind_end'][i] = index_end\n",
    "            params_all_for_ranges['subset_size'][i] = -9999\n",
    "            ### populate variability (STD) for input data\n",
    "#                print(\"****STD [>=]: \", index_begin, index_end, numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end]), numpy.nanstd(data['tair'][index_begin:index_end]), numpy.nanstd(data['rg'][index_begin:index_end]))\n",
    "            params_all_for_ranges['nee_avg'][i] = NAN\n",
    "            params_all_for_ranges['ta_avg'][i] = NAN\n",
    "            params_all_for_ranges['rg_avg'][i] = NAN\n",
    "            params_all_for_ranges['nee_std'][i] = NAN\n",
    "            params_all_for_ranges['ta_std'][i] = NAN\n",
    "            params_all_for_ranges['rg_std'][i] = NAN\n",
    "        else:\n",
    "            index_begin, index_end = int(ind_return[2, i_ok_temp - 1]), int(ind_return[2, i_ok_temp + 1])\n",
    "#                print(\"index [el]: \", index_begin, index_end)\n",
    "            params_all_for_ranges['ind_begin'][i] = index_begin\n",
    "            params_all_for_ranges['ind_end'][i] = index_end\n",
    "            params_all_for_ranges['subset_size'][i] = params_all_for_ranges['ind_end'][i] - params_all_for_ranges['ind_begin'][i]\n",
    "            ### populate variability (STD) for input data\n",
    "#                print(\"****STD [el]: \", index_begin, index_end, numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end]), numpy.nanstd(data['tair'][index_begin:index_end]), numpy.nanstd(data['rg'][index_begin:index_end]))\n",
    "            params_all_for_ranges['nee_avg'][i] = numpy.nanmean(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "            params_all_for_ranges['ta_avg'][i] = numpy.nanmean(data['tair'][index_begin:index_end])\n",
    "            params_all_for_ranges['rg_avg'][i] = numpy.nanmean(data['rg'][index_begin:index_end])\n",
    "            params_all_for_ranges['nee_std'][i] = numpy.nanstd(data['nee_f'][data['nee_fqc'] == 0][index_begin:index_end])\n",
    "            params_all_for_ranges['ta_std'][i] = numpy.nanstd(data['tair'][index_begin:index_end])\n",
    "            params_all_for_ranges['rg_std'][i] = numpy.nanstd(data['rg'][index_begin:index_end])\n",
    "        i_ok_temp = i_ok_temp + 1\n",
    "    else:\n",
    "        params_all_for_ranges['ind_begin'][i] = -9999\n",
    "        params_all_for_ranges['ind_end'][i] = -9999\n",
    "        params_all_for_ranges['subset_size'][i] = -9999\n",
    "\n",
    "var_names = 'alpha,beta,k,rref,e0,alpha_se,beta_se,k_se,rref_se,e0_se'\n",
    "var_names_timestamp = 'ok,day_begin,alpha,beta,k,rref,e0,alpha_se,beta_se,k_se,rref_se,e0_se'\n",
    "\n",
    "var_names_index = 'alpha,beta,k,rref,e0,alpha_se,beta_se,k_se,rref_se,e0_se,index1,index2,index3'\n",
    "\n",
    "#numpy.savetxt('test_es_python_before.csv', numpy.transpose(params_return), delimiter=',', fmt='%s')\n",
    "#numpy.savetxt('test_es_params_all_python.csv', numpy.transpose(params_all), delimiter=',', fmt='%s')\n",
    "#numpy.savetxt('test_es_params_all_python.csv', numpy.transpose(params_all), delimiter=',', header=var_names, fmt='%s')\n",
    "#numpy.savetxt('test_es_params_all_timestamp_python.csv', params_all_timestamp, delimiter=',', fmt='%s')\n",
    "#numpy.savetxt('test_es_params_all_timestamp_python.csv', params_all_timestamp, delimiter=',', header=var_names_timestamp, fmt='%s')\n",
    "\n",
    "#numpy.savetxt('test_es_params_index_all_timestamp_python.csv', numpy.transpose(numpy.concatenate((params_all, ind_ok), axis=0)), delimiter=',', header=var_names_index, fmt='%s')\n",
    "\n",
    "filename_range = 'nee_' + ustar_type + '_' + str(percentile_num) + '_' + site_id + '_' + str(year) + '_params_after_es_python.csv'\n",
    "numpy.savetxt(os.path.join(dt_output_dir, filename_range), params_all_for_ranges, delimiter=',', header=','.join(params_all_for_ranges.dtype.names), fmt='%s')\n",
    "#exit()\n",
    "# end of code\n",
    "\n",
    "_log.info(\"Finished estimate_parasets of daytime for nee_{u}_{p}_{s}_{y}\".format(u=ustar_type, p=percentile_num, s=site_id, y=year))\n",
    "\n",
    "return numpy.concatenate((params_return, ind_return), axis=0), whichmodel_return, JTJ_inv_return, res_cor_return, p_correl_return\n",
    "#end of estimate_parasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsOK = np.where(params == -9999)\n",
    "\n",
    "if len(paramsOK[0]) == params.size:\n",
    "    print(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eee4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calling compute_flux to calculate the Reco and GPP variables\n",
    "reco_flux, gpp_flux, pf_flux1, pf_flux2 = compute_flux(data=h_data, params=params, dt_output_dir=dt_output_dir, site_id=site_id, \n",
    "                                                       ustar_type=ustar_type, percentile_num=percentile_num, year=year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calling compute_var to get the predicted variable by specifying\n",
    "#### the model we used in estimate_params\n",
    "varGPP = compute_var(data=h_data, params=params, whichmodel=whichmodel, JTJ_inv=JTJ_inv, res_cor=res_cor)\n",
    "\n",
    "#print(\"flux\")\n",
    "#print(flux)\n",
    "#print(\"varGPP\")\n",
    "#print(varGPP)\n",
    "\n",
    "if reco_flux.size == 0 or gpp_flux.size == 0:\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_se = np.isnan(pf_flux1)\n",
    "j_se = np.isinf(pf_flux1)\n",
    "\n",
    "pf_flux1[i_se] = NAN\n",
    "pf_flux1[j_se] = NAN\n",
    "\n",
    "add_empty_vars(data=h_data, records=reco_flux, column='reco_hblr')\n",
    "add_empty_vars(data=h_data, records=gpp_flux, column='gpp_hblr')\n",
    "\n",
    "#### Check if predicted model has values\n",
    "if varGPP.size == 0:\n",
    "    h_data['se_gpp_hblr'] = NAN\n",
    "else:\n",
    "    add_empty_vars(data=h_data, records=np.sqrt(varGPP), column='se_gpp_hblr')\n",
    "\n",
    "nee = reco_flux - gpp_flux\n",
    "add_empty_vars(data=h_data, records=nee, column='nee_hblr')\n",
    "add_empty_vars(data=h_data, records=pf_flux1, column='p_flag1')\n",
    "add_empty_vars(data=h_data, records=pf_flux2, column='p_flag2')\n",
    "\n",
    "index = params[12, :]\n",
    "'''\n",
    "print(\"index\")\n",
    "print(index)\n",
    "'''\n",
    "h_data['rb'][index.astype(int)] = params[3, :]\n",
    "h_data['beta'][index.astype(int)] = params[1, :]\n",
    "h_data['k'][index.astype(int)] = params[2, :]\n",
    "h_data['e0'][params[10, :].astype(int)] = params[4, :]\n",
    "h_data['alpha'][params[11, :].astype(int)] = params[0, :]\n",
    "\n",
    "h_data['flag_sum'] = pf_flux1 + pf_flux2\n",
    "\n",
    "print(\"Finished flux_part_gl2010 of daytime for nee_{u}_{p}_{s}_{y}\".format(u=ustar_type, p=percentile_num, s=site_id, y=year))\n",
    "\n",
    "return h_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78934853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### call flux_part_gl2010 for day time (main partitioning process)\n",
    "result_year_data = flux_part_gl2010(data=working_year_data, name_file=name_file, name_out=name_out, dt_output_dir=dt_output_dir, site_id=siteid, ustar_type=ustar_type, percentile_num=percentile, year=year)\n",
    "\n",
    "if result_year_data is None:\n",
    "    print(\"Error processing output file '{f}\".format(f=output_filename))\n",
    "else:\n",
    "    # save output data file\n",
    "    print(\"Saving output file '{f}\".format(f=output_filename))\n",
    "    np.savetxt(fname=output_filename, X=result_year_data, delimiter=',', fmt='%s', header=','.join(result_year_data.dtype.names), comments='')\n",
    "    print(\"Saved output file '{f}\".format(f=output_filename))\n",
    "\n",
    "print(\"Finished processing percentile '{p}'\".format(p=percentile))\n",
    "print(\"Finished processing year '{y}'\".format(y=year))\n",
    "print(\"Finished processing UStar threshold type '{u}'\".format(u=ustar_type))\n",
    "print(\"Finished DT partitioning of {s}\".format(s=siteid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ddfd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "dbfa2387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n",
      "[4.]\n",
      "[4.]\n",
      "[4.00000006]\n",
      "[3.25000002]\n",
      "[3.25000007]\n",
      "[2.12500021]\n",
      "[3.00257722]\n",
      "[3.00257726]\n",
      "[2.7557839]\n",
      "[2.97784125]\n",
      "[3.00000038]\n",
      "[3.00000043]\n",
      "[2.99742348]\n",
      "[2.99974269]\n",
      "[2.99997461]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3.00000038]), 1)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import leastsq\n",
    "def func(x):\n",
    "    print(x)\n",
    "    return 2*(x-3)**2+1\n",
    "leastsq(func, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b6258ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from scipy.optimize import leastsq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = array([12, 8, 11, 7, 5, 2, 3, 5, 6, 4, 5, 7, 8, 13, 19, 22, 25])\n",
    "x = array(range(len(y)))\n",
    "\n",
    "\n",
    "def func1(params, x, y):\n",
    "    a, b, c = params[0], params[1], params[2]\n",
    "    residual = y-(a*x**2+b*x+c)\n",
    "    return residual\n",
    "\n",
    "def func2(params, x, y):\n",
    "    a, b, c = params[0], params[1], params[2]\n",
    "    residual = y-(a*x**3+b*x+c)\n",
    "    return residual\n",
    "\n",
    "def func3(params, x, y):\n",
    "    a, b, c = params[0], params[1], params[2]\n",
    "    residual = y-(a*x**2+b*x)\n",
    "    return residual\n",
    "\n",
    "params=[0, 0, 0]\n",
    "\n",
    "result = leastsq(func1, x0=params,args=(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "821b3a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.23993808, -3.07920537, 13.04850363]), 3)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1b591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsi",
   "language": "python",
   "name": "hsi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
